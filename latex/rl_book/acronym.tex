\newacronym{dp}{DP}{Dynamic Programming}
\newacronym{drl}{DRL}{Deep Reinforcement Learning}
\newacronym{mdp}{MDP}{Markov Decision Process}
\newacronym{mc}{MC}{Monte Carlo}
\newacronym{rl}{RL}{Reinforcement Learning}
\newacronym{td}{TD}{Temporal Difference}
\newacronym{dqn}{DQN}{Deep Q Network}
\newacronym{ddqn}{DDQN}{Double Deep Q Network}
\newacronym{d3qn}{D3QN}{Dueling Double Deep Q Network}
\newacronym{per}{PER}{Priority Experience Replay}
\newacronym{ddpg}{DDPG}{Deep Deterministic Policy Gradient}
\newacronym{trpo}{TRPO}{Trust Region Policy Optimization}
\newacronym{ppo}{PPO}{Proximal Policy Optimization}
\newacronym{pg}{PG}{Policy Gradient}
\newacronym{ac}{AC}{Actor-Critic}
\newacronym{a2c}{A2C}{Advantage Actor-Critic}
\newacronym{a3c}{A3C}{Asynchronous Advantage Actor-Critic}
\newacronym{sac}{SAC}{Soft Actor-Critic}

