\chapter{Soft Actor-Critic: A Maximum Entropy Reinforcement Learning Algorithm}
\label{chap:sac_intro}
\index{Actor-Critic!SAC}
\index{Soft Actor-Critic (SAC)}


The \textbf{Soft Actor-Critic (SAC)} algorithm \cite{haarnoja2018soft} stands as a cornerstone in
modern deep reinforcement learning, particularly for \textbf{continuous control tasks}. Unlike
traditional reinforcement learning methods that solely focus on maximizing expected cumulative
reward, SAC operates within the framework of \textbf{maximum entropy reinforcement learning}. This
paradigm not only seeks to achieve high rewards but also encourages the policy to be as random as
possible, given the reward constraints. This inherent drive for \textbf{exploration} makes SAC
robust, stable, and highly effective in complex environments.

\section{The Maximum Entropy Objective}
\label{sec:max_entropy_objective}

The standard RL objective is to maximize the expected cumulative reward:
\begin{equation}
  J(\pi) = \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} \left[ \sum_t \gamma^t r(s_t, a_t) \right],
  \label{eq:rl_obj}
\end{equation}

SAC modifies this objective by including an entropy term to encourage exploration. Therefore, SAC's
power stems from optimizing a policy that simultaneously maximizes both the expected cumulative
reward and the \textbf{entropy of the policy}. This objective is formally expressed as:
\begin{equation}
J(\pi) = \sum_{t=0}^T \mathbb{E}_{(s_t, a_t) \sim \rho_\pi} \left[\gamma^t r(s_t, a_t) + \alpha H(\pi(\cdot|s_t)) \right]
\label{eq:sac_obj}
\end{equation}

Let's break down the components of this crucial objective:
\begin{itemize}
    \item $r(s_t, a_t)$ represents the \textbf{reward} obtained at time $t$ for executing action
      $a_t$ in state $s_t$ and \(\gamma \in [0, 1)\) is the discount factor.
    \item $\rho_\pi$ signifies the \textbf{state-action marginal visitation probability
      distribution} when following policy $\pi$. See \ref{sec:samvp} for more details.

    \item $H(\pi(\cdot|s_t))$ denotes the \textbf{entropy of the policy} at state $s_t$. Entropy, in
      information theory, quantifies the uncertainty or randomness of a probability distribution.
      For a discrete distribution $P$, it's $H(P) = -\sum_x P(x) \log P(x)$. For continuous
      distributions, it's the differential entropy, typically $H(P) = \mathbb{E}_{x \sim P}[-\log
      P(x)]$. A higher entropy value implies a more uncertain or exploratory policy.

    \item $\alpha$ is the \textbf{temperature parameter}. This critical hyperparameter dictates the
      trade-off between maximizing reward and maximizing entropy. A larger $\alpha$ value places a
      greater emphasis on exploration and randomness, while a smaller $\alpha$ prioritizes reward
      accumulation. Interestingly, $\alpha$ can also be adaptively learned during the training
      process, a common enhancement in SAC implementations.

\end{itemize}

\section{Soft Value Functions: The Foundation of SAC}
\label{sec:soft_value_functions}

From the maximum entropy objective, SAC defines "soft" versions of the traditional value functions, which explicitly incorporate the entropy term.

\subsection{The Soft Q-Function}
\label{subsec:soft_q_function}

The \textbf{soft Q-function}, denoted $Q^\pi(s, a)$, quantifies the expected return from taking
action $a$ in state $s$ and subsequently following policy $\pi$, with the added entropy bonus. Its
Bellman equation is:
\begin{equation}
Q^\pi(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}_{s_{t+1} \sim P, a_{t+1} \sim \pi} \left[ Q^\pi(s_{t+1}, a_{t+1}) - \alpha \log \pi(a_{t+1}|s_{t+1}) \right]
\label{eq:soft_q}
\end{equation}

Notice the crucial distinction from the standard Bellman equation: an entropy term, $\alpha \log
\pi(a_{t+1}|s_{t+1})$, is \textit{subtracted} from the future Q-value. This subtraction effectively
incentivizes the policy to favor actions that lead to states with higher subsequent policy entropy,
thereby promoting diverse behavior.

\subsection{The Soft Value Function}
\label{subsec:soft_value_function}

Similarly, the \textbf{soft value function}, $V^\pi(s)$, represents the expected return from state
$s$ under policy $\pi$, inherently including the entropy bonus:
\begin{equation}
V^\pi(s_t) = \mathbb{E}_{a_t \sim \pi} \left[ Q^\pi(s_t, a_t) - \alpha \log \pi(a_t|s_t) \right]
\label{eq:soft_v}
\end{equation}

This equation explicitly highlights that the value of a state in SAC is not solely based on the
expected Q-value but also receives a bonus proportional to the entropy of the policy within that
state.

\section{Architecture and Training Dynamics} \label{sec:sac_arch}
\label{sec:architecture_training}

SAC typically employs a set of interconnected neural networks and an experience replay buffer to facilitate its learning process.

\subsection{Key Components:}
\label{subsec:key_components}

\begin{enumerate}
    \item \textbf{Actor Network ($\pi_\theta$)}: Parameterized by $\theta$, this network represents the \textbf{stochastic policy}. For continuous action spaces, it commonly outputs the mean and standard deviation of a Gaussian distribution, from which actions are sampled.
    \item \textbf{Critic Networks (Q-functions $Q_{\phi_1}, Q_{\phi_2}$)}: SAC utilizes \textbf{two separate Q-function networks}, parameterized by $\phi_1$ and $\phi_2$. These critics estimate the soft Q-value for a given state-action pair. Employing dual critics is a technique borrowed from \textbf{TD3 (Twin Delayed DDPG)}, designed to mitigate the problem of \textbf{Q-value overestimation}, a common pitfall in deep Q-learning algorithms.
    \item \textbf{Target Critic Networks ($Q'_{\phi_1}, Q'_{\phi_2}$)}: These are smoothed, delayed copies of the primary critic networks. They provide stable targets for Q-value updates, crucial for learning stability. Their parameters are typically updated using \textbf{Polyak averaging} (e.g., $\phi' \leftarrow \rho \phi' + (1-\rho) \phi$, where $\rho$ is a small constant like $0.995$) or periodically copied from the main critics.
    \item \textbf{Replay Buffer}: As an \textbf{off-policy algorithm}, SAC extensively uses a \textbf{replay buffer} to store past transitions (state, action, reward, next state, done flag). This mechanism allows for efficient reuse of historical data, significantly boosting \textbf{sample efficiency} compared to on-policy methods.
\end{enumerate}

\subsection{Training Objectives and Updates:}
\label{subsec:training_objectives}

The training of SAC involves a simultaneous and iterative update of the policy network, the two Q-function networks, and optionally, the temperature parameter.

\begin{enumerate}
    \item \textbf{Q-Network Update (Critic Update)}:
    The Q-networks are trained to minimize the \textbf{soft Bellman residual}. For each Q-network $Q_{\phi_i}$, the loss function is typically a squared error:
    \begin{equation}
    L(\phi_i) = \mathbb{E}_{(s,a,r,s') \sim D} \left[ \left( Q_{\phi_i}(s,a) - y \right)^2 \right]
    \label{eq:sac_loss_critic}
  \end{equation}

    Here, $D$ denotes the experience replay buffer. The \textbf{target value $y$} is computed as:
    \begin{equation}
    y = r + \gamma \left( \min_{j=1,2} Q'_{\phi_j}(s', \tilde{a}') - \alpha \log \pi_\theta(\tilde{a}'|s') \right)
      \label{eq:sac_critic_target}
    \end{equation}

    In this equation, $\tilde{a}'$ is an action \textit{sampled from the current policy}
    $\pi_\theta(\cdot|s')$, and $Q'_{\phi_j}$ are the target Q-networks. The $\min$ operator (a
    feature of clipped double Q-learning) is critical for preventing overestimation bias in the
    Q-value targets.

    \item \textbf{Policy Network Update (Actor Update)}:
    The actor network is updated to maximize the soft Q-value, while simultaneously maintaining a
    high level of policy entropy. The policy objective can be derived from minimizing the KL
    divergence between the policy and a "soft-max" distribution of Q-values. More directly, it's
    often expressed as minimizing:
    \begin{equation}
    J(\theta) = \mathbb{E}_{s \sim D, \epsilon \sim \mathcal{N}(0,1)} \left[ \alpha \log \pi_\theta(\tilde{a}|s) - \min_{j=1,2} Q_{\phi_j}(s, \tilde{a}) \right]
      \label{eq:sac_actor_loss}
    \end{equation}

    A crucial technique used here is the \textbf{reparameterization trick}. For a continuous policy
    (e.g., Gaussian), the sampled action $\tilde{a}$ can be expressed as a deterministic function of
    the policy parameters and a noise variable $\epsilon$. For instance, if the policy outputs mean
    $\mu_\theta(s)$ and standard deviation $\sigma_\theta(s)$, then $\tilde{a} = \mu_\theta(s) +
    \sigma_\theta(s) \odot \epsilon$, where $\epsilon \sim \mathcal{N}(0,1)$. This trick makes the
    objective function differentiable with respect to $\theta$, enabling gradient-based
    optimization.

    \item \textbf{Temperature Parameter Update (Optional)}:
    Many SAC implementations include an adaptive learning mechanism for the temperature parameter
    $\alpha$. This is typically achieved by minimizing a loss function that drives the policy's
    entropy towards a predefined \textbf{target entropy} $H_0$:
    \begin{align}
    L(\alpha) = \mathbb{E}_{s \sim D, a \sim \pi_\theta} \left[ -\alpha (\log \pi_\theta(a|s) + H_0) \right]
      \label{eq:sac_alpha_loss}
    \end{align}

    By learning $\alpha$, the algorithm can automatically adjust the balance between exploration and exploitation throughout the training process, adapting to the specific task's requirements.
\end{enumerate}

\subsection{Soft Actor-Critic Pseudocode}
\label{subsec:sac_pseudocode}

Algorithm \ref{alg:sac} provides a summary of the Soft Actor-Critic training process.

\begin{algorithm}
\caption{Soft Actor-Critic (SAC)}\label{alg:sac}
\begin{algorithmic}[1]
\Require Initial policy parameters $\theta$, Q-function parameters $\phi_1, \phi_2$, target Q-function parameters $\phi_1^{\text{target}}, \phi_2^{\text{target}}$, temperature $\alpha$ (or its initial log value $\log\alpha$), learning rates $\eta_\theta, \eta_\phi, \eta_\alpha$, discount factor $\gamma$, Polyak averaging coefficient $\rho$, replay buffer capacity $M$, target entropy $H_0$ (if learning $\alpha$).
\State Initialize replay buffer $\mathcal{D} \leftarrow \emptyset$
\State Initialize $\phi_1^{\text{target}} \leftarrow \phi_1$, $\phi_2^{\text{target}} \leftarrow \phi_2$
\For{each iteration $k = 1, \dots, N_{\text{iterations}}$}
    \State Sample state $s_t$ from environment
    \State Sample action $a_t \sim \pi_\theta(s_t)$
    \State Execute $a_t$ in environment, observe $r_t, s_{t+1}, \text{done}_t$
    \State Store $(s_t, a_t, r_t, s_{t+1}, \text{done}_t)$ in $\mathcal{D}$
    \For{each gradient step $m = 1, \dots, N_{\text{gradient\_steps}}$}
        \State Sample a batch of transitions $(s, a, r, s', \text{done})$ from $\mathcal{D}$
        \State Compute $\tilde{a}' \sim \pi_\theta(s')$ using reparameterization trick
        \State Compute $\log \pi_\theta(\tilde{a}'|s')$
        \State Compute target Q-values:
        \State $y = r + \gamma (1 - \text{done}) \left( \min(Q_{\phi_1}^{\text{target}}(s', \tilde{a}'), Q_{\phi_2}^{\text{target}}(s', \tilde{a}')) - \alpha \log \pi_\theta(\tilde{a}'|s') \right)$
        \State Update Q-function parameters:
        \State $\phi_1 \leftarrow \phi_1 - \eta_\phi \nabla_{\phi_1} \left( Q_{\phi_1}(s,a) - y \right)^2$
        \State $\phi_2 \leftarrow \phi_2 - \eta_\phi \nabla_{\phi_2} \left( Q_{\phi_2}(s,a) - y \right)^2$
        \State Compute $\tilde{a} \sim \pi_\theta(s)$ using reparameterization trick
        \State Compute $\log \pi_\theta(\tilde{a}|s)$
        \State Update policy parameters:
        \State $\theta \leftarrow \theta - \eta_\theta \nabla_\theta \left( \alpha \log \pi_\theta(\tilde{a}|s) - \min(Q_{\phi_1}(s, \tilde{a}), Q_{\phi_2}(s, \tilde{a})) \right)$
        \If{learning $\alpha$}
            \State Update temperature parameter $\alpha$:
            \State $\log\alpha \leftarrow \log\alpha - \eta_\alpha \nabla_{\log\alpha} \left( -\alpha (\log \pi_\theta(\tilde{a}|s) + H_0) \right)$
        \EndIf
        \State Update target Q-function networks (Polyak averaging):
        \State $\phi_1^{\text{target}} \leftarrow \rho \phi_1^{\text{target}} + (1-\rho) \phi_1$
        \State $\phi_2^{\text{target}} \leftarrow \rho \phi_2^{\text{target}} + (1-\rho) \phi_2$
    \EndFor
\EndFor
\end{algorithmic}
\end{algorithm}

\section{Advantages of Soft Actor-Critic}
\label{sec:advantages}

SAC has gained significant traction in the reinforcement learning community due to several compelling advantages:

\begin{itemize}
    \item \textbf{Exceptional Sample Efficiency}: Thanks to its off-policy nature and reliance on experience replay, SAC can effectively reuse collected data. This is a major benefit in scenarios where data acquisition is costly or time-consuming, such as in robotics.
    \item \textbf{Enhanced Stability}: The explicit \textbf{entropy regularization} helps to stabilize the training process. By encouraging diverse actions, SAC is less prone to collapsing into suboptimal deterministic policies or exhibiting brittle behavior due to minor estimation errors in value functions.
    \item \textbf{State-of-the-Art Performance}: SAC has consistently demonstrated top-tier performance across a wide array of continuous control tasks, ranging from simulated locomotion to complex robotic manipulation.
    \item \textbf{Robustness}: The combination of the maximum entropy framework, the use of dual critics, and often, an adaptively learned temperature parameter, contributes to SAC's robustness against varying hyperparameter choices and environmental stochasticity.
\end{itemize}

\section{Implementing SAC Algorithm using Python}
In this section, we will discuss the details of implementing the SAC algorithm using Tensorflow 2.x
and Python. SAC is implemented using an actor-critic architecture where the actor is to used
estimate the action policies while a critic is used to evaluate the actor by estimating a Q or value
function. The details are discussed in the following subsections.

\subsection{Actor Network}
As stated in section \ref{sec:sac_arch}, the actor learns a stochastic policy. The actor uses a
sequential deep network to estimate mean ($\mu$) and log of standard deviation ($\sigma$),
\texttt{log\_std}, for a given
input state. It is also possible to output standard deviation \texttt{std} instead of
\texttt{log\_std}. It is important to note that the \texttt{log\_std} value is clipped in the range
$(-20.0, 2)$ to avoid instability. The method \texttt{sample\_action()} is used to sample an action
from a Gaussian distribution created using the output of the actor network $(\mu, \sigma)$. During
the training phase, a reparameterization trick is used that adds noise to a deterministic policy to
improve exploration.  It is also important to avoid numerical instability (\texttt{NaN} values) by
ensuring that actor output is quashed by using a $\tanh()$ function and scaled to its actual range.
Similarly, the values to the \texttt{log\_prob} function is quashed to a range (0, 1) to avoid zero
values or negative values which can result in a NaN value. 


\begin{listing}
  \begin{small}
  \begin{pygments}[frame=single, indent=L]{python}
class Actor():
    def __init__(self, obs_shape, action_shape,
                 learning_rate=0.0001, 
                 action_upper_bound=1.0,
                model=None):
        self.obs_shape = obs_shape
        self.action_size = action_shape[0]
        self.lr = learning_rate
        self.max_action = action_upper_bound

        if model is None:
            self.model = self._build_model()
        else:
            self.model = tf.keras.models.clone_model(model)
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.lr)

        # Constraints for log_std to ensure numerical stability 
        # and reasonable exploration
        # log_std values are usually clipped to a range like [-20, 2]        
        self.max_log_std = tf.constant(2.0, dtype=tf.float32)
        self.min_log_std = tf.constant(-20.0, dtype=tf.float32)
        
    def _build_model(self):
        x = tf.keras.layers.Input(shape=self.obs_shape)
        f = tf.keras.layers.Dense(256, activation='relu',
                kernel_initializer=tf.keras.initializers.HeUniform())(x)
        f = tf.keras.layers.Dense(256, activation='relu',
                kernel_initializer=tf.keras.initializers.HeUniform())(f)
        mu = tf.keras.layers.Dense(self.action_size, activation=None,
                kernel_initializer=tf.keras.initializers.HeUniform())(f)
        log_std = tf.keras.layers.Dense(self.action_size, activation=None,
                kernel_initializer=tf.keras.initializers.HeUniform())(f)
        model = tf.keras.models.Model(inputs=x, 
               outputs=[mu, log_std], name='actor')
        model.summary()
        return model
    
    def __call__(self, states):
        mean, log_std = self.model(states)
        log_std = tf.clip_by_value(log_std, self.min_log_std, 
                                       self.max_log_std)
        return mean, log_std

    def sample_action(self, state, reparameterize=False):
        mean, log_std = self(state)
        std = tf.exp(log_std)
        dist = tfp.distributions.Normal(mean, std)

        if reparameterize: # Reparameterization trick
            z = mean + std * tf.random.normal(tf.shape(mean))
        else:
            z = dist.sample()

        action = tf.tanh(z) * self.max_action 
        squash = 1 - tf.math.pow(tf.tanh(z), 2) # squash values to (0,1)
        squash = tf.clip_by_value(squash, 1e-6, 1.0)  # Avoid division by zero
        log_prob = dist.log_prob(z) 
        log_prob -= tf.math.log(squash)
        log_prob = tf.math.reduce_sum(log_prob,  axis=-1, keepdims=True)
        return action, log_prob
  \end{pygments}
\end{small}
  \caption{Python code for the SAC Actor Class}
  \label{lst:sac_actor}
\end{listing}

\subsection{Critic Network}
The \texttt{Critic} class estimates the Q-function $Q(s,a)$ by using a sequential deep network from
the input state and action values. The output is a scalar value. 

\begin{listing}
  \begin{small}
  \begin{pygments}[frame=single, indent=L]{python}
class Critic:
    """ Approximates Q(s,a) function """
    def __init__(self, obs_shape, action_shape, 
                 learning_rate=1e-3,
                 model=None):
        self.obs_shape = obs_shape    # shape: (m,)
        self.action_shape = action_shape  # shape: (n, )
        self.lr = learning_rate        
        # create NN model
        if model is None:
            self.model = self._build_net()
        else:
            self.model = tf.keras.models.clone_model(model)
        self.optimizer = tf.keras.optimizers.Adam(self.lr)

    def _build_net(self):
        state_input = tf.keras.layers.Input(shape=self.obs_shape)
        action_input = tf.keras.layers.Input(shape=self.action_shape)
        concat = tf.keras.layers.Concatenate()([state_input, action_input])
        out = tf.keras.layers.Dense(256, activation='relu',
           kernel_initializer=tf.keras.initializers.HeUniform())(concat)
        out = tf.keras.layers.Dense(256, activation='relu',
           kernel_initializer=tf.keras.initializers.HeUniform())(out)
        out = tf.keras.layers.Dense(256, activation='relu',
           kernel_initializer=tf.keras.initializers.HeUniform())(out)
        net_out = tf.keras.layers.Dense(1)(out)
        model = tf.keras.Model(inputs=[state_input, action_input], 
            outputs=net_out, name='critic')
        model.summary()
        return model

    def __call__(self, states, actions):
        """ Returns Q(s,a) value """
        return self.model([states, actions])
    
  \end{pygments}
\end{small}
  \caption{Python code for the SAC Critic class definition}
  \label{sec:sac_critic}
\end{listing}

\subsection{SAC Agent}
The \texttt{SACAgent} class creates two critic models \texttt{self.critic\_1} and
\texttt{self.critic\_2} and the corresponding two target models \texttt{self.target\_critic\_1} and
\texttt{self.target\_critic\_2} respectively, one actor model \texttt{self.actor} and a replay
buffer \texttt{self.buffer}. The target critic networks and the primary critic networks share the
same weights in the beginning. The target weights are updated slowly over time using Polyak's
averaging method. The temperature parameter $\alpha$ is also updated to minimize the loss function
given by equation \eqref{eq:sac_alpha_loss}. The critic parameters are updated to minimize the soft
Bellman residual loss functions given by equation \eqref{eq:sac_loss_critic}. The actor parameters
are updated to maximize the loss function given by equation \eqref{eq:sac_actor_loss} which
essentially tries to maximize the soft Q function while maintaining a high level of policy
entropy. The performance of SAC algorithm can be improved by having multiple critic update steps for
each actor update step. This helps in learning the Q function faster leading to better training
stability and improved performance. 


\begin{listing}
  \begin{scriptsize}
  \begin{pygments}[frame=single, indent=L]{python}
class SACAgent:
    def __init__(self, obs_shape, action_shape, 
                 lr_a=1e-4, lr_c=3e-4, 
                 lr_alpha=3e-4, alpha=0.2,
                 gamma=0.99, polyak=0.999, 
                 action_upper_bound=1.0,
                 buffer_size=1000000, 
                 batch_size=256,
                 reward_scale=1.0,
                 max_grad_norm=None, # required for gradient clipping
                 actor_model=None,
                 critic_model=None ):

        self.obs_shape = obs_shape
        self.action_shape = action_shape
        self.gamma = gamma  # Discount factor
        self.polyak = polyak # Polyak averaging coefficient
        self.batch_size = batch_size
        self.buffer_size = buffer_size
        self.actor_lr = lr_a
        self.critic_lr = lr_c
        self.alpha_lr = lr_alpha
        self.reward_scale = reward_scale
        self.max_grad_norm = max_grad_norm
        self.name = 'SAC'

        # Initialize actor and critic networks
        self.actor = Actor(obs_shape, action_shape, self.actor_lr, 
                           action_upper_bound, model=actor_model)
        self.critic_1 = Critic(obs_shape, action_shape, self.critic_lr, 
                                                   model=critic_model)
        self.critic_2 = Critic(obs_shape, action_shape, self.critic_lr, 
                                                model=critic_model)

        # Target networks for soft updates
        self.target_critic_1 = Critic(obs_shape, action_shape, 
                           self.critic_lr, model=critic_model)
        self.target_critic_2 = Critic(obs_shape, action_shape, 
                           self.critic_lr, model=critic_model)

        # make alpha a trainable variable
        self.log_alpha = tf.Variable(tf.math.log(alpha), 
            dtype=tf.float32, trainable=True, name='log_alpha')
        self.alpha_optimizer = tf.keras.optimizers.Adam(
                                 learning_rate=self.alpha_lr)

        # Target entropy for action space 
        self.target_entropy = tf.constant(-np.prod(self.action_shape), 
                                                   dtype=tf.float32)  

        # Replay buffer
        self.buffer = ReplayBuffer(self.buffer_size)  

        # Initialize target networks with the same weights as the main networks
        self.target_critic_1.model.set_weights(
                     self.critic_1.model.get_weights())
        self.target_critic_2.model.set_weights(
                     self.critic_2.model.get_weights())

    def update_target_networks(self):
        """
        Soft update target networks using Polyak averaging
        """
        for target_var, var in zip(self.target_critic_1.model.trainable_variables, 
                                self.critic_1.model.trainable_variables):
            target_var.assign(self.polyak * target_var + (1 - self.polyak) * var)

        for target_var, var in zip(self.target_critic_2.model.trainable_variables, 
                                self.critic_2.model.trainable_variables):
            target_var.assign(self.polyak * target_var + (1 - self.polyak) * var)

    def choose_action(self, state, evaluate=False):
        """
        Choose an action based on the current state
        """
        state = tf.expand_dims(tf.convert_to_tensor(state, dtype=tf.float32), axis=0)
        action, _ = self.actor.sample_action(state, reparameterize=not evaluate)
        action = tf.squeeze(action, axis=0)  # Remove batch dimension
        return action.numpy()

    def store_transition(self, state, action, reward, next_state, done):
        """
        Store a transition in the replay buffer
        : inputs are numpy arrays
        """
        self.buffer.add((state, action, reward, next_state, done))

    def update_critic(self, states, actions, rewards, next_states, dones):
        """
        Update the critic networks using the sampled transitions
        : inputs are tensors
        """

        with tf.GradientTape(persistent=True) as tape:
            # predict next_actions and log_probs for next states
            next_actions, next_log_probs = self.actor.sample_action(next_states) 

            # compute target Q-values using the target critic networks
            target_q1 = self.target_critic_1(next_states, next_actions)
            target_q2 = self.target_critic_2(next_states, next_actions)
            min_target_q_next = tf.minimum(target_q1, target_q2) 

            # Soft Bellman backup equation for target Q-value
            # y = r + gamma * (1 - done) * (min_Q_target(s', a') - alpha * log_pi(a'|s'))
            target_q_values = self.reward_scale * rewards + \
                  self.gamma * (tf.ones_like(dones) - dones) * \
                  (min_target_q_next - tf.exp(self.log_alpha) * next_log_probs)
            
            # compute current Q-values
            current_q1 = self.critic_1(states, actions)
            current_q2 = self.critic_2(states, actions)

            # compute critic losses
            critic_1_loss = tf.reduce_mean(tf.square(current_q1 - target_q_values))
            critic_2_loss = tf.reduce_mean(tf.square(current_q2 - target_q_values))

        # compute gradients for critic networks
        critic_1_grads = tape.gradient(critic_1_loss, self.critic_1.model.trainable_variables)
        critic_2_grads = tape.gradient(critic_2_loss, self.critic_2.model.trainable_variables)  

        if self.max_grad_norm is not None:
            # Clip gradients to avoid exploding gradients
            critic_1_grads, _ = tf.clip_by_global_norm(critic_1_grads, self.max_grad_norm)
            critic_2_grads, _ = tf.clip_by_global_norm(critic_2_grads, self.max_grad_norm)

        # apply gradients to the critic networks if gradients are not None
        if critic_1_grads is not None:
            self.critic_1.optimizer.apply_gradients(zip(critic_1_grads, 
                                            self.critic_1.model.trainable_variables))
        if critic_2_grads is not None:
            self.critic_2.optimizer.apply_gradients(zip(critic_2_grads, 
                                            self.critic_2.model.trainable_variables))

        mean_c_loss = (critic_1_loss + critic_2_loss) / 2.0
        return mean_c_loss

    def update_actor(self, states):
        """
        Update the actor network
        inputs are tensors
        outputs: actor_loss and alpha_loss
        """

        with tf.GradientTape(persistent=True) as tape:
            # Sample actions and log probabilities for current states
            new_actions, log_probs = self.actor.sample_action(states)

            # Compute Q-values for the sampled actions
            q1_new = self.critic_1(states, new_actions)
            q2_new = self.critic_2(states, new_actions)
            min_q = tf.minimum(q1_new, q2_new)

            # Actor loss is the mean of the Q-values minus the entropy term
            # Actor loss (maximize soft Q-value, incorporating entropy)
            # J_pi = E_s,a~pi [alpha * log_pi(a|s) - Q(s,a)] -> minimize -J_pi
            actor_loss = tf.reduce_mean(tf.exp(self.log_alpha) * log_probs - min_q)

            # alpha loss is computed as the mean of the target entropy minus the log probability
            alpha_loss = tf.reduce_mean(tf.negative(self.log_alpha) * \
                                          (log_probs + self.target_entropy))

        # Compute gradients for the actor network
        actor_grads = tape.gradient(actor_loss, self.actor.model.trainable_variables)
        # Compute gradients for alpha
        alpha_grads = tape.gradient(alpha_loss, [self.log_alpha])

        if self.max_grad_norm is not None:
            # Clip gradients to avoid exploding gradients
            actor_grads, _ = tf.clip_by_global_norm(actor_grads, self.max_grad_norm)
            alpha_grads, _ = tf.clip_by_global_norm(alpha_grads, self.max_grad_norm)

        # Apply gradients to the actor network if gradients are not None
        if actor_grads is not None:
            self.actor.optimizer.apply_gradients(zip(actor_grads, 
                                            self.actor.model.trainable_variables)) 

        # Apply gradients to alpha if gradients are not None
        if alpha_grads is not None: 
            self.alpha_optimizer.apply_gradients(zip(alpha_grads, [self.log_alpha]))
        return actor_loss, alpha_loss

    def train(self, update_per_step=1):
        if len(self.buffer) < self.batch_size:
            return 0, 0, 0

        c_losses, a_losses, alpha_losses = [], [], []
        for _ in range(update_per_step):

            # Sample a batch of transitions from the replay buffer
            states, actions, rewards, next_states, dones = \
                  self.buffer.sample_unpacked(self.obs_shape, self.action_shape, self.batch_size)

            states = tf.convert_to_tensor(states, dtype=tf.float32)
            actions = tf.convert_to_tensor(actions, dtype=tf.float32)       
            rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)
            next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)
            dones = tf.convert_to_tensor(dones, dtype=tf.float32)

            # Update Critic networks
            critic_loss = self.update_critic(states, actions, rewards, next_states, dones)
            actor_loss, alpha_loss = self.update_actor(states)

            # Update target networks
            self.update_target_networks()

            c_losses.append(critic_loss)
            a_losses.append(actor_loss)
            alpha_losses.append(alpha_loss)
        mean_critic_loss = tf.reduce_mean(c_losses)
        mean_actor_loss = tf.reduce_mean(a_losses)
        mean_alpha_loss = tf.reduce_mean(alpha_losses)
        return mean_critic_loss, mean_actor_loss, mean_alpha_loss
  \end{pygments}
\end{scriptsize}
\caption{Python code for implementing SAC agent class}
\label{lst:sac_agent}
\end{listing}

\subsection{Replay Buffer}
As mentioned before, SAC is an off-policy algorithm where the experiences are first stored in a replay
buffer and then sampled in batches during training phase. The replay buffer used here is the same
one discussed in Section \ref{sec:replay_buffer}. The Python class for implementing replay buffer is
provided in the code listing \ref{lst:replay_buffer}. We augment this class by providing an
additional method \texttt{sample\_unpacked()} that unpacks the samples into 

\begin{listing}
  \begin{scriptsize}
  \begin{pygments}[frame=single, indent=L]{python}
    def sample_unpacked(self, obs_shape, action_shape, batch_size=24):
        """ Returns a batch of experiences as a tuple of numpy arrays.
        Input: 
            batch_size: int
            obs_shape: tuple, shape of the observation space
        returns: (states, actions, rewards, next_states, dones) """

        mini_batch = self.sample(batch_size)
        assert len(mini_batch[0]) == 5, "Each experience tuple must have 5 elements: (s,a,r,s',d)"
        states = np.zeros((batch_size, *obs_shape))
        next_states = np.zeros((batch_size, *obs_shape))
        actions = np.zeros((batch_size, *action_shape))
        rewards = np.zeros((batch_size, 1))
        dones = np.zeros((batch_size, 1))

        for i in range(len(mini_batch)):
            states[i] = mini_batch[i][0]
            actions[i] = mini_batch[i][1]
            rewards[i] = mini_batch[i][2]
            next_states[i] = mini_batch[i][3]
            dones[i]  = mini_batch[i][4]
        return states, actions, rewards, next_states, dones
  \end{pygments}
\end{scriptsize}
  \caption{Method to sample experiences from the replay buffer and convert them into numpy arrays}
  \label{lst:sample_unpacked}
\end{listing}


\subsection{Value Network}
The above implementation uses the minimum of two target critics minus the entropy term as the target
for training the critic networks. In practice, this target can be noisy or biased early in training
when the critics are not yet accurate. This can destabilize the learning process. This can be
avoided by using a separate value network to provide a smoother target for critic training. The use
a separate value network reduces variances and stabilizes learning. The Python code for creating a
value network class is provided in code listing \ref{lst:value_net}.

\begin{listing}
  \begin{small}
  \begin{pygments}[frame=single, indent=L]{python}
class ValueNetwork():
    """  Approximates V(s) function   """
    def __init__(self, obs_shape, 
                 learning_rate=1e-3,
                 model=None):
        self.obs_shape = obs_shape
        self.lr = learning_rate
        # create NN model
        if model is None:
            self.model = self._build_net()
        else:
            self.model = tf.keras.models.clone_model(model)
        self.optimizer = tf.keras.optimizers.Adam(self.lr)

    def _build_net(self):
        inp = tf.keras.layers.Input(shape=self.obs_shape)
        out = tf.keras.layers.Dense(256, activation='relu',
                kernel_initializer=tf.keras.initializers.HeUniform())(inp)
        out = tf.keras.layers.Dense(256, activation='relu',
                kernel_initializer=tf.keras.initializers.HeUniform())(out)
        out = tf.keras.layers.Dense(256, activation='relu',
                kernel_initializer=tf.keras.initializers.HeUniform())(out)
        net_out = tf.keras.layers.Dense(1)(out)
        model = tf.keras.Model(inputs=inp, outputs=net_out, name='value_network')
        model.summary()
        return model

    def __call__(self, states):
        """ Returns V(s) value  """
        v = self.model(states)  
        return v  
  \end{pygments}
\end{small}
  \caption{Python class for creating a value network}
  \label{lst:value_net}
\end{listing}

\subsection{SAC Agent with a separate value network}
The SAC agent class remains mostly same as before with the following changes. The agent has one
actor model, two critic models, a replay buffer and one primary value model and a target value
model. The two critics are updated by using value network to compute the q-target values. It reduces
variance and enhances the stability of the algorithm.  The target value network is updated using
Polyak averaging. We call this agent as 'SAC2' agent to differentiate from the previous
implementation that does not use a value network.


\begin{listing}
  \begin{scriptsize}
  \begin{pygments}[frame=single, indent=L]{python}
class SACAgent:
    """  Soft Actor-Critic Agent """
    def __init__(self, obs_shape, action_shape,
               action_upper_bound=1.0, <other args ...>):
        self.obs_shape = obs_shape
        self.action_shape = action_shape
        self.action_size = action_shape[0]
        self.action_upper_bound = action_upper_bound
        self.batch_size = batch_size
        self.name = 'SAC2'
        self.target_entropy = -np.prod(action_shape)  # Target entropy = -|A|
        <snip>
        
        # Initialize networks
        self.actor = Actor(obs_shape, action_shape, lr_a, 
                                 action_upper_bound, model=actor_model)
        self.critic_1 = Critic(obs_shape, action_shape, 
                                 learning_rate=self.actor_lr, model=critic_model)
        self.critic_2 = Critic(obs_shape, action_shape, 
                                 learning_rate=self.critic_lr, model=critic_model)
        self.value_network = ValueNetwork(obs_shape, 
                                 learning_rate=self.critic_lr, model = value_model)

        # create a replay buffer
        self.buffer = ReplayBuffer(self.buffer_size)

        # Target networks for stability
        self.target_value_network = ValueNetwork(obs_shape, learning_rate=self.critic_lr)
        self.target_value_network.model.set_weights(self.value_network.model.get_weights())

        # Initialize alpha (temperature parameter)
        self.log_alpha = tf.Variable(tf.math.log(alpha), trainable=True, dtype=tf.float32)
        self.alpha_optimizer = tf.keras.optimizers.Adam(learning_rate=self.alpha_lr)


    def choose_action(self, state, evaluate=False):
        """ same as before """  
        pass
    
    def store_transition(self, state, action, reward, next_state, done):
        """ same as before """
        pass

    def update_target_networks(self):
        """ Update only target value network using Polyak averaging   """
        for target_var, var in zip(self.target_value_network.model.trainable_variables,
                                   self.value_network.model.trainable_variables):
            target_var.assign(self.polyak * target_var + (1 - self.polyak) * var)
            
    def update_value_network(self, states, next_states):
        with tf.GradientTape() as tape:
            next_actions, next_log_probs = self.actor.sample_action(next_states)
            next_q1 = self.critic_1(next_states, next_actions)
            next_q2 = self.critic_2(next_states, next_actions)
            next_q = tf.minimum(next_q1, next_q2)
            value_target = next_q - tf.exp(self.log_alpha) * next_log_probs
            value = self.value_network(states)
            value_loss = tf.reduce_mean(tf.square(value - value_target))

        value_grads = tape.gradient(value_loss, self.value_network.model.trainable_variables)

        if self.max_grad_norm is not None:
            value_grads, _ = tf.clip_by_global_norm(value_grads, self.max_grad_norm)

        if value_grads is not None:
            # Apply gradients to the value network
            self.value_network.optimizer.apply_gradients(zip(value_grads, \
                                            self.value_network.model.trainable_variables))
        return value_loss 

    def update_critic(self, states, actions, rewards, next_states, dones):
        with tf.GradientTape(persistent=True) as tape:
            q1 = self.critic_1(states, actions)
            q2 = self.critic_2(states, actions)
            next_v = self.target_value_network(next_states)
            target_q = self.reward_scale * rewards + self.gamma * (1 - dones) * next_v
            critic_1_loss = tf.reduce_mean(tf.square(target_q - q1))
            critic_2_loss = tf.reduce_mean(tf.square(target_q - q2))

        critic_1_grads = tape.gradient(critic_1_loss, self.critic_1.model.trainable_variables)
        critic_2_grads = tape.gradient(critic_2_loss, self.critic_2.model.trainable_variables)

        if self.max_grad_norm is not None:
            critic_1_grads, _ = tf.clip_by_global_norm(critic_1_grads, self.max_grad_norm)
            critic_2_grads, _ = tf.clip_by_global_norm(critic_2_grads, self.max_grad_norm)

        if critic_1_grads is not None:
            self.critic_1.optimizer.apply_gradients(zip(critic_1_grads, \
                                            self.critic_1.model.trainable_variables))
        if critic_2_grads is not None:
            self.critic_2.optimizer.apply_gradients(zip(critic_2_grads, \
                                            self.critic_2.model.trainable_variables))
        return critic_1_loss, critic_2_loss

    def update_actor(self, states):
        """ same as before """  
        pass

    def train(self):
        """
        Train the agent using a batch of transitions from the replay buffer
        """

        if len(self.buffer) < self.batch_size:
            return 0, 0, 0, 0
        
        # Sample a batch of transitions from the replay buffer
        states, actions, rewards, next_states, dones = \
               self.buffer.sample_unpacked(self.obs_shape,
                           self.action_shape, self.batch_size)
        # Convert to tensors
        states = tf.convert_to_tensor(states, dtype=tf.float32)
        actions = tf.convert_to_tensor(actions, dtype=tf.float32)
        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)
        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)
        dones = tf.convert_to_tensor(dones, dtype=tf.float32)

        # Update value network
        value_loss = self.update_value_network(states, next_states)

        # Update critic networks
        critic_1_loss, critic_2_loss = self.update_critic(states, actions, 
                                             rewards, next_states, dones)

        critic_loss = (critic_1_loss + critic_2_loss)/2.0 

        # Update actor network
        actor_loss, alpha_loss = self.update_actor(states)

        # Update target networks
        self.update_target_networks()
        return value_loss, critic_loss, actor_loss, alpha_loss
  \end{pygments}
\end{scriptsize}
  \caption{SAC agent class with a separate value network}
  \label{lst:sac_agent_2}
\end{listing}


\subsection{Training a SAC agent}
The function for training a SAC agent on a given problem environment is provided in the code listing
\ref{lst:train_sac}. The agent generates action as per its own policy and saves the experiences in a
replay buffer. 

\begin{listing}
  \begin{scriptsize}
  \begin{pygments}[frame=single, indent=L]{python}
def train_sac_agent(env, agent, num_episodes=1500, 
                    stop_score=-200, 
                    warmup_steps=1000,
                    update_per_step=1,
                    log_freq=100):
    
    print('Environment name: ', env.spec.id)
    print('RL Agent name:', agent.name)
    
    ep_scores = []
    best_score = -np.inf
    total_steps = 0
    a_loss, c_loss, alpha_loss = 0, 0, 0 
    if SAC2:
        v_loss = 0
    for e in range(num_episodes):
        done = False
        truncated = False
        state = env.reset()[0]
        ep_score = 0
        ep_steps = 0
        c_losses, a_losses, alpha_losses = [], [], []
        if SAC2:    
            v_losses = []
        while not done and not truncated:
            
            if total_steps < warmup_steps:
                action = env.action_space.sample()
            else:
                # Use the agent's policy to select an action
                action = agent.choose_action(state)
                
            next_state, reward, done, truncated, _ = env.step(action)
            agent.store_transition(state, action, reward, next_state, done)

            state = next_state
            ep_score += reward
            ep_steps += 1
            total_steps += 1

            # train the agent
            if total_steps >= warmup_steps: 
                c_l, a_l, ap_l = agent.train(update_per_step=update_per_step)   
                c_losses.append(c_l)
                a_losses.append(a_l)
                alpha_losses.append(ap_l)            
        # while loop ends here - end of episode
        ep_scores.append(ep_score)
        c_loss = np.mean(c_losses) 
        a_loss = np.mean(a_losses) 
        alpha_loss = np.mean(alpha_losses) 

        if e % log_freq == 0:
            print(f'e:{e}, ep_score:{ep_score:.2f}, avg_ep_score:{np.mean(ep_scores):.2f},\
            avg100score:{np.mean(ep_scores[-100:]):.2f}, \
                best_score:{best_score:.2f}')
        
        if ep_score > best_score:
            best_score = ep_score
            agent.save_weights(filename='ll_sac.weights.h5')
            print(f'Best Score: {ep_score:.2f}, episode: {e}. Model saved.')

        if np.mean(ep_scores[-100:]) > stop_score:
            print('The problem is solved in {} episodes'.format(e))
            break
    # for loop ends here
  \end{pygments}
\end{scriptsize}
  \caption{Function for training a SAC agent on a given problem environment}
  \label{lst:train_sac}
\end{listing}

\subsection{Solving \texttt{Pendulum-v1} Problem using SAC}
The main code for training the SAC agent on \texttt{Pendulum-v1} is provided in the code listing
\ref{lst:pendu_sac}. It is an environment with continuous action space. The training performance of the SAC
agent for this environment is shown in Figure \ref{fig:pendu_sac2}. As one can see, the problem is
solved in about 100 episodes when the average of last 100 episodes (\texttt{Avg100Score}) exceeds a
value of -200. The best episodic score achieved is about -0.6. 

\begin{listing}
  \begin{scriptsize}
  \begin{pygments}[frame=single, indent=L]{python}
import gymnasium as gym
from sac import SACAgent
if __name__ == "__main__":
    # Create the gym environment
    env = gym.make('Pendulum-v1', g=9.81)

    obs_shape = env.observation_space.shape
    action_shape = env.action_space.shape
    print(f"Observation shape: {obs_shape}, Action shape: {action_shape}")
    action_upper_bound = env.action_space.high[0]  # Assuming continuous action space
    print(f"Action upper bound: {action_upper_bound}")
    print(f"Action lower bound: {env.action_space.low}")

    # Initialize the SAC agent
    agent = SACAgent(obs_shape, action_shape,
                     action_upper_bound=action_upper_bound,
                     reward_scale=1.0,
                     buffer_size=1000000,
                     batch_size=256,
                     max_grad_norm=None)


    # train the agent
    train_sac_agent(env, agent, num_episodes=1500,
                    stop_score=-200)
    
  \end{pygments}
\end{scriptsize}
  \begin{framed}
    \begin{scriptsize}
    \begin{verbatim}
Output:
Observation shape: (3,), Action shape: (1,)
Action upper bound: 2.0
Action lower bound: [-2.]
Environment name:  Pendulum-v1
RL Agent name: SAC
e:0, ep_score:-773.44, avg_ep_score:-773.44, avg100score:-773.44, best_score:-inf
Best Score: -773.44, episode: 0. Model saved.
Best Score: -662.23, episode: 50. Model saved.
Best Score: -522.63, episode: 55. Model saved.
Best Score: -388.88, episode: 56. Model saved.
Best Score: -261.68, episode: 61. Model saved.
Best Score: -132.35, episode: 62. Model saved.
Best Score: -129.37, episode: 66. Model saved.
Best Score: -129.07, episode: 68. Model saved.
Best Score: -127.75, episode: 74. Model saved.
Best Score: -126.07, episode: 76. Model saved.
Best Score: -1.27, episode: 79. Model saved.
Best Score: -1.17, episode: 83. Model saved.
e:100, ep_score:-120.53, avg_ep_score:-788.01, avg100score:-788.15, best_score:-1.17
Best Score: -1.13, episode: 109. Model saved.
Best Score: -0.98, episode: 116. Model saved.
Best Score: -0.85, episode: 122. Model saved.
The problem is solved in 154 episodes
    \end{verbatim}
  \end{scriptsize}
  \end{framed}
  \caption{Main program for solving \texttt{Pendulum-v1} using SAC}
  \label{lst:pendu_sac}
\end{listing}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.15]{./figures/chap08/pendulum_v1_sac.png}
  \caption{Training performance of SAC2 agent for solving \texttt{Pendulum-v1} problem}
  \label{fig:pendu_sac2}
\end{figure}

\subsection{Solving \texttt{LunarLander-v3-Continuous} Problem using SAC}
\index{Environment!Gym!LunarLander-Continuous-v3}
In this version of the environment, both the observation and the action spaces are continuous. The
specific details of the environment is provided in Table \ref{tab:llv3cont_param}. An episode is
considered a solution if it scores at least 200 points. The main code for training a SAC agent to
solve this problem is provide in the listing \ref{lst:llv3cont_sac}. The training performance of the
SAC agent on this environment is shown in Figure \ref{fig:llv3cont_sac}. As one can observe, the
agent successfully solves this problem by achieving an \texttt{avg100score} of about 200 and best
episodic score of about 300 by training over 350 episodes.  


\begin{table*}[htbp]
  \caption{\small Parameters for \texttt{LunarLanderContinuous-v3} Environment}
  \label{tab:llv3cont_param}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{|c|c|c|c|c|c|}\hline
    Variables & Shape & Type & Elements & Min & Max \\ \hline 
    Observation, state $s$ & (8,) & Box (Continuous) & {\scriptsize Position $(x,y,\theta)$,
    and velocities $(\dot{x}, \dot{y},
  \dot{\theta})$, two booleans representing leg contact with ground} & -10 & 10 \\ \hline
  Action, $a$ & (2, ) & Box (Continuous) & {\scriptsize Throttle of left and right engines} & -1 & 1 \\ \hline
  Reward, $r$ & () & - & {\scriptsize Variable reward is granted for each step} & - & - \\ \hline 
\end{tabular}}
\end{table*}
\begin{listing}
  \begin{scriptsize}
  \begin{pygments}[frame=single, indent=L]{python}
import gymnasium as gym
from sac2 import SACAgent
if __name__ == "__main__":
    # Create the LunarLanderContinuous-v3 environment
    env = gym.make('LunarLander-v3', continuous=True)

    obs_shape = env.observation_space.shape
    action_shape = env.action_space.shape
    print(f"Observation shape: {obs_shape}, Action shape: {action_shape}")
    action_upper_bound = env.action_space.high  # Assuming continuous action space
    print(f"Action upper bound: {action_upper_bound}")
    print(f"Action lower bound: {env.action_space.low}")

    # Initialize the SAC agent
    agent = SACAgent(obs_shape, action_shape,
                 buffer_size=1000000,
                 batch_size=256,
                 action_upper_bound=action_upper_bound,
                 reward_scale=1.0,
                 lr_a=0.001, lr_c=1e-4, lr_alpha=1e-4,
                 polyak=0.995)

    # train the agent
    train_sac_agent(env, agent, num_episodes=1500,
                    max_score=500, min_score=-300,
                    stop_score=200,
                    update_per_step=1,
                    wandb_log=True)
    
  \end{pygments}
\end{scriptsize}
\begin{framed}
  \begin{scriptsize}
  \begin{verbatim}
Environment name:  LunarLander-v3
RL Agent name: SAC2
e:0, ep_score:-304.12, avg_ep_score:-304.12, avg100score:-304.12, best_score:-inf
Best Score: -304.11826608741774, episode: 0. Model saved.
Best Score: -64.66004638633584, episode: 2. Model saved.
Best Score: -58.68014608926175, episode: 17. Model saved.
Best Score: 232.4166849492071, episode: 29. Model saved.
Best Score: 261.54218244093954, episode: 79. Model saved.
e:100, ep_score:-58.37, avg_ep_score:-92.34, avg100score:-90.22, best_score:261.54
Best Score: 276.6843418882439, episode: 110. Model saved.
Best Score: 295.11690047683317, episode: 180. Model saved.
Best Score: 306.79686798603245, episode: 192. Model saved.
e:200, ep_score:262.25, avg_ep_score:-16.02, avg100score:61.06, best_score:306.80
e:300, ep_score:252.19, avg_ep_score:43.81, avg100score:164.06, best_score:306.80
Best Score: 314.70652883744344, episode: 330. Model saved.
The problem is solved in 358 episodes
  \end{verbatim}
\end{scriptsize}
\end{framed}
  \caption{Code for training SAC agent to solve \texttt{LunarLanderContinous-v3} environment}
  \label{lst:llv3cont_sac}
\end{listing}

\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.15]{./figures/chap08/lunarlander-v3-cont-sac.png}
  \caption{Training performance of SAC agent on \texttt{LunarLanderContinous-v3} environment}
  \label{fig:llv3cont_sac}
\end{figure}

\subsection{Solving \texttt{FetchReachDense-v3} problem with SAC}
\index{Environment!Gym!Fetch Reach}
This environment was introduced in \cite{plappert2018multi} and is now available with Gymnasium
\cite{fetchreachgym}. The task in the environment is for a manipulator to move the end effector to a randomly selected
position in the robots workspace. The robot is a 7-DoF Fetch Mobile Manipulator with a two-fingered
parallel gripper. The robot is controlled by small displacements of the gripper in Cartesian
coordinates and the inverse kinematics are computed internally by the
MuJoCo\footnote[1]{\url{https://mujoco.org/}} framework. The task is also continuing which means
that the robot has to maintain the end effectors position for an indefinite period of time. The
observation space and the action space for this environment are continuous making it a challenging
problem to solve. The specific details about the environment is provided in Table
\ref{tab:freach_param}. Few screenshots of the problem environment is shown in Figure
\ref{fig:fr_obs}. The last picture shows the case when the end-effector successfully reaches the
desired goal location (shown as a red dot).  The main Python code for training a SAC agent on this environment
is provided in Listing \ref{lst:frd_sac}. The training performance of the SAC agent on
\texttt{FetchReachDense-v3} environment is shown in Figure \ref{fig:freach_sac}. It can be seen that
the agent achieves an \texttt{average100score} of about -19 and the best score of about -6 after
training over 1400 episodes. Ideally, average episodic score should be 0 for successfully solving
this problem. The corresponding loss functions of the SAC agent is shown in Figure
\ref{fig:frd_sac_loss}. The value and critic losses are minimized and,  actor and alpha losses are maximized
during training as is being seen in this plot.  

\begin{table*}[htbp]
  \caption{\small Parameters for \texttt{FetchReachDense-v3} Environment}
  \label{tab:freach_param}
  \centering
  \resizebox{\textwidth}{!}{
  \begin{tabular}{|c|c|c|c|c|c|}\hline
    Variables & Shape & Type & Elements & Min & Max \\ \hline 
    Observation, state $s$ & (10,) & Box (Continuous) & {\scriptsize End-effector position $(x,y,z)$,
    and velocities $(\dot{x}, \dot{y},
  \dot{z})$, left and right gripper finger position and velocities } & - & - \\ \hline
  Action, $a$ & (4, ) & Box (Continuous) & {\scriptsize Displacement of end-effector $(dx, dy, dz)$ and last variable
  controlling the opening and closing of gripper}& -1 & 1 \\ \hline
  Reward, $r$ & () & Continuous & {\scriptsize Negative Euclidean distance between achieved goal and desired
    goal} & $-\infty$ & 0 \\ \hline 
\end{tabular}}
\end{table*}

\begin{figure}[htbp]
  \centering
  \begin{tabular}{cccc}
    \includegraphics[scale=0.2]{./figures/chap08/fetch_reach_step_0.png}  & 
    \includegraphics[scale=0.2]{./figures/chap08/fetch_reach_step_1.png}  & 
    \includegraphics[scale=0.27]{./figures/chap08/fetch_reach_step_5.png}  &
    \includegraphics[scale=0.27]{./figures/chap08/fetch_reach_final.png}   
\end{tabular}
\caption{\small Screenshots of a few observation states of \texttt{FetchReachDense-v3} environment. The red dot
is the desired goal that the robot end-effector expected to reach for successful completion of task.}
  \label{fig:fr_obs}
\end{figure}

\begin{listing}
  \begin{small}
  \begin{pygments}[frame=single, indent=L]{python}
import gymnasium as gym
from sac2 import SACAgent
if __name__ == "__main__":

    # Create the LunarLanderContinuous-v3 environment
    env = gym.make('FetchReachDense-v3', 
            max_episode_steps=100, render_mode='rgb_array')

    obs_shape = env.observation_space['observation'].shape
    action_shape = env.action_space.shape
    print(f"Observation shape: {obs_shape}, Action shape: {action_shape}")
    action_upper_bound = env.action_space.high  
    print(f"Action upper bound: {action_upper_bound}")
    print(f"Action lower bound: {env.action_space.low}")

    # Initialize the SAC agent
    agent = SACAgent(obs_shape, action_shape,
                    action_upper_bound=action_upper_bound,
                    reward_scale=2.0,
                    buffer_size=1000000,
                    batch_size=256, )

    # train the agent
    train_sac_agent(env, agent, num_episodes=1500,
                    max_score=None, min_score=None,
                    stop_score=0,
                    ep_max_steps=None,
                    update_per_step=1)
  \end{pygments}
\end{small}
\begin{framed}
  \begin{scriptsize}
    \begin{verbatim}
Output:
Observation shape: (10,), Action shape: (4,)
Action upper bound: [1. 1. 1. 1.]
Action lower bound: [-1. -1. -1. -1.]
Environment name:  FetchReachDense-v3
RL Agent name: SAC2
e:0, ep_score:-42.39, avg_ep_score:-42.39, avg100score:-42.39,  best_score:-inf
Best Score: -42.39, episode: 0. Model saved.
Best Score: -41.00, episode: 1. Model saved.
Best Score: -40.82, episode: 6. Model saved.
Best Score: -30.89, episode: 19. Model saved.
Best Score: -30.21, episode: 34. Model saved.
Best Score: -29.32, episode: 37. Model saved.
Best Score: -23.87, episode: 43. Model saved.
Best Score: -12.88, episode: 84. Model saved.
e:100, ep_score:-15.25, avg_ep_score:-40.83, avg100score:-40.82,  best_score:-12.88
Best Score: -10.64, episode: 124. Model saved.
Best Score: -10.37, episode: 154. Model saved.
Best Score: -8.45, episode: 179. Model saved.
Best Score: -8.21, episode: 192. Model saved.
e:200, ep_score:-20.71, avg_ep_score:-30.78, avg100score:-20.62,  best_score:-8.21
Best Score: -6.61, episode: 223. Model saved.
e:300, ep_score:-20.45, avg_ep_score:-27.05, avg100score:-19.56,  best_score:-6.61
e:400, ep_score:-10.52, avg_ep_score:-25.32, avg100score:-20.12,  best_score:-6.61
e:500, ep_score:-32.46, avg_ep_score:-24.26, avg100score:-20.01,  best_score:-6.61
e:600, ep_score:-18.83, avg_ep_score:-23.54, avg100score:-19.89,  best_score:-6.61
e:700, ep_score:-14.12, avg_ep_score:-22.76, avg100score:-18.08,  best_score:-6.61
e:800, ep_score:-17.94, avg_ep_score:-22.13, avg100score:-17.70,  best_score:-6.61
Best Score: -5.96, episode: 808. Model saved.
e:900, ep_score:-20.48, avg_ep_score:-21.76, avg100score:-18.84,  best_score:-5.96
e:1000, ep_score:-23.76, avg_ep_score:-21.43, avg100score:-18.42, best_score:-5.96
e:1100, ep_score:-27.59, avg_ep_score:-21.17, avg100score:-18.61, best_score:-5.96
e:1200, ep_score:-23.49, avg_ep_score:-20.97, avg100score:-18.79, best_score:-5.96
e:1300, ep_score:-23.06, avg_ep_score:-20.80, avg100score:-18.67, best_score:-5.96
e:1400, ep_score:-20.86, avg_ep_score:-20.64, avg100score:-18.59, best_score:-5.96
    \end{verbatim}
  \end{scriptsize}
\end{framed}
\caption{\small Main code for training SAC agent on \texttt{FetchReachDense-v3} environment}
\label{lst:frd_sac}
\end{listing}


\begin{figure}[htbp]
  \centering
  \includegraphics[scale=0.15]{./figures/chap08/fetchreachdense_v3_sac.png}
  \caption{\small Training performance of SAC algorithm on \texttt{FetchReachDense-v3} environment}
  \label{fig:freach_sac}
\end{figure}

\begin{figure*}[htbp]
  \centering
  \begin{tabular}{cc}
    \includegraphics[scale=0.08]{./figures/chap08/fetchreachdense_v3_sac_actor_loss.png} & 
    \includegraphics[scale=0.08]{./figures/chap08/fetchreachdense_v3_sac_critic_loss.png} 
  \end{tabular}
    \caption{\small SAC agent's soss functions: (a) Actor \& Alpha losses, (b) Value \& Critic losses}
  \label{fig:frd_sac_loss}
\end{figure*}

\subsection{Comparing SAC vs SAC2}
As discussed above, two different implementations of SAC algorithm is presented. The first version,
called SAC, uses an actor model, two critic models and two target critic models for training. On the
other hand, SAC2 uses a separate value network and a target value network in addition to an actor
model and two critic models. In terms of parameters, there is not much difference as both use 5 deep
network models. In terms of performances, SAC2 performs slightly better in general as estimating
value separately reduces variance and provides better training stability. The performance comparison between
these two variants is shown in Figure \ref{fig:sac_comp}. We see that the performance of these
variants are mostly same for the \texttt{FetchReachDense-v3} environment. However, SAC2 performs
better in case of \texttt{Pendulum-v1} and \texttt{LunarLanderContinuous-v3} environments.  

\begin{figure}[htbp]
  \centering
  \begin{tabular}{ccc}
    \includegraphics[scale=0.07]{./figures/chap08/pendulum_v1_sac_comp.png} & 
    \includegraphics[scale=0.07]{./figures/chap08/frd_sac_comp.png} & 
    \includegraphics[scale=0.07]{./figures/chap08/lunarlandercont_v3_sac_comp.png}  
 \end{tabular}
  \caption{Comparing performances of two implementations: SAC \& SAC2}
  \label{fig:sac_comp}
\end{figure}

\section{Conclusion}
In conclusion, Soft Actor-Critic represents a sophisticated yet practical approach to reinforcement
learning. By seamlessly integrating the pursuit of reward with the encouragement of exploration
through entropy maximization, SAC provides a powerful, stable, and sample-efficient algorithm for
tackling challenging continuous control problems. It outperforms DDPG due to better exploration and
reduced bias, surpasses A2C in sample efficiency and performance in complex tasks, and often
outshines PPO in continuous control due to its off-policy nature. However, it can  be
computationally intensive due to multiple networks and entropy calculations. We discussed two
implementations of SAC algorithm - one that uses two target critic networks and the other that uses
a separate value network and its corresponding target network. We demonstrate the performance of SAC
algorithm in solving three benchmark problems with continuous action spaces, namely, \texttt{Pendulum-v1},
\texttt{LunarLanderContinuous-v3} and \texttt{FetchReachDense-v3} environments. In general, we see
that SAC algorithm provides superior performance compared to the algorithms discussed in previous
chapters.  


