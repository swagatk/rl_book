\chapter{Basics of Probability \& Statistics} \label{chap:app01}

\section{Theorems \& Definitions}
\begin{theorem}
If $X$ and $Y$  are two jointly distributed random variables, then 
\[\mathbb{E}(X + Y) = \mathbb{E}(X) + \mathbb{E}(Y)\]
\label{th:exp_add}
\end{theorem}

\begin{proof}
  \begin{eqnarray*}
    \mathbb{E}(X+Y) &=&  \sum_i \sum_j (x_i+y_j)p(x_i,y_j) \\
    &=& \sum_i\sum_j x_ip(x_i,y_j) + \sum_i\sum_j y_j p(x_i, y_j) \\
    &=& \sum_i x_i \sum_j p(x_i, y_j) + \sum_j y_j \sum_i p(x_i, y_j)
    \\
    &=& \sum_i x_i P(x_i) + \sum_j y_j P(y_j) \\
    &=& \mathbb{E}(X) + \mathbb{E}(Y)        
  \end{eqnarray*}
\end{proof}

\begin{defn}
  \textbf{Bayesian Theorem of Conditional Probability:} 
  \begin{equation}
    P(A, B) = P(A|B) P(B)
    \label{eq:bayes}
  \end{equation}
  This leads to the following conditional probability relationships: 

  \begin{eqnarray*}
    P(A|B) &=&  P(B|A) * P(A) / P(B) \\
    P(B|A) &=& P(A|B) * P(B) / P(A)
  \end{eqnarray*}
  
\end{defn}


\section{Descriptions \& Explanations}
\index{State-Action Marginal Visitation Probability Distribution}

\subsection{State-Action Marginal Visitation Probability Distribution}
\label{sec:samvp}

In the context of reinforcement learning, the \textbf{state-action marginal visitation probability
distribution}, often denoted as $\rho_\pi(s, a)$, formally defines the likelihood of visiting a
specific state-action pair $(s, a)$ over the entire lifespan of an agent following a particular
policy $\pi$.

More precisely, for a given policy $\pi$ and a discount factor $\gamma \in [0,1)$, it is defined as:

\begin{equation}
\rho_\pi(s, a) = \sum_{t=0}^{\infty} \gamma^t P(s_t = s, a_t = a \mid s_0, \pi)
\label{eq:savp}
\end{equation}

Where:
\begin{itemize}
\item $s_t$ and $a_t$ are the state and action at time step $t$, respectively.
\item $P(s_t = s, a_t = a \mid s_0, \pi)$ is the probability of being in state $s$ and taking action
  $a$ at time $t$, given an initial state $s_0$ and following policy $\pi$. This probability
  inherently depends on the environment's transition dynamics $P(s_{t+1} \mid s_t, a_t)$ and the
  policy $\pi(a_t \mid s_t)$.

\item The sum over $t$ (time steps) accounts for the long-term visitation.
\item The discount factor $\gamma^t$ gives more weight to state-action pairs visited earlier in an
  episode, reflecting the standard discounted return objective. In some contexts, particularly for
  average reward settings, $\gamma^t$ might be omitted or replaced with a uniform probability over
  time steps.

\end{itemize}
In simpler terms, $\rho_\pi(s, a)$ tells you, on average, how frequently (or with what probability
density for continuous spaces) the agent will encounter a specific state $s$ and then choose action
$a$ when it consistently behaves according to policy $\pi$ over an extended period. It is a
fundamental concept for understanding the expected behavior and performance of a policy.

