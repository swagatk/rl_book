\relax 
\providecommand\zref@newlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\@newglossary[4]{}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{main.ist}
\@glsorder{word}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{Acronyms}{7}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\nonumberline List of Figures}{9}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\nonumberline List of Algorithms}{11}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\nonumberline List of Tables}{13}{}\protected@file@percent }
\citation{sutton2018reinforcement}
\citation{norris1998markov}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction to Reinforcement Learning}{15}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}What is Reinforcement Learning?}{15}{}\protected@file@percent }
\newlabel{sec:whatis}{{1.1}{15}{What is Reinforcement Learning?}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Reinforcement Learning Problem}{15}{}\protected@file@percent }
\newlabel{sec:rlprob}{{1.2}{15}{Reinforcement Learning Problem}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.1}The Components of a Reinforcement Learning System}{15}{}\protected@file@percent }
\newlabel{sec:rlcomp}{{1.2.1}{15}{The Components of a Reinforcement Learning System}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2.2}Problem Formulation}{15}{}\protected@file@percent }
\newlabel{eq:be_value_fn}{{1.1}{16}{Problem Formulation}{}{}}
\newlabel{eq:be_qfn}{{1.2}{16}{Problem Formulation}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Components of a Reinforcement Learning System. $\mathbb  {E}_{\pi } (R)$ is the expected future reward under a given policy $\pi $.}}{17}{}\protected@file@percent }
\newlabel{fig:rl_sys_comp}{{1.1}{17}{Problem Formulation}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Types of Reinforcement Learning}{17}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Types of RL Environment}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.5}Reinforcement Learning Algorithms}{18}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.5.1}Applications of Reinforcement Learning}{18}{}\protected@file@percent }
\citation{sutton2018reinforcement}
\citation{skinner1971operant}
\citation{haney2020applied}
\citation{arulkumaran2017brief}
\citation{li2017deep}
\citation{franccois2018introduction}
\citation{silver2017mastering}
\@writefile{toc}{\contentsline {section}{\numberline {1.6}The history of Reinforcement Learning}{19}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Markov Decision Process and Dynamic Programming}{21}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:mdp}{{2}{21}{Markov Decision Process and Dynamic Programming}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Markov chain and Markov Decision process}{21}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Rewards and Returns}{21}{}\protected@file@percent }
\newlabel{eq:cum_re}{{2.1}{21}{Rewards and Returns}{}{}}
\newlabel{eq:cum_re1}{{2.2}{22}{Rewards and Returns}{}{}}
\newlabel{eq:cum_red}{{2.3}{22}{Rewards and Returns}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}The policy function}{22}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}State-Value function}{22}{}\protected@file@percent }
\newlabel{eq:value1}{{2.4}{22}{State-Value function}{}{}}
\newlabel{eq:value_func}{{2.5}{22}{State-Value function}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}State-action value function (Q-function)}{23}{}\protected@file@percent }
\newlabel{eq:q_func}{{2.6}{23}{State-action value function (Q-function)}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.5}Optimality}{23}{}\protected@file@percent }
\newlabel{eq:opt_val}{{2.7}{23}{Optimality}{}{}}
\newlabel{eq:opt_q}{{2.8}{23}{Optimality}{}{}}
\newlabel{eq:opt_policy}{{2.9}{23}{Optimality}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.6}Bellman Equation}{23}{}\protected@file@percent }
\newlabel{eq:be_value}{{2.10}{23}{Bellman Equation}{}{}}
\newlabel{eq:be_q}{{2.11}{24}{Bellman Equation}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.7}Deriving Bellman equation}{24}{}\protected@file@percent }
\newlabel{eq:value_func_2}{{2.12}{24}{Deriving Bellman equation}{}{}}
\newlabel{eq:exp1}{{2.13}{24}{Deriving Bellman equation}{}{}}
\newlabel{eq:prs}{{2.14}{24}{Deriving Bellman equation}{}{}}
\newlabel{eq:v_part1}{{2.15}{24}{Deriving Bellman equation}{}{}}
\newlabel{eq:pgs}{{2.16}{24}{Deriving Bellman equation}{}{}}
\newlabel{eq:pgs_2}{{2.17}{24}{Deriving Bellman equation}{}{}}
\citation{bellman1966dynamic}
\newlabel{eq:v_part2}{{2.18}{25}{Deriving Bellman equation}{}{}}
\newlabel{eq:be_value_3}{{2.19}{25}{Deriving Bellman equation}{}{}}
\newlabel{eq:be_q_func_3}{{2.20}{25}{Deriving Bellman equation}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Solving the Bellman Equation}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Dynamic Programming}{25}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Value Iteration}{26}{}\protected@file@percent }
\newlabel{sec:value_iter}{{2.2.2}{26}{Value Iteration}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {2.1}{\ignorespaces Python class for implementing value iteration algorithm.}}{28}{}\protected@file@percent }
\newlabel{lst:vi}{{2.1}{28}{Value Iteration}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Policy Iteration}{28}{}\protected@file@percent }
\newlabel{sec:policy_iter}{{2.2.3}{28}{Policy Iteration}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {2.2}{\ignorespaces Python class for implementing Policy iteration algorithm.}}{30}{}\protected@file@percent }
\newlabel{lst:pi}{{2.2}{30}{Policy Iteration}{}{}}
\citation{gym}
\@writefile{lol}{\contentsline {listing}{\numberline {2.3}{\ignorespaces Python code for Policy Iteration Algorithm for learning a stochastic policy distribution $\pi (s,a)$.}}{32}{}\protected@file@percent }
\newlabel{lst:pi_2}{{2.3}{32}{Policy Iteration}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Solving the Frozen Lake Problem}{32}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Visualization of the Frozen Lake Environment}}{32}{}\protected@file@percent }
\newlabel{fig:flake}{{2.1}{32}{Solving the Frozen Lake Problem}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {2.4}{\ignorespaces Applying value iteration algorithm to solve the Frozen Lake Problem}}{33}{}\protected@file@percent }
\newlabel{lst:fl_vi_pi}{{2.4}{33}{Solving the Frozen Lake Problem}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Solving the Taxi Problem}{33}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Visualization of The Taxi-v3 environment}}{34}{}\protected@file@percent }
\newlabel{fig:taxi}{{2.2}{34}{Solving the Taxi Problem}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {2.5}{\ignorespaces The performance of Random Policy for the Taxi environment}}{35}{}\protected@file@percent }
\newlabel{lst:random}{{2.5}{35}{Solving the Taxi Problem}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Summary}{35}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Monte Carlo Simulation for Reinforcement Learning}{37}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}What is Monte Carlo simulation?}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}Tossing a Coin}{37}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Estimating the value of pi using Monte Carlo}{37}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Estimating $\pi $ through Monte-Carlo Simulation. (a) A quadrant of circle inside a square. (b) Estimate of $\pi $ improves with increasing number of samples}}{38}{}\protected@file@percent }
\newlabel{fig:est_pi}{{3.1}{38}{Estimating the value of pi using Monte Carlo}{}{}}
\newlabel{eq:est_pi}{{3.1}{38}{Estimating the value of pi using Monte Carlo}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {3.6}{\ignorespaces Python code for estimating $\pi $ through random sampling.}}{39}{}\protected@file@percent }
\newlabel{lst:est_pi_code}{{3.6}{39}{Estimating the value of pi using Monte Carlo}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Monte Carlo Prediction}{39}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}First-Visit Monte-Carlo Algorithm}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Every-visit Monte-Carlo Algorithm}{40}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.3}Playing Blackjack Game}{41}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {3.7}{\ignorespaces Python code to visualize Blackjack environment}}{42}{}\protected@file@percent }
\newlabel{lst:bj_env}{{3.7}{42}{Playing Blackjack Game}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Visualization of a state of Blackjack environment. The second image shows a state with an usable ace.}}{42}{}\protected@file@percent }
\newlabel{fig:bj_viz}{{3.2}{42}{Playing Blackjack Game}{}{}}
\newlabel{eq:mcup1}{{3.2}{43}{Playing Blackjack Game}{}{}}
\newlabel{eq:mcup2}{{3.3}{43}{Playing Blackjack Game}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces First-Visit Monte Carlo Algorithm (\emph  {for state-action values})}}{43}{}\protected@file@percent }
\newlabel{alg:fvmcp}{{3.1}{43}{Playing Blackjack Game}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {3.8}{\ignorespaces Python code for estimating value function using First-Visit Monte-Carlo Prediction}}{46}{}\protected@file@percent }
\newlabel{lst:mcp_fv}{{3.8}{46}{Playing Blackjack Game}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {3.9}{\ignorespaces Python code for plotting Value and Policy for Blackjack environment}}{47}{}\protected@file@percent }
\newlabel{lst:plot_bj_vp}{{3.9}{47}{Playing Blackjack Game}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Estimated Value function with and without usable ace}}{48}{}\protected@file@percent }
\newlabel{fig:bj_v_est}{{3.3}{48}{Playing Blackjack Game}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {3.10}{\ignorespaces Another implementation of first visit algorithm for estimating Q values.}}{48}{}\protected@file@percent }
\newlabel{lst:fvmc_2}{{3.10}{48}{Playing Blackjack Game}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Monte Carlo Control}{48}{}\protected@file@percent }
\newlabel{eq:mcc_qupdate}{{3.4}{49}{Monte Carlo Control}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.2}{\ignorespaces First-Visit Constant-$\alpha $ GLIE MC Control Algorithm}}{49}{}\protected@file@percent }
\newlabel{alg:fvmcc}{{3.2}{49}{Monte Carlo Control}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {3.11}{\ignorespaces Python code for implementing First-Visit GLIE Monte-Carlo Control algorithm.}}{51}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Estimated Value and Policy for Blackjack Environment obtained with Monte-Carlo Control algorithm.}}{52}{}\protected@file@percent }
\newlabel{fig:mcc_vp}{{3.4}{52}{Monte Carlo Control}{}{}}
\newlabel{lst:fvmcc_code}{{3.11}{53}{Monte Carlo Control}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}On-Policy \& Off-Policy Monte Carlo Algorithms}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.5}challenges of Using Monte Carlo Simulation for Reinforcement Learning}{53}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Conclusion}{53}{}\protected@file@percent }
\citation{sutton1988learning}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Temporal Difference Learning}{55}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:tdl}{{4}{55}{Temporal Difference Learning}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{55}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.2}TD Learning}{55}{}\protected@file@percent }
\newlabel{eq:td_value}{{4.1}{55}{TD Learning}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}TD Control Algorithms}{56}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Q Learning}{56}{}\protected@file@percent }
\newlabel{sec:ql}{{4.4}{56}{Q Learning}{}{}}
\newlabel{eq:qlearn}{{4.2}{56}{Q Learning}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {4.12}{\ignorespaces Python Code for Q Learning Algorithm.}}{59}{}\protected@file@percent }
\newlabel{lst:qlearn}{{4.12}{59}{Q Learning}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {4.13}{\ignorespaces Outcome of applying Q Learning to solve the Frozen Lake problem}}{59}{}\protected@file@percent }
\newlabel{lst:flake_qlearn}{{4.13}{59}{Q Learning}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {4.14}{\ignorespaces Outcome of applying Q Learning to solve the Frozen Lake problem}}{60}{}\protected@file@percent }
\newlabel{lst:taxi_qlearn}{{4.14}{60}{Q Learning}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}SARSA Algorithm}{60}{}\protected@file@percent }
\newlabel{eq:sarsa}{{4.3}{60}{SARSA Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {4.15}{\ignorespaces Python Code for SARSA Algorithm}}{63}{}\protected@file@percent }
\newlabel{lst:sarsa}{{4.15}{63}{SARSA Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {4.16}{\ignorespaces Output of SARSA algorithm when applied to solve the 'Taxi-v3' problem.}}{63}{}\protected@file@percent }
\newlabel{lst:taxi_sarsa}{{4.16}{63}{SARSA Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {4.17}{\ignorespaces Output of SARSA Algorithm when applied to solve the Frozen Lake problem}}{64}{}\protected@file@percent }
\newlabel{lst:flake_sarsa}{{4.17}{64}{SARSA Algorithm}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Comparing the training performance of SARSA and Q learning algorithm for \texttt  {Taxi-v3} environment.}}{64}{}\protected@file@percent }
\newlabel{fig:qlearn_sarsa_taxi}{{4.1}{64}{SARSA Algorithm}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Difference between Q-Learning and SARSA Algorithms}{64}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Conclusion}{65}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Deep Q Network}{67}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:dqn}{{5}{67}{Deep Q Network}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{67}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}DQN Algorithm}{67}{}\protected@file@percent }
\newlabel{sec:dqn_algo}{{5.2}{67}{DQN Algorithm}{}{}}
\newlabel{eq:q_update}{{5.1}{67}{DQN Algorithm}{}{}}
\newlabel{eq:dqn_loss}{{5.2}{67}{DQN Algorithm}{}{}}
\citation{van2016deep}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Schematic of DQN Learning Algorithm}}{68}{}\protected@file@percent }
\newlabel{fig:dqn_scheme}{{5.1}{68}{DQN Algorithm}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Double DQN Algorithm}{68}{}\protected@file@percent }
\newlabel{sec:ddqn}{{5.3}{68}{Double DQN Algorithm}{}{}}
\newlabel{eq:dqn_loss_1}{{5.3}{68}{Double DQN Algorithm}{}{}}
\newlabel{eq:ddqn_loss}{{5.4}{68}{Double DQN Algorithm}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.1}{\ignorespaces DQN Algorithm}}{69}{}\protected@file@percent }
\newlabel{algo:dqn}{{5.1}{69}{DQN Algorithm}{}{}}
\newlabel{eq:target_q_value}{{5.5}{69}{Double DQN Algorithm}{}{}}
\newlabel{eq:polyak}{{5.6}{69}{Double DQN Algorithm}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Python implementation of DQN Algorithm}{69}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Replay Buffer}{69}{}\protected@file@percent }
\newlabel{sec:replay_buffer}{{5.4.1}{69}{Replay Buffer}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.2}{\ignorespaces Double DQN (DDQN) Algorithm}}{70}{}\protected@file@percent }
\newlabel{algo:ddqn}{{5.2}{70}{Double DQN Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {5.18}{\ignorespaces Python class for implementing Replay Buffer.}}{71}{}\protected@file@percent }
\newlabel{lst:replay_buffer}{{5.18}{71}{Replay Buffer}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}The DQN Class}{71}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.19}{\ignorespaces (Double) DQN agent class template}}{73}{}\protected@file@percent }
\newlabel{lst:ddqn}{{5.19}{73}{The DQN Class}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Epsilon-Greedy Policy}{73}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.20}{\ignorespaces Code for DQN Class: epsilon-greedy policy}}{73}{}\protected@file@percent }
\newlabel{lst:ddqn_egp}{{5.20}{73}{Epsilon-Greedy Policy}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Experience Replay}{73}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.21}{\ignorespaces Code for DQN class: experience replay class}}{75}{}\protected@file@percent }
\newlabel{lst:ddqn_exprep}{{5.21}{75}{Experience Replay}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.5}Training an agent for solving a Gym Problem}{75}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.22}{\ignorespaces Python code for training a DQN for a given Gym Environment}}{77}{}\protected@file@percent }
\newlabel{lst:ddqn_train}{{5.22}{77}{Training an agent for solving a Gym Problem}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.6}Solving the CartPole problem using DQN algorithm}{77}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Visualization of CartPole environment. The task is to balance the pole in the vertical position by controlling the motion of the cart.}}{78}{}\protected@file@percent }
\newlabel{fig:cartpole}{{5.2}{78}{Solving the CartPole problem using DQN algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {5.23}{\ignorespaces Python code for training a DQN agent and its console output}}{78}{}\protected@file@percent }
\newlabel{lst:dqn_cp_train}{{5.23}{78}{Solving the CartPole problem using DQN algorithm}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Performance of DDQN algorithm in solving the CartPole problem}}{79}{}\protected@file@percent }
\newlabel{fig:dqn_cp}{{5.3}{79}{Solving the CartPole problem using DQN algorithm}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Priority Experience Replay (PER)}{79}{}\protected@file@percent }
\newlabel{sec:per}{{5.5}{79}{Priority Experience Replay (PER)}{}{}}
\newlabel{eq:p_score}{{5.7}{79}{Priority Experience Replay (PER)}{}{}}
\newlabel{eq:p_prob}{{5.8}{79}{Priority Experience Replay (PER)}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Adding samples with priority to a sum-tree replay buffer}}{80}{}\protected@file@percent }
\newlabel{fig:st_add}{{5.4}{80}{Sum-Tree Data Structure for Priority Replay Buffer}{}{}}
\newlabel{eq:isw}{{5.9}{80}{Priority Experience Replay (PER)}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Sum-Tree Data Structure for Priority Replay Buffer}{80}{}\protected@file@percent }
\newlabel{sec:sumtree}{{5.5.1}{80}{Sum-Tree Data Structure for Priority Replay Buffer}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Adding experience to the replay buffer}{80}{}\protected@file@percent }
\newlabel{sec:add_sample}{{5.5.1}{80}{Adding experience to the replay buffer}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Retrieving samples with priority from a sum-tree replay buffer}}{81}{}\protected@file@percent }
\newlabel{fig:st_retr}{{5.5}{81}{Retrieving samples from the buffer}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Retrieving samples from the buffer}{81}{}\protected@file@percent }
\newlabel{sec:retr_sample}{{5.5.1}{81}{Retrieving samples from the buffer}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.2}Python Implementation of Sum-Tree class}{82}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.24}{\ignorespaces Python implementation of Sum-tree class}}{83}{}\protected@file@percent }
\newlabel{lst:st_class}{{5.24}{83}{Python Implementation of Sum-Tree class}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.3}Python implementation of Priority Replay Buffer using Sum-Tree}{84}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.25}{\ignorespaces Python implementation of Priority Replay Buffer}}{85}{}\protected@file@percent }
\newlabel{lst:st_buffer}{{5.25}{85}{Python implementation of Priority Replay Buffer using Sum-Tree}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.4}Python code for DQN Agent with PER}{85}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Gym's Mountain Car Problem. The goal is to reach the top of the hill as soon as possible.}}{87}{}\protected@file@percent }
\newlabel{fig:gym_mc}{{5.6}{87}{Solving MountainCar Problem using DQN with PER}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {5.26}{\ignorespaces Python Code for implementing DQN with PER}}{87}{}\protected@file@percent }
\newlabel{lst:dqn_per}{{5.26}{87}{Python code for DQN Agent with PER}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.5}Solving MountainCar Problem using DQN with PER}{87}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Training the agent}{87}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.27}{\ignorespaces Training function used for solving the Mountain Car Problem.}}{89}{}\protected@file@percent }
\newlabel{lst:mc_train}{{5.27}{89}{Training the agent}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Main Code for creating and training agent}{89}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.28}{\ignorespaces Solving the Mountain Car Problem with DQN and PER}}{91}{}\protected@file@percent }
\newlabel{lst:mc_dqn_per}{{5.28}{91}{Main Code for creating and training agent}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Training performance of the DQN PER Agent used for solving the Mountain Car Problem. The problem is considered solved if the car position reaches 0.5, episodic reward reaches 200 in less than 200 steps.}}{91}{}\protected@file@percent }
\newlabel{fig:mc_dqn_per}{{5.7}{91}{Main Code for creating and training agent}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Solving Atari Games using DQN}{92}{}\protected@file@percent }
\zref@newlabel{mdf@pagelabel-1}{\default{5.6}\page{92}\abspage{92}\mdf@pagevalue{92}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces A PacMan Atari Environment Observation}}{93}{}\protected@file@percent }
\newlabel{fig:pc_obs}{{5.8}{93}{Solving Atari Games using DQN}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {5.29}{\ignorespaces Python code for creating an atari environment and visualizing state observation.}}{93}{}\protected@file@percent }
\newlabel{lst:atari}{{5.29}{93}{Solving Atari Games using DQN}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.1}Image Stacking Wrapper}{93}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.30}{\ignorespaces A wrapper class to stack frames in Gym environment }}{95}{}\protected@file@percent }
\newlabel{lst:stack_frames}{{5.30}{95}{Image Stacking Wrapper}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.2}DQN Agent for Atari Environments}{95}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.31}{\ignorespaces Python class for applying DQN algorithm to Atari environments. It inherits attributes from DQN and DQNPERAgent}}{97}{}\protected@file@percent }
\newlabel{lst:dqn_atari}{{5.31}{97}{DQN Agent for Atari Environments}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6.3}Training agent on Atari PacMan Environment}{97}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {5.32}{\ignorespaces Python code for creating and training a DQN agent for Atari PacMan environment.}}{99}{}\protected@file@percent }
\newlabel{lst:dqn_pacman}{{5.32}{99}{Training agent on Atari PacMan Environment}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces Training performance of DQN and DQN+PER algorithm for PacMan Environment}}{99}{}\protected@file@percent }
\newlabel{fig:pc_dqn_comp}{{5.9}{99}{Training agent on Atari PacMan Environment}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Summary}{99}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Policy Gradient Methods}{101}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:pgm}{{6}{101}{Policy Gradient Methods}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{101}{}\protected@file@percent }
\newlabel{eq:q_greedy_policy}{{6.1}{101}{Introduction}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Policy Gradient}{101}{}\protected@file@percent }
\newlabel{sec:pg}{{6.2}{101}{Policy Gradient}{}{}}
\newlabel{eq:pg_obj}{{6.2}{101}{Policy Gradient}{}{}}
\newlabel{eq:reward_func}{{6.3}{101}{Policy Gradient}{}{}}
\newlabel{eq:policy_update}{{6.4}{101}{Policy Gradient}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Computing $\nabla _\theta J(\theta )$}{102}{}\protected@file@percent }
\newlabel{eq:mean}{{6.5}{102}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:pg_obj_expanded}{{6.6}{102}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:p_grad_1}{{6.7}{102}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:p_grad_2}{{6.8}{102}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:prob_state}{{6.9}{102}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:grad_p}{{6.11}{103}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:p_grad_3}{{6.12}{103}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:pg_obj_wg}{{6.15}{103}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:pg_wg}{{6.16}{103}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:pg_return}{{6.17}{104}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:pg_wq}{{6.18}{104}{Computing $\nabla _\theta J(\theta )$}{}{}}
\newlabel{eq:pg_wa}{{6.19}{104}{Computing $\nabla _\theta J(\theta )$}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Monte-Carlo Policy Gradient Algorithm}{104}{}\protected@file@percent }
\newlabel{sec:mcpga}{{6.3}{104}{Monte-Carlo Policy Gradient Algorithm}{}{}}
\newlabel{eq:pg_update_2}{{6.20}{104}{Monte-Carlo Policy Gradient Algorithm}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.1}Python Code for REINFORCE Algorithm}{105}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.33}{\ignorespaces Python code for creating Policy Network}}{106}{}\protected@file@percent }
\newlabel{lst:policy_net}{{6.33}{106}{Python Code for REINFORCE Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {6.34}{\ignorespaces Python Code for REINFORCE Algorithm.}}{107}{}\protected@file@percent }
\newlabel{lst:pg_mc}{{6.34}{107}{Python Code for REINFORCE Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {6.35}{\ignorespaces Code for training a monte carlo policy gradient agent on a given Gym environment.}}{108}{}\protected@file@percent }
\newlabel{lst:train_env}{{6.35}{108}{Python Code for REINFORCE Algorithm}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.2}Solving CartPole Problem}{108}{}\protected@file@percent }
\newlabel{pg_cp}{{6.3.2}{108}{Solving CartPole Problem}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {6.36}{\ignorespaces Solving \texttt  {CartPole-v0} environment using REINFORCE agent.}}{108}{}\protected@file@percent }
\newlabel{lst:pg_cp}{{6.36}{108}{Solving CartPole Problem}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Performance of REINFORCE algorithm on \texttt  {Cartpole-v0} environment. It shows average episodic score and average score of last 100 episodes as training progresses.}}{109}{}\protected@file@percent }
\newlabel{fig:pg_cp}{{6.1}{109}{Solving CartPole Problem}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces A few snapshots of Lunar-Lander environment. }}{109}{}\protected@file@percent }
\newlabel{fig:ll_img}{{6.2}{109}{Solving Lunar-Lander Problem}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3.3}Solving Lunar-Lander Problem}{109}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.37}{\ignorespaces Applying REINFORCE agent to solve Lunar-Lander Problem.}}{110}{}\protected@file@percent }
\newlabel{lst:ll_mcpg}{{6.37}{110}{Solving Lunar-Lander Problem}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Performance of REINFORCE algorithm on \texttt  {LunarLander-v2} problem. The problem is considered solved if episodic score exceeds 200.}}{110}{}\protected@file@percent }
\newlabel{fig:ll_mcpg}{{6.3}{110}{Solving Lunar-Lander Problem}{}{}}
\citation{lillicrap2015continuous}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Actor-Critic Model}{111}{}\protected@file@percent }
\newlabel{sec:ac_model}{{6.4}{111}{Actor-Critic Model}{}{}}
\newlabel{eq:ac_pu}{{6.21}{111}{Actor-Critic Model}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Deep Deterministic Policy Gradient}{111}{}\protected@file@percent }
\newlabel{sec:ddpg}{{6.5}{111}{Deep Deterministic Policy Gradient}{}{}}
\citation{polyak1992acceleration}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Actor-Critic Architecture}}{112}{}\protected@file@percent }
\newlabel{fig:ac_diag}{{6.4}{112}{Actor-Critic Model}{}{}}
\newlabel{eq:ddpg_qloss}{{6.22}{112}{Deep Deterministic Policy Gradient}{}{}}
\newlabel{eq:ddpg_po}{{6.23}{112}{Deep Deterministic Policy Gradient}{}{}}
\newlabel{eq:pa}{{6.24}{112}{Deep Deterministic Policy Gradient}{}{}}
\citation{maller2009ornstein}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.1}{\ignorespaces DDPG Algorithm}}{113}{}\protected@file@percent }
\newlabel{alg:ddpg}{{6.1}{113}{Deep Deterministic Policy Gradient}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Python Implementation of DDPG algorithm}{113}{}\protected@file@percent }
\citation{maller2009ornstein}
\citation{maller2009ornstein}
\@writefile{lol}{\contentsline {listing}{\numberline {6.38}{\ignorespaces Python Code for creating class for generating action noise using Ornstein-Uhlenbeck process \cite  {maller2009ornstein}.}}{114}{}\protected@file@percent }
\newlabel{lst:action_noise}{{6.38}{114}{Python Implementation of DDPG algorithm}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Actor Model}{114}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.39}{\ignorespaces Actor class for DDPG algorithm. Actor estimates the policy function. It learns by maximizing critic output.}}{116}{}\protected@file@percent }
\newlabel{lst:ddpg_actor}{{6.39}{116}{Actor Model}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Critic Class}{116}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.40}{\ignorespaces Critic Class for DDPG algorithm. Critic estimates the Q function and learns by minimizing the TD error.}}{118}{}\protected@file@percent }
\newlabel{lst:ddpg_critic}{{6.40}{118}{Critic Class}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline DDPG Agent}{118}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.41}{\ignorespaces Python code for implementing DDPG Agent. It uses the actor-critic architecture.}}{120}{}\protected@file@percent }
\newlabel{lst:ddpg}{{6.41}{120}{DDPG Agent}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Solving Pendulum Problem}{120}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Input-Output variables for `\texttt  {Pendulum-v1}' Gym Simulation Environment.}}{120}{}\protected@file@percent }
\newlabel{tab:pendu_param}{{6.1}{120}{Solving Pendulum Problem}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces A few snapshots of \texttt  {Pendulum-v1} environment states. (d) shows the final successful state of the environment.}}{121}{}\protected@file@percent }
\newlabel{fig:pendu_states}{{6.5}{121}{Solving Pendulum Problem}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Training DDPG Agent}{121}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.42}{\ignorespaces Python function for solving a Gym problem environment}}{122}{}\protected@file@percent }
\newlabel{lst:ddpg_train_env}{{6.42}{122}{Training DDPG Agent}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.3}Main Code to generate output}{122}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.43}{\ignorespaces Main code for training DDPG agent to solve the \texttt  {Pendulum-v1} problem. }}{123}{}\protected@file@percent }
\newlabel{lst:ddpg_main_pendu}{{6.43}{123}{Main Code to generate output}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Training performance of DDPG algorithm in solving \texttt  {Pendulum-v1} problem}}{123}{}\protected@file@percent }
\newlabel{fig:ddpg_pendu}{{6.6}{123}{Main Code to generate output}{}{}}
\citation{schulman2015trust}
\@writefile{toc}{\contentsline {section}{\numberline {6.6}Trust Region Policy Optimization}{124}{}\protected@file@percent }
\newlabel{sec:trpo}{{6.6}{124}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:old_dr}{{6.24}{124}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:new_dr}{{6.25}{124}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:new_dr_vf}{{6.26}{124}{Trust Region Policy Optimization}{}{}}
\citation{kakade2002approximately}
\newlabel{eq:dvf}{{6.27}{125}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:new_dr_appx}{{6.28}{125}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:kl_update}{{6.29}{125}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:kl_bound}{{6.30}{125}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:kld_policy}{{6.31}{125}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:mi}{{6.32}{125}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:eta1}{{6.33}{125}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:eta2}{{6.34}{125}{Trust Region Policy Optimization}{}{}}
\citation{schulman2017proximal}
\newlabel{eq:eta3}{{6.35}{126}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:kl_policy_update}{{6.36}{126}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:kl_policy_update_2}{{6.37}{126}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:avg_kld}{{6.38}{126}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:trpo}{{6.39}{126}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:trpo2}{{6.40}{126}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:ise}{{6.41}{126}{Trust Region Policy Optimization}{}{}}
\newlabel{eq:trpo3}{{6.42}{126}{Trust Region Policy Optimization}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.7}Proximal Policy Optimization (PPO) Algorithm}{127}{}\protected@file@percent }
\newlabel{sec:ppo}{{6.7}{127}{Proximal Policy Optimization (PPO) Algorithm}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.1}Clipped Surrogate Objective}{127}{}\protected@file@percent }
\newlabel{eq:pr}{{6.43}{127}{Clipped Surrogate Objective}{}{}}
\newlabel{eq:l_cpi}{{6.44}{127}{Clipped Surrogate Objective}{}{}}
\newlabel{eq:j_clip}{{6.45}{127}{Clipped Surrogate Objective}{}{}}
\newlabel{eq:ppo_clip}{{6.46}{127}{Clipped Surrogate Objective}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Single time step of surrogate function $J^{\text  {CLIP}}$ as a function of probability ratio $r$ for positive (left) and negative (right) advantages. The red circle shows the starting point of policy optimization}}{128}{}\protected@file@percent }
\newlabel{fig:ppo_clip_pr}{{6.7}{128}{Clipped Surrogate Objective}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.2}{\ignorespaces \small  \add@extra@listi {sml}PPO-Clip Algorithm}}{128}{}\protected@file@percent }
\newlabel{alg:ppo_clip}{{6.2}{128}{Clipped Surrogate Objective}{}{}}
\newlabel{eq:act_up_clip}{{6.47}{128}{Clipped Surrogate Objective}{}{}}
\newlabel{eq:crit_up}{{6.48}{128}{Clipped Surrogate Objective}{}{}}
\citation{schulman2015high}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.2}Adaptive KL Penalty Coefficient}{129}{}\protected@file@percent }
\newlabel{eq:ppo_klp}{{6.49}{129}{Adaptive KL Penalty Coefficient}{}{}}
\newlabel{eq:beta_update}{{6.50}{129}{Adaptive KL Penalty Coefficient}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {6.3}{\ignorespaces \small  \add@extra@listi {sml}PPO with Adaptive KL Penalty}}{129}{}\protected@file@percent }
\newlabel{alg:ppo_klp}{{6.3}{129}{Adaptive KL Penalty Coefficient}{}{}}
\newlabel{eq:act_up_penalty}{{6.51}{129}{Adaptive KL Penalty Coefficient}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.3}Generalized Advantage Estimator}{129}{}\protected@file@percent }
\newlabel{eq:gae}{{6.52}{129}{Generalized Advantage Estimator}{}{}}
\newlabel{eq:tde}{{6.53}{129}{Generalized Advantage Estimator}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.4}Python implementation of PPO Algorithm}{130}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline PPO Actor Class}{130}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.44}{\ignorespaces Python code for PPO Actor Class.}}{132}{}\protected@file@percent }
\newlabel{lst:ppo_actor}{{6.44}{132}{PPO Actor Class}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline PPO Critic Class}{132}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.45}{\ignorespaces Python code for PPO Critic Class}}{134}{}\protected@file@percent }
\newlabel{lst:ppo_critic}{{6.45}{134}{PPO Critic Class}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline PPO Agent Class}{134}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.46}{\ignorespaces Python Class definition for PPO Agent}}{137}{}\protected@file@percent }
\newlabel{lst:ppo_agent}{{6.46}{137}{PPO Agent Class}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Training a PPO agent}{137}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.47}{\ignorespaces Python code for training a PPO agent on a given Gym environment}}{139}{}\protected@file@percent }
\newlabel{lst:ppo_train}{{6.47}{139}{Training a PPO agent}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.5}Solving Pendulum environment using PPO}{139}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.48}{\ignorespaces Python Code for solving \texttt  {Pendulum-v1} environment by using a PPO agent.}}{140}{}\protected@file@percent }
\newlabel{lst:ppo_pendu}{{6.48}{140}{Solving Pendulum environment using PPO}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces PPO training performance for \texttt  {Pendulum-v1} environment}}{140}{}\protected@file@percent }
\newlabel{fig:ppo_pendu}{{6.8}{140}{Solving Pendulum environment using PPO}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.7.6}Solving LunarLander-v2 Continuous with PPO}{140}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {6.49}{\ignorespaces Solving LunarLander-v2 with PPO}}{142}{}\protected@file@percent }
\newlabel{lst:llv2_ppo}{{6.49}{142}{Solving LunarLander-v2 Continuous with PPO}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces PPO training performance for \texttt  {LunarLander-v2-Continuous} environment}}{142}{}\protected@file@percent }
\newlabel{fig:ppo_lunarlander}{{6.9}{142}{Solving LunarLander-v2 Continuous with PPO}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.8}Summary}{142}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Actor-Critic Models}{145}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Naive Actor-Critic Model}{145}{}\protected@file@percent }
\newlabel{sec:naive_ac}{{7.1}{145}{Naive Actor-Critic Model}{}{}}
\newlabel{eq:nac_cost}{{7.1}{145}{Naive Actor-Critic Model}{}{}}
\newlabel{eq:nac_disc_return}{{7.2}{146}{Naive Actor-Critic Model}{}{}}
\newlabel{eq:nac_pg_update}{{7.3}{146}{Naive Actor-Critic Model}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.1}Temporal Difference (TD) Error}{146}{}\protected@file@percent }
\newlabel{eq:nac_td_error}{{7.4}{146}{Temporal Difference (TD) Error}{}{}}
\newlabel{eq:nac_td_error_q}{{7.5}{146}{Temporal Difference (TD) Error}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.2}Advantage Function}{146}{}\protected@file@percent }
\newlabel{sec:nac_adv}{{7.1.2}{146}{Advantage Function}{}{}}
\newlabel{eq:nac_adv}{{7.6}{146}{Advantage Function}{}{}}
\newlabel{eq:nac_pg_update_adv}{{7.7}{146}{Advantage Function}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7.1}{\ignorespaces Actor-Critic Algorithm}}{147}{}\protected@file@percent }
\newlabel{alg:nac}{{7.1}{147}{Advantage Function}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.3}Advantages of Actor-Critic Algorithm}{147}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.4}Limitations of Naive Actor-Critic Algorithm}{147}{}\protected@file@percent }
\newlabel{eq:nac_actor_update}{{7.8}{147}{Limitations of Naive Actor-Critic Algorithm}{}{}}
\newlabel{eq:nac_actor_update_2}{{7.9}{148}{Limitations of Naive Actor-Critic Algorithm}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1.5}Python Code for implementing Actor-Critic Algorithm}{148}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {7.50}{\ignorespaces Python code for creating Actor Class}}{149}{}\protected@file@percent }
\newlabel{lst:nac_actor}{{7.50}{149}{Python Code for implementing Actor-Critic Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {7.51}{\ignorespaces Python code for creating Critic Class}}{150}{}\protected@file@percent }
\newlabel{lst:nac_critic}{{7.51}{150}{Python Code for implementing Actor-Critic Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {7.52}{\ignorespaces Python Code for Actor-Critic Agent Class}}{151}{}\protected@file@percent }
\newlabel{lst:nac_agent}{{7.52}{151}{Python Code for implementing Actor-Critic Algorithm}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Solving CartPole problem using Naive Actor-Critic Algorithm}{151}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {7.53}{\ignorespaces Function for training an actor-critic agent for a given Gym environment}}{152}{}\protected@file@percent }
\newlabel{lst:nac_train}{{7.53}{152}{Solving CartPole problem using Naive Actor-Critic Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {7.54}{\ignorespaces Python code for training an actor-critic agent to solve the \texttt  {CartPole-v1} problem.}}{153}{}\protected@file@percent }
\newlabel{lst:nac_cpv1}{{7.54}{153}{Solving CartPole problem using Naive Actor-Critic Algorithm}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Performance of Naive Actor Critic Algorithm in solving \texttt  {CartPole-v1} problem.}}{153}{}\protected@file@percent }
\newlabel{fig:nac_cpv1}{{7.1}{153}{Solving CartPole problem using Naive Actor-Critic Algorithm}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Solving Lunar Lander Problem using Naive Actor-Critic Algorithm}{153}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {7.55}{\ignorespaces Python code for training a actor-critic agent to solve the \texttt  {LunarLander-v3} problem.}}{154}{}\protected@file@percent }
\newlabel{lst:nac_ll}{{7.55}{154}{Solving Lunar Lander Problem using Naive Actor-Critic Algorithm}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Advantage Actor-Critic (A2C) Algorithm}{154}{}\protected@file@percent }
\newlabel{sec:a2c}{{7.2}{154}{Advantage Actor-Critic (A2C) Algorithm}{}{}}
\newlabel{eq:entropy}{{7.10}{154}{Advantage Actor-Critic (A2C) Algorithm}{}{}}
\newlabel{eq:a2c_actor_loss}{{7.11}{154}{Advantage Actor-Critic (A2C) Algorithm}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {7.2}{\ignorespaces Advantage Actor-Critic (A2C) Algorithm}}{155}{}\protected@file@percent }
\newlabel{alg:a2c}{{7.2}{155}{Advantage Actor-Critic (A2C) Algorithm}{}{}}
\newlabel{eq:adv_tde}{{7.12}{155}{Advantage Actor-Critic (A2C) Algorithm}{}{}}
\newlabel{eq:a2c_c_update}{{7.13}{155}{Advantage Actor-Critic (A2C) Algorithm}{}{}}
\newlabel{eq:a2c_a_update}{{7.14}{155}{Advantage Actor-Critic (A2C) Algorithm}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.1}Python Implementation of A2C Algorithm}{155}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {7.56}{\ignorespaces Python Code for implementing A2C agent class}}{157}{}\protected@file@percent }
\newlabel{lst:a2c_agent}{{7.56}{157}{Python Implementation of A2C Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {7.57}{\ignorespaces Code for computing TD error from discounted returns}}{159}{}\protected@file@percent }
\newlabel{lst:tde_dr}{{7.57}{159}{Python Implementation of A2C Algorithm}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {7.58}{\ignorespaces Training function for an A2C agent}}{160}{}\protected@file@percent }
\newlabel{lst:a2c_train}{{7.58}{160}{Python Implementation of A2C Algorithm}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2.2}Solving \texttt  {LunarLander-v3} with A2C algorithm}{160}{}\protected@file@percent }
\citation{sewak2019actor}
\@writefile{lol}{\contentsline {listing}{\numberline {7.59}{\ignorespaces Main function for applying A2C agent to solve \texttt  {LunarLander-v3} problem}}{162}{}\protected@file@percent }
\newlabel{lst:ll_a2c}{{7.59}{162}{Solving \texttt  {LunarLander-v3} with A2C algorithm}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Training performance of A2C algorithm for \texttt  {Lunarlander-v3} environment}}{162}{}\protected@file@percent }
\newlabel{fig:a2c_llv3}{{7.2}{162}{Solving \texttt  {LunarLander-v3} with A2C algorithm}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Asynchronous Advantage Actor-Critic (A3C) Model}{162}{}\protected@file@percent }
\newlabel{sec:a3c}{{7.3}{162}{Asynchronous Advantage Actor-Critic (A3C) Model}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces Block Diagram to understand A3C architecture}}{163}{}\protected@file@percent }
\newlabel{fig:a3c_bd}{{7.3}{163}{Asynchronous Advantage Actor-Critic (A3C) Model}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Python Implementation of A3C Model}{163}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline The \texttt  {worker} function}{163}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {7.60}{\ignorespaces Worker function that creates an agent to collect experiences from its own individual environment and computes gradient. These gradients are then sent to the global network asynchronously for update}}{166}{}\protected@file@percent }
\newlabel{lst:a3c_worker}{{7.60}{166}{The \texttt  {worker} function}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Running multiple workers}{166}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {7.61}{\ignorespaces Python Function to run multiple workers in parallel. The gradients are collected from each worker and used for updating the parameters of the global network asynchronously.}}{170}{}\protected@file@percent }
\newlabel{lst:a3c_runw}{{7.61}{170}{Running multiple workers}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Updating \texttt  {A2CAgent} to work with multiple workers}{170}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {7.62}{\ignorespaces Additional methods needed to use \texttt  {A2CAgent} with multiple workers. Specifically, the gradient computation is separated from the updating parameters stage.}}{172}{}\protected@file@percent }
\newlabel{lst:a3c_grad_comp}{{7.62}{172}{Updating \texttt  {A2CAgent} to work with multiple workers}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Solving \texttt  {LunarLander-v3} using A3C model}{172}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {7.63}{\ignorespaces Main python code for training A3C agent on \texttt  {LunarLander-v3} problem environment}}{174}{}\protected@file@percent }
\newlabel{lst:a3c_llv3}{{7.63}{174}{Solving \texttt  {LunarLander-v3} using A3C model}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces Training performance of A3C algorithm for \texttt  {LunarLander-v3} environment}}{174}{}\protected@file@percent }
\newlabel{fig:a3c_llv3}{{7.4}{174}{Solving \texttt  {LunarLander-v3} using A3C model}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Summary}{174}{}\protected@file@percent }
\citation{haarnoja2018soft}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Soft Actor-Critic: A Maximum Entropy Reinforcement Learning Algorithm}{175}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:sac_intro}{{8}{175}{Soft Actor-Critic: A Maximum Entropy Reinforcement Learning Algorithm}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}The Maximum Entropy Objective}{175}{}\protected@file@percent }
\newlabel{sec:max_entropy_objective}{{8.1}{175}{The Maximum Entropy Objective}{}{}}
\newlabel{eq:rl_obj}{{8.1}{175}{The Maximum Entropy Objective}{}{}}
\newlabel{eq:sac_obj}{{8.2}{175}{The Maximum Entropy Objective}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Soft Value Functions: The Foundation of SAC}{176}{}\protected@file@percent }
\newlabel{sec:soft_value_functions}{{8.2}{176}{Soft Value Functions: The Foundation of SAC}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}The Soft Q-Function}{176}{}\protected@file@percent }
\newlabel{subsec:soft_q_function}{{8.2.1}{176}{The Soft Q-Function}{}{}}
\newlabel{eq:soft_q}{{8.3}{176}{The Soft Q-Function}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}The Soft Value Function}{176}{}\protected@file@percent }
\newlabel{subsec:soft_value_function}{{8.2.2}{176}{The Soft Value Function}{}{}}
\newlabel{eq:soft_v}{{8.4}{176}{The Soft Value Function}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Architecture and Training Dynamics}{176}{}\protected@file@percent }
\newlabel{sec:sac_arch}{{8.3}{176}{Architecture and Training Dynamics}{}{}}
\newlabel{sec:architecture_training}{{8.3}{176}{Architecture and Training Dynamics}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Key Components:}{176}{}\protected@file@percent }
\newlabel{subsec:key_components}{{8.3.1}{176}{Key Components:}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Training Objectives and Updates:}{177}{}\protected@file@percent }
\newlabel{subsec:training_objectives}{{8.3.2}{177}{Training Objectives and Updates:}{}{}}
\newlabel{eq:sac_loss_critic}{{8.5}{177}{Training Objectives and Updates:}{}{}}
\newlabel{eq:sac_critic_target}{{8.6}{177}{Training Objectives and Updates:}{}{}}
\newlabel{eq:sac_actor_loss}{{8.7}{177}{Training Objectives and Updates:}{}{}}
\newlabel{eq:sac_alpha_loss}{{8.8}{178}{Training Objectives and Updates:}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.3}Soft Actor-Critic Pseudocode}{178}{}\protected@file@percent }
\newlabel{subsec:sac_pseudocode}{{8.3.3}{178}{Soft Actor-Critic Pseudocode}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Advantages of Soft Actor-Critic}{178}{}\protected@file@percent }
\newlabel{sec:advantages}{{8.4}{178}{Advantages of Soft Actor-Critic}{}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {8.1}{\ignorespaces Soft Actor-Critic (SAC)}}{179}{}\protected@file@percent }
\newlabel{alg:sac}{{8.1}{179}{Soft Actor-Critic Pseudocode}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Implementing SAC Algorithm using Python}{180}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}Actor Network}{180}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {8.64}{\ignorespaces Python code for the SAC Actor Class}}{181}{}\protected@file@percent }
\newlabel{lst:sac_actor}{{8.64}{181}{Actor Network}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}Critic Network}{181}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {8.65}{\ignorespaces Python code for the SAC Critic class definition}}{182}{}\protected@file@percent }
\newlabel{sec:sac_critic}{{8.65}{182}{Critic Network}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}SAC Agent}{182}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {8.66}{\ignorespaces Python code for implementing SAC agent class}}{186}{}\protected@file@percent }
\newlabel{lst:sac_agent}{{8.66}{186}{SAC Agent}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.4}Replay Buffer}{186}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {8.67}{\ignorespaces Method to sample experiences from the replay buffer and convert them into numpy arrays}}{187}{}\protected@file@percent }
\newlabel{lst:sample_unpacked}{{8.67}{187}{Replay Buffer}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.5}Value Network}{187}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {8.68}{\ignorespaces Python class for creating a value network}}{188}{}\protected@file@percent }
\newlabel{lst:value_net}{{8.68}{188}{Value Network}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.6}SAC Agent with a separate value network}{188}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {8.69}{\ignorespaces SAC agent class with a separate value network}}{190}{}\protected@file@percent }
\newlabel{lst:sac_agent_2}{{8.69}{190}{SAC Agent with a separate value network}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.7}Training a SAC agent}{190}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {8.70}{\ignorespaces Function for training a SAC agent on a given problem environment}}{191}{}\protected@file@percent }
\newlabel{lst:train_sac}{{8.70}{191}{Training a SAC agent}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.8}Solving \texttt  {Pendulum-v1} Problem using SAC}{191}{}\protected@file@percent }
\@writefile{lol}{\contentsline {listing}{\numberline {8.71}{\ignorespaces Main program for solving \texttt  {Pendulum-v1} using SAC}}{192}{}\protected@file@percent }
\newlabel{lst:pendu_sac}{{8.71}{192}{Solving \texttt  {Pendulum-v1} Problem using SAC}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Training performance of SAC2 agent for solving \texttt  {Pendulum-v1} problem}}{193}{}\protected@file@percent }
\newlabel{fig:pendu_sac2}{{8.1}{193}{Solving \texttt  {Pendulum-v1} Problem using SAC}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.9}Solving \texttt  {LunarLander-v3-Continuous} Problem using SAC}{193}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces \small  \add@extra@listi {sml}Parameters for \texttt  {LunarLanderContinuous-v3} Environment}}{193}{}\protected@file@percent }
\newlabel{tab:llv3cont_param}{{8.1}{193}{Solving \texttt  {LunarLander-v3-Continuous} Problem using SAC}{}{}}
\citation{plappert2018multi}
\citation{fetchreachgym}
\@writefile{lol}{\contentsline {listing}{\numberline {8.72}{\ignorespaces Code for training SAC agent to solve \texttt  {LunarLanderContinous-v3} environment}}{194}{}\protected@file@percent }
\newlabel{lst:llv3cont_sac}{{8.72}{194}{Solving \texttt  {LunarLander-v3-Continuous} Problem using SAC}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.10}Solving \texttt  {FetchReachDense-v3} problem with SAC}{194}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces Training performance of SAC agent on \texttt  {LunarLanderContinous-v3} environment}}{195}{}\protected@file@percent }
\newlabel{fig:llv3cont_sac}{{8.2}{195}{Solving \texttt  {LunarLander-v3-Continuous} Problem using SAC}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {8.2}{\ignorespaces \small  \add@extra@listi {sml}Parameters for \texttt  {FetchReachDense-v3} Environment}}{195}{}\protected@file@percent }
\newlabel{tab:freach_param}{{8.2}{195}{Solving \texttt  {FetchReachDense-v3} problem with SAC}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces \small  \add@extra@listi {sml}Screenshots of a few observation states of \texttt  {FetchReachDense-v3} environment. The red dot is the desired goal that the robot end-effector expected to reach for successful completion of task.}}{196}{}\protected@file@percent }
\newlabel{fig:fr_obs}{{8.3}{196}{Solving \texttt  {FetchReachDense-v3} problem with SAC}{}{}}
\@writefile{lol}{\contentsline {listing}{\numberline {8.73}{\ignorespaces \small  \add@extra@listi {sml}Main code for training SAC agent on \texttt  {FetchReachDense-v3} environment}}{197}{}\protected@file@percent }
\newlabel{lst:frd_sac}{{8.73}{197}{Solving \texttt  {FetchReachDense-v3} problem with SAC}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces \small  \add@extra@listi {sml}Training performance of SAC algorithm on \texttt  {FetchReachDense-v3} environment}}{197}{}\protected@file@percent }
\newlabel{fig:freach_sac}{{8.4}{197}{Solving \texttt  {FetchReachDense-v3} problem with SAC}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.11}Comparing SAC vs SAC2}{197}{}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {8.5}{\ignorespaces \small  \add@extra@listi {sml}SAC agent's soss functions: (a) Actor \& Alpha losses, (b) Value \& Critic losses}}{198}{}\protected@file@percent }
\newlabel{fig:frd_sac_loss}{{8.5}{198}{Solving \texttt  {FetchReachDense-v3} problem with SAC}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.6}{\ignorespaces Comparing performances of two implementations: SAC \& SAC2}}{198}{}\protected@file@percent }
\newlabel{fig:sac_comp}{{8.6}{198}{Comparing SAC vs SAC2}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Conclusion}{198}{}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Basics of Probability \& Statistics}{201}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:app01}{{A}{201}{Basics of Probability \& Statistics}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Theorems \& Definitions}{201}{}\protected@file@percent }
\newlabel{th:exp_add}{{A.1}{201}{Theorems \& Definitions}{}{}}
\newlabel{eq:bayes}{{A.1}{201}{Theorems \& Definitions}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Descriptions \& Explanations}{201}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2.1}State-Action Marginal Visitation Probability Distribution}{201}{}\protected@file@percent }
\newlabel{sec:samvp}{{A.2.1}{201}{State-Action Marginal Visitation Probability Distribution}{}{}}
\bibstyle{unsrt}
\bibdata{ref}
\newlabel{eq:savp}{{A.2}{202}{State-Action Marginal Visitation Probability Distribution}{}{}}
\bibcite{sutton2018reinforcement}{1}
\bibcite{norris1998markov}{2}
\bibcite{skinner1971operant}{3}
\bibcite{haney2020applied}{4}
\bibcite{arulkumaran2017brief}{5}
\bibcite{li2017deep}{6}
\bibcite{franccois2018introduction}{7}
\bibcite{silver2017mastering}{8}
\bibcite{bellman1966dynamic}{9}
\bibcite{gym}{10}
\bibcite{sutton1988learning}{11}
\bibcite{van2016deep}{12}
\bibcite{lillicrap2015continuous}{13}
\bibcite{polyak1992acceleration}{14}
\@writefile{toc}{\contentsline {chapter}{\nonumberline Bibliography}{203}{}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\bibcite{maller2009ornstein}{15}
\bibcite{schulman2015trust}{16}
\bibcite{kakade2002approximately}{17}
\bibcite{schulman2017proximal}{18}
\bibcite{schulman2015high}{19}
\bibcite{sewak2019actor}{20}
\bibcite{haarnoja2018soft}{21}
\bibcite{plappert2018multi}{22}
\bibcite{fetchreachgym}{23}
\@writefile{toc}{\contentsline {chapter}{Alphabetical Index}{205}{}\protected@file@percent }
\global\csname @altsecnumformattrue\endcsname
\global\@namedef{scr@dte@chapter@lastmaxnumwidth}{16.1933pt}
\global\@namedef{scr@dte@section@lastmaxnumwidth}{24.1507pt}
\global\@namedef{scr@dte@subsection@lastmaxnumwidth}{35.40482pt}
\global\@namedef{scr@dte@table@lastmaxnumwidth}{21.4132pt}
\global\@namedef{scr@dte@figure@lastmaxnumwidth}{21.4132pt}
\@writefile{toc}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\@writefile{lof}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\@writefile{lot}{\providecommand\tocbasic@end@toc@file{}\tocbasic@end@toc@file}
\gdef \@abspage@last{205}
