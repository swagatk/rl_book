\babel@toc {english}{}\relax 
\contentsline {chapter}{Acronyms}{7}{}%
\contentsline {chapter}{\nonumberline List of Figures}{9}{}%
\contentsline {chapter}{\nonumberline List of Algorithms}{11}{}%
\contentsline {chapter}{\nonumberline List of Tables}{13}{}%
\contentsline {chapter}{\numberline {1}Introduction to Reinforcement Learning}{15}{}%
\contentsline {section}{\numberline {1.1}What is Reinforcement Learning?}{15}{}%
\contentsline {section}{\numberline {1.2}Reinforcement Learning Problem}{15}{}%
\contentsline {subsection}{\numberline {1.2.1}The Components of a Reinforcement Learning System}{15}{}%
\contentsline {subsection}{\numberline {1.2.2}Problem Formulation}{15}{}%
\contentsline {section}{\numberline {1.3}Types of Reinforcement Learning}{17}{}%
\contentsline {section}{\numberline {1.4}Types of RL Environment}{18}{}%
\contentsline {section}{\numberline {1.5}Reinforcement Learning Algorithms}{18}{}%
\contentsline {subsection}{\numberline {1.5.1}Applications of Reinforcement Learning}{18}{}%
\contentsline {section}{\numberline {1.6}The history of Reinforcement Learning}{19}{}%
\contentsline {chapter}{\numberline {2}Markov Decision Process and Dynamic Programming}{21}{}%
\contentsline {section}{\numberline {2.1}Markov chain and Markov Decision process}{21}{}%
\contentsline {subsection}{\numberline {2.1.1}Rewards and Returns}{21}{}%
\contentsline {subsection}{\numberline {2.1.2}The policy function}{22}{}%
\contentsline {subsection}{\numberline {2.1.3}State-Value function}{22}{}%
\contentsline {subsection}{\numberline {2.1.4}State-action value function (Q-function)}{23}{}%
\contentsline {subsection}{\numberline {2.1.5}Optimality}{23}{}%
\contentsline {subsection}{\numberline {2.1.6}Bellman Equation}{23}{}%
\contentsline {subsection}{\numberline {2.1.7}Deriving Bellman equation}{24}{}%
\contentsline {section}{\numberline {2.2}Solving the Bellman Equation}{25}{}%
\contentsline {subsection}{\numberline {2.2.1}Dynamic Programming}{25}{}%
\contentsline {subsection}{\numberline {2.2.2}Value Iteration}{26}{}%
\contentsline {subsection}{\numberline {2.2.3}Policy Iteration}{28}{}%
\contentsline {subsection}{\numberline {2.2.4}Solving the Frozen Lake Problem}{32}{}%
\contentsline {subsection}{\numberline {2.2.5}Solving the Taxi Problem}{33}{}%
\contentsline {section}{\numberline {2.3}Summary}{35}{}%
\contentsline {chapter}{\numberline {3}Monte Carlo Simulation for Reinforcement Learning}{37}{}%
\contentsline {section}{\numberline {3.1}Introduction}{37}{}%
\contentsline {section}{\numberline {3.2}What is Monte Carlo simulation?}{37}{}%
\contentsline {subsection}{\numberline {3.2.1}Tossing a Coin}{37}{}%
\contentsline {subsection}{\numberline {3.2.2}Estimating the value of pi using Monte Carlo}{37}{}%
\contentsline {section}{\numberline {3.3}Monte Carlo Prediction}{39}{}%
\contentsline {subsection}{\numberline {3.3.1}First-Visit Monte-Carlo Algorithm}{40}{}%
\contentsline {subsection}{\numberline {3.3.2}Every-visit Monte-Carlo Algorithm}{40}{}%
\contentsline {subsection}{\numberline {3.3.3}Playing Blackjack Game}{41}{}%
\contentsline {section}{\numberline {3.4}Monte Carlo Control}{48}{}%
\contentsline {subsection}{\numberline {3.4.1}On-Policy \& Off-Policy Monte Carlo Algorithms}{53}{}%
\contentsline {section}{\numberline {3.5}challenges of Using Monte Carlo Simulation for Reinforcement Learning}{53}{}%
\contentsline {section}{\numberline {3.6}Conclusion}{53}{}%
\contentsline {chapter}{\numberline {4}Temporal Difference Learning}{55}{}%
\contentsline {section}{\numberline {4.1}Introduction}{55}{}%
\contentsline {section}{\numberline {4.2}TD Learning}{55}{}%
\contentsline {section}{\numberline {4.3}TD Control Algorithms}{56}{}%
\contentsline {section}{\numberline {4.4}Q Learning}{56}{}%
\contentsline {section}{\numberline {4.5}SARSA Algorithm}{60}{}%
\contentsline {section}{\numberline {4.6}Difference between Q-Learning and SARSA Algorithms}{64}{}%
\contentsline {section}{\numberline {4.7}Conclusion}{65}{}%
\contentsline {chapter}{\numberline {5}Deep Q Network}{67}{}%
\contentsline {section}{\numberline {5.1}Introduction}{67}{}%
\contentsline {section}{\numberline {5.2}DQN Algorithm}{67}{}%
\contentsline {section}{\numberline {5.3}Double DQN Algorithm}{68}{}%
\contentsline {section}{\numberline {5.4}Python implementation of DQN Algorithm}{69}{}%
\contentsline {subsection}{\numberline {5.4.1}Replay Buffer}{69}{}%
\contentsline {subsection}{\numberline {5.4.2}The DQN Class}{71}{}%
\contentsline {subsection}{\numberline {5.4.3}Epsilon-Greedy Policy}{73}{}%
\contentsline {subsection}{\numberline {5.4.4}Experience Replay}{73}{}%
\contentsline {subsection}{\numberline {5.4.5}Training an agent for solving a Gym Problem}{75}{}%
\contentsline {subsection}{\numberline {5.4.6}Solving the CartPole problem using DQN algorithm}{77}{}%
\contentsline {section}{\numberline {5.5}Priority Experience Replay (PER)}{79}{}%
\contentsline {subsection}{\numberline {5.5.1}Sum-Tree Data Structure for Priority Replay Buffer}{80}{}%
\contentsline {subsubsection}{\nonumberline Adding experience to the replay buffer}{80}{}%
\contentsline {subsubsection}{\nonumberline Retrieving samples from the buffer}{81}{}%
\contentsline {subsection}{\numberline {5.5.2}Python Implementation of Sum-Tree class}{82}{}%
\contentsline {subsection}{\numberline {5.5.3}Python implementation of Priority Replay Buffer using Sum-Tree}{84}{}%
\contentsline {subsection}{\numberline {5.5.4}Python code for DQN Agent with PER}{85}{}%
\contentsline {subsection}{\numberline {5.5.5}Solving MountainCar Problem using DQN with PER}{87}{}%
\contentsline {subsubsection}{\nonumberline Training the agent}{87}{}%
\contentsline {subsubsection}{\nonumberline Main Code for creating and training agent}{89}{}%
\contentsline {section}{\numberline {5.6}Solving Atari Games using DQN}{92}{}%
\contentsline {subsection}{\numberline {5.6.1}Image Stacking Wrapper}{93}{}%
\contentsline {subsection}{\numberline {5.6.2}DQN Agent for Atari Environments}{95}{}%
\contentsline {subsection}{\numberline {5.6.3}Training agent on Atari PacMan Environment}{97}{}%
\contentsline {section}{\numberline {5.7}Summary}{99}{}%
\contentsline {chapter}{\numberline {6}Policy Gradient Methods}{101}{}%
\contentsline {section}{\numberline {6.1}Introduction}{101}{}%
\contentsline {section}{\numberline {6.2}Policy Gradient}{101}{}%
\contentsline {subsection}{\numberline {6.2.1}Computing $\nabla _\theta J(\theta )$}{102}{}%
\contentsline {section}{\numberline {6.3}Monte-Carlo Policy Gradient Algorithm}{104}{}%
\contentsline {subsection}{\numberline {6.3.1}Python Code for REINFORCE Algorithm}{105}{}%
\contentsline {subsection}{\numberline {6.3.2}Solving CartPole Problem}{108}{}%
\contentsline {subsection}{\numberline {6.3.3}Solving Lunar-Lander Problem}{109}{}%
\contentsline {section}{\numberline {6.4}Actor-Critic Model}{111}{}%
\contentsline {section}{\numberline {6.5}Deep Deterministic Policy Gradient}{111}{}%
\contentsline {subsection}{\numberline {6.5.1}Python Implementation of DDPG algorithm}{113}{}%
\contentsline {subsubsection}{\nonumberline Actor Model}{114}{}%
\contentsline {subsubsection}{\nonumberline Critic Class}{116}{}%
\contentsline {subsubsection}{\nonumberline DDPG Agent}{118}{}%
\contentsline {subsection}{\numberline {6.5.2}Solving Pendulum Problem}{120}{}%
\contentsline {subsubsection}{\nonumberline Training DDPG Agent}{121}{}%
\contentsline {subsection}{\numberline {6.5.3}Main Code to generate output}{122}{}%
\contentsline {section}{\numberline {6.6}Trust Region Policy Optimization}{124}{}%
\contentsline {section}{\numberline {6.7}Proximal Policy Optimization (PPO) Algorithm}{127}{}%
\contentsline {subsection}{\numberline {6.7.1}Clipped Surrogate Objective}{127}{}%
\contentsline {subsection}{\numberline {6.7.2}Adaptive KL Penalty Coefficient}{129}{}%
\contentsline {subsection}{\numberline {6.7.3}Generalized Advantage Estimator}{129}{}%
\contentsline {subsection}{\numberline {6.7.4}Python implementation of PPO Algorithm}{130}{}%
\contentsline {subsubsection}{\nonumberline PPO Actor Class}{130}{}%
\contentsline {subsubsection}{\nonumberline PPO Critic Class}{132}{}%
\contentsline {subsubsection}{\nonumberline PPO Agent Class}{134}{}%
\contentsline {subsubsection}{\nonumberline Training a PPO agent}{137}{}%
\contentsline {subsection}{\numberline {6.7.5}Solving Pendulum environment using PPO}{139}{}%
\contentsline {subsection}{\numberline {6.7.6}Solving LunarLander-v2 Continuous with PPO}{140}{}%
\contentsline {section}{\numberline {6.8}Summary}{142}{}%
\contentsline {chapter}{\numberline {7}Actor-Critic Models}{145}{}%
\contentsline {section}{\numberline {7.1}Naive Actor-Critic Model}{145}{}%
\contentsline {subsection}{\numberline {7.1.1}Temporal Difference (TD) Error}{146}{}%
\contentsline {subsection}{\numberline {7.1.2}Advantage Function}{146}{}%
\contentsline {subsection}{\numberline {7.1.3}Advantages of Actor-Critic Algorithm}{147}{}%
\contentsline {subsection}{\numberline {7.1.4}Limitations of Naive Actor-Critic Algorithm}{147}{}%
\contentsline {subsection}{\numberline {7.1.5}Python Code for implementing Actor-Critic Algorithm}{148}{}%
\contentsline {subsubsection}{\nonumberline Solving CartPole problem using Naive Actor-Critic Algorithm}{151}{}%
\contentsline {subsubsection}{\nonumberline Solving Lunar Lander Problem using Naive Actor-Critic Algorithm}{153}{}%
\contentsline {section}{\numberline {7.2}Advantage Actor-Critic (A2C) Algorithm}{154}{}%
\contentsline {subsection}{\numberline {7.2.1}Python Implementation of A2C Algorithm}{155}{}%
\contentsline {subsection}{\numberline {7.2.2}Solving \texttt {LunarLander-v3} with A2C algorithm}{160}{}%
\contentsline {section}{\numberline {7.3}Asynchronous Advantage Actor-Critic (A3C) Model}{162}{}%
\contentsline {subsection}{\numberline {7.3.1}Python Implementation of A3C Model}{163}{}%
\contentsline {subsubsection}{\nonumberline The \texttt {worker} function}{163}{}%
\contentsline {subsubsection}{\nonumberline Running multiple workers}{166}{}%
\contentsline {subsubsection}{\nonumberline Updating \texttt {A2CAgent} to work with multiple workers}{170}{}%
\contentsline {subsection}{\numberline {7.3.2}Solving \texttt {LunarLander-v3} using A3C model}{172}{}%
\contentsline {section}{\numberline {7.4}Summary}{174}{}%
\contentsline {chapter}{\numberline {8}Soft Actor-Critic: A Maximum Entropy Reinforcement Learning Algorithm}{175}{}%
\contentsline {section}{\numberline {8.1}The Maximum Entropy Objective}{175}{}%
\contentsline {section}{\numberline {8.2}Soft Value Functions: The Foundation of SAC}{176}{}%
\contentsline {subsection}{\numberline {8.2.1}The Soft Q-Function}{176}{}%
\contentsline {subsection}{\numberline {8.2.2}The Soft Value Function}{176}{}%
\contentsline {section}{\numberline {8.3}Architecture and Training Dynamics}{176}{}%
\contentsline {subsection}{\numberline {8.3.1}Key Components:}{176}{}%
\contentsline {subsection}{\numberline {8.3.2}Training Objectives and Updates:}{177}{}%
\contentsline {subsection}{\numberline {8.3.3}Soft Actor-Critic Pseudocode}{178}{}%
\contentsline {section}{\numberline {8.4}Advantages of Soft Actor-Critic}{178}{}%
\contentsline {section}{\numberline {8.5}Implementing SAC Algorithm using Python}{180}{}%
\contentsline {subsection}{\numberline {8.5.1}Actor Network}{180}{}%
\contentsline {subsection}{\numberline {8.5.2}Critic Network}{181}{}%
\contentsline {subsection}{\numberline {8.5.3}SAC Agent}{182}{}%
\contentsline {subsection}{\numberline {8.5.4}Replay Buffer}{186}{}%
\contentsline {subsection}{\numberline {8.5.5}Value Network}{187}{}%
\contentsline {subsection}{\numberline {8.5.6}SAC Agent with a separate value network}{188}{}%
\contentsline {subsection}{\numberline {8.5.7}Training a SAC agent}{190}{}%
\contentsline {subsection}{\numberline {8.5.8}Solving \texttt {Pendulum-v1} Problem using SAC}{191}{}%
\contentsline {subsection}{\numberline {8.5.9}Solving \texttt {LunarLander-v3-Continuous} Problem using SAC}{193}{}%
\contentsline {subsection}{\numberline {8.5.10}Solving \texttt {FetchReachDense-v3} problem with SAC}{194}{}%
\contentsline {subsection}{\numberline {8.5.11}Comparing SAC vs SAC2}{197}{}%
\contentsline {section}{\numberline {8.6}Conclusion}{198}{}%
\contentsline {chapter}{\numberline {A}Basics of Probability \& Statistics}{201}{}%
\contentsline {section}{\numberline {A.1}Theorems \& Definitions}{201}{}%
\contentsline {section}{\numberline {A.2}Descriptions \& Explanations}{201}{}%
\contentsline {subsection}{\numberline {A.2.1}State-Action Marginal Visitation Probability Distribution}{201}{}%
\contentsline {chapter}{\nonumberline Bibliography}{203}{}%
\contentsline {chapter}{Alphabetical Index}{205}{}%
\providecommand \tocbasic@end@toc@file {}\tocbasic@end@toc@file 
