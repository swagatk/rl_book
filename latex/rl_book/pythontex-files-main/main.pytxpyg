
\makeatletter
\def\PYG@reset{\let\PYG@it=\relax \let\PYG@bf=\relax%
    \let\PYG@ul=\relax \let\PYG@tc=\relax%
    \let\PYG@bc=\relax \let\PYG@ff=\relax}
\def\PYG@tok#1{\csname PYG@tok@#1\endcsname}
\def\PYG@toks#1+{\ifx\relax#1\empty\else%
    \PYG@tok{#1}\expandafter\PYG@toks\fi}
\def\PYG@do#1{\PYG@bc{\PYG@tc{\PYG@ul{%
    \PYG@it{\PYG@bf{\PYG@ff{#1}}}}}}}
\def\PYG#1#2{\PYG@reset\PYG@toks#1+\relax+\PYG@do{#2}}

\@namedef{PYG@tok@w}{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PYG@tok@c}{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYG@tok@cp}{\def\PYG@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PYG@tok@k}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@kp}{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@kt}{\def\PYG@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PYG@tok@o}{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYG@tok@ow}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PYG@tok@nb}{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@nf}{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PYG@tok@nc}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PYG@tok@nn}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PYG@tok@ne}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PYG@tok@nv}{\def\PYG@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYG@tok@no}{\def\PYG@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PYG@tok@nl}{\def\PYG@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PYG@tok@ni}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PYG@tok@na}{\def\PYG@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PYG@tok@nt}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@nd}{\def\PYG@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PYG@tok@s}{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYG@tok@sd}{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYG@tok@si}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PYG@tok@se}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PYG@tok@sr}{\def\PYG@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PYG@tok@ss}{\def\PYG@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYG@tok@sx}{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@m}{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYG@tok@gh}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PYG@tok@gu}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PYG@tok@gd}{\def\PYG@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PYG@tok@gi}{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PYG@tok@gr}{\def\PYG@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PYG@tok@ge}{\let\PYG@it=\textit}
\@namedef{PYG@tok@gs}{\let\PYG@bf=\textbf}
\@namedef{PYG@tok@ges}{\let\PYG@bf=\textbf\let\PYG@it=\textit}
\@namedef{PYG@tok@gp}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PYG@tok@go}{\def\PYG@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PYG@tok@gt}{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PYG@tok@err}{\def\PYG@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PYG@tok@kc}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@kd}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@kn}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@kr}{\let\PYG@bf=\textbf\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@bp}{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYG@tok@fm}{\def\PYG@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PYG@tok@vc}{\def\PYG@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYG@tok@vg}{\def\PYG@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYG@tok@vi}{\def\PYG@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYG@tok@vm}{\def\PYG@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYG@tok@sa}{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYG@tok@sb}{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYG@tok@sc}{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYG@tok@dl}{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYG@tok@s2}{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYG@tok@sh}{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYG@tok@s1}{\def\PYG@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYG@tok@mb}{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYG@tok@mf}{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYG@tok@mh}{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYG@tok@mi}{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYG@tok@il}{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYG@tok@mo}{\def\PYG@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYG@tok@ch}{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYG@tok@cm}{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYG@tok@cpf}{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYG@tok@c1}{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYG@tok@cs}{\let\PYG@it=\textit\def\PYG@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYGZbs{\char`\\}
\def\PYGZus{\char`\_}
\def\PYGZob{\char`\{}
\def\PYGZcb{\char`\}}
\def\PYGZca{\char`\^}
\def\PYGZam{\char`\&}
\def\PYGZlt{\char`\<}
\def\PYGZgt{\char`\>}
\def\PYGZsh{\char`\#}
\def\PYGZpc{\char`\%}
\def\PYGZdl{\char`\$}
\def\PYGZhy{\char`\-}
\def\PYGZsq{\char`\'}
\def\PYGZdq{\char`\"}
\def\PYGZti{\char`\~}
% for compatibility with earlier versions
\def\PYGZat{@}
\def\PYGZlb{[}
\def\PYGZrb{]}
\makeatother

\makeatletter
\def\PYGdefault@reset{\let\PYGdefault@it=\relax \let\PYGdefault@bf=\relax%
    \let\PYGdefault@ul=\relax \let\PYGdefault@tc=\relax%
    \let\PYGdefault@bc=\relax \let\PYGdefault@ff=\relax}
\def\PYGdefault@tok#1{\csname PYGdefault@tok@#1\endcsname}
\def\PYGdefault@toks#1+{\ifx\relax#1\empty\else%
    \PYGdefault@tok{#1}\expandafter\PYGdefault@toks\fi}
\def\PYGdefault@do#1{\PYGdefault@bc{\PYGdefault@tc{\PYGdefault@ul{%
    \PYGdefault@it{\PYGdefault@bf{\PYGdefault@ff{#1}}}}}}}
\def\PYGdefault#1#2{\PYGdefault@reset\PYGdefault@toks#1+\relax+\PYGdefault@do{#2}}

\@namedef{PYGdefault@tok@w}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PYGdefault@tok@c}{\let\PYGdefault@it=\textit\def\PYGdefault@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYGdefault@tok@cp}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PYGdefault@tok@k}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@kp}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@kt}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PYGdefault@tok@o}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYGdefault@tok@ow}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PYGdefault@tok@nb}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@nf}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PYGdefault@tok@nc}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PYGdefault@tok@nn}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PYGdefault@tok@ne}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PYGdefault@tok@nv}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYGdefault@tok@no}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PYGdefault@tok@nl}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PYGdefault@tok@ni}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PYGdefault@tok@na}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PYGdefault@tok@nt}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@nd}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PYGdefault@tok@s}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYGdefault@tok@sd}{\let\PYGdefault@it=\textit\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYGdefault@tok@si}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PYGdefault@tok@se}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PYGdefault@tok@sr}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PYGdefault@tok@ss}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYGdefault@tok@sx}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@m}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYGdefault@tok@gh}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PYGdefault@tok@gu}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PYGdefault@tok@gd}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PYGdefault@tok@gi}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PYGdefault@tok@gr}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PYGdefault@tok@ge}{\let\PYGdefault@it=\textit}
\@namedef{PYGdefault@tok@gs}{\let\PYGdefault@bf=\textbf}
\@namedef{PYGdefault@tok@ges}{\let\PYGdefault@bf=\textbf\let\PYGdefault@it=\textit}
\@namedef{PYGdefault@tok@gp}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PYGdefault@tok@go}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PYGdefault@tok@gt}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PYGdefault@tok@err}{\def\PYGdefault@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PYGdefault@tok@kc}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@kd}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@kn}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@kr}{\let\PYGdefault@bf=\textbf\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@bp}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PYGdefault@tok@fm}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PYGdefault@tok@vc}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYGdefault@tok@vg}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYGdefault@tok@vi}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYGdefault@tok@vm}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PYGdefault@tok@sa}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYGdefault@tok@sb}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYGdefault@tok@sc}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYGdefault@tok@dl}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYGdefault@tok@s2}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYGdefault@tok@sh}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYGdefault@tok@s1}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PYGdefault@tok@mb}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYGdefault@tok@mf}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYGdefault@tok@mh}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYGdefault@tok@mi}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYGdefault@tok@il}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYGdefault@tok@mo}{\def\PYGdefault@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PYGdefault@tok@ch}{\let\PYGdefault@it=\textit\def\PYGdefault@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYGdefault@tok@cm}{\let\PYGdefault@it=\textit\def\PYGdefault@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYGdefault@tok@cpf}{\let\PYGdefault@it=\textit\def\PYGdefault@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYGdefault@tok@c1}{\let\PYGdefault@it=\textit\def\PYGdefault@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PYGdefault@tok@cs}{\let\PYGdefault@it=\textit\def\PYGdefault@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYGdefaultZbs{\char`\\}
\def\PYGdefaultZus{\char`\_}
\def\PYGdefaultZob{\char`\{}
\def\PYGdefaultZcb{\char`\}}
\def\PYGdefaultZca{\char`\^}
\def\PYGdefaultZam{\char`\&}
\def\PYGdefaultZlt{\char`\<}
\def\PYGdefaultZgt{\char`\>}
\def\PYGdefaultZsh{\char`\#}
\def\PYGdefaultZpc{\char`\%}
\def\PYGdefaultZdl{\char`\$}
\def\PYGdefaultZhy{\char`\-}
\def\PYGdefaultZsq{\char`\'}
\def\PYGdefaultZdq{\char`\"}
\def\PYGdefaultZti{\char`\~}
% for compatibility with earlier versions
\def\PYGdefaultZat{@}
\def\PYGdefaultZlb{[}
\def\PYGdefaultZrb{]}
\makeatother
\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@0}
\PYG{k}{class} \PYG{n+nc}{ValueIterationAgent}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{max\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env} \PYG{o}{=} \PYG{n}{env}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}iterations} \PYG{o}{=} \PYG{n}{max\PYGZus{}iterations}

  \PYG{k}{def} \PYG{n+nf}{value\PYGZus{}iteration}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{threshold}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}20}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{value\PYGZus{}table} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}iterations}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{updated\PYGZus{}value\PYGZus{}table} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{n}{value\PYGZus{}table}\PYG{p}{)}
      \PYG{k}{for} \PYG{n}{state} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{Q\PYGZus{}value} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} Q(s,a)}
        \PYG{k}{for} \PYG{n}{action} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
          \PYG{n}{Q\PYGZus{}value}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}
          \PYG{p}{[}\PYG{n}{trans\PYGZus{}prob} \PYG{o}{*} \PYGZbs{}
          \PYG{p}{(}\PYG{n}{reward} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{updated\PYGZus{}value\PYGZus{}table}\PYG{p}{[}\PYG{n}{next\PYGZus{}state}\PYG{p}{]}\PYG{p}{)} \PYGZbs{}
          \PYG{k}{for} \PYG{n}{trans\PYGZus{}prob}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYGZbs{}
                           \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{P}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{value\PYGZus{}table}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]} \PYG{o}{=}  \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{Q\PYGZus{}value}\PYG{p}{)}
      \PYG{k}{if} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}
            \PYG{n}{np}\PYG{o}{.}\PYG{n}{fabs}\PYG{p}{(}\PYG{n}{updated\PYGZus{}value\PYGZus{}table} \PYG{o}{\PYGZhy{}} \PYG{n}{value\PYGZus{}table}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{threshold}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Value\PYGZhy{}iteration converged at iteration \PYGZsh{} }\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s2}{\PYGZdq{}} \PYG{o}{\PYGZpc{}} \PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{break}
    \PYG{k}{return} \PYG{n}{value\PYGZus{}table}

  \PYG{k}{def} \PYG{n+nf}{extract\PYGZus{}policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{value\PYGZus{}table}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{policy} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{state} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{Q\PYGZus{}table} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}
      \PYG{k}{for} \PYG{n}{action} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{Q\PYGZus{}table}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}
        \PYG{p}{[}\PYG{n}{trans\PYGZus{}prob} \PYG{o}{*} \PYGZbs{}
        \PYG{p}{(}\PYG{n}{reward} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{value\PYGZus{}table}\PYG{p}{[}\PYG{n}{next\PYGZus{}state}\PYG{p}{]}\PYG{p}{)}\PYGZbs{}
        \PYG{k}{for} \PYG{n}{trans\PYGZus{}prob}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYGZbs{}
                  \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{P}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
      \PYG{n}{policy}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{Q\PYGZus{}table}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{policy}

  \PYG{k}{def} \PYG{n+nf}{train\PYGZus{}and\PYGZus{}validate}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Compute optimal policy}
    \PYG{n}{optimal\PYGZus{}value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value\PYGZus{}iteration}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{optimal\PYGZus{}policy} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{extract\PYGZus{}policy}\PYG{p}{(}\PYG{n}{optimal\PYGZus{}value}\PYG{p}{)}
    \PYG{n}{ep\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{ep\PYGZus{}steps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{rewards} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
      \PYG{n}{state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
      \PYG{n}{step} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
        \PYG{n}{step} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{optimal\PYGZus{}policy}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}
        \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{screen} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{render}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
      \PYG{n}{ep\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}
      \PYG{n}{ep\PYGZus{}steps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{step}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}steps}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}del\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@1}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{k}{class} \PYG{n+nc}{PolicyIterationAgent}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{max\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env} \PYG{o}{=} \PYG{n}{env}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}iterations} \PYG{o}{=} \PYG{n}{max\PYGZus{}iterations}

  \PYG{k}{def} \PYG{n+nf}{evaluate\PYGZus{}policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{policy}\PYG{p}{,} \PYG{n}{threshold}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{value\PYGZus{}table} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}
    \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
      \PYG{n}{updated\PYGZus{}value\PYGZus{}table} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{n}{value\PYGZus{}table}\PYG{p}{)}
      \PYG{k}{for} \PYG{n}{state} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{policy}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}
        \PYG{n}{value\PYGZus{}table}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}
        \PYG{p}{[}\PYG{n}{trans\PYGZus{}prob} \PYG{o}{*}   \PYGZbs{}
        \PYG{p}{(}\PYG{n}{reward} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{updated\PYGZus{}value\PYGZus{}table}\PYG{p}{[}\PYG{n}{next\PYGZus{}state}\PYG{p}{]}\PYG{p}{)}\PYGZbs{}
        \PYG{k}{for} \PYG{n}{trans\PYGZus{}prob}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n}{env}\PYG{o}{.}\PYG{n}{P}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
      \PYG{k}{if} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}
         \PYG{n}{np}\PYG{o}{.}\PYG{n}{fabs}\PYG{p}{(}\PYG{n}{updated\PYGZus{}value\PYGZus{}table} \PYG{o}{\PYGZhy{}} \PYG{n}{value\PYGZus{}table}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{threshold}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{break}
    \PYG{k}{return} \PYG{n}{value\PYGZus{}table}

  \PYG{k}{def} \PYG{n+nf}{improve\PYGZus{}policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{value\PYGZus{}table}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{policy} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{state} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{Q\PYGZus{}table} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}
      \PYG{k}{for} \PYG{n}{action} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{Q\PYGZus{}table}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}
        \PYG{p}{[}\PYG{n}{trans\PYGZus{}prob} \PYG{o}{*} \PYGZbs{}
        \PYG{p}{(}\PYG{n}{reward} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{value\PYGZus{}table}\PYG{p}{[}\PYG{n}{next\PYGZus{}state}\PYG{p}{]}\PYG{p}{)} \PYGZbs{}
        \PYG{k}{for} \PYG{n}{trans\PYGZus{}prob}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYGZbs{}
                     \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{P}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
      \PYG{n}{policy}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{Q\PYGZus{}table}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{policy}

  \PYG{k}{def} \PYG{n+nf}{policy\PYGZus{}iteration}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{current\PYGZus{}policy} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}iterations}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{new\PYGZus{}value\PYGZus{}function} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{evaluate\PYGZus{}policy}\PYG{p}{(}\PYG{n}{current\PYGZus{}policy}\PYG{p}{)}
      \PYG{n}{new\PYGZus{}policy} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{improve\PYGZus{}policy}\PYG{p}{(}\PYG{n}{new\PYGZus{}value\PYGZus{}function}\PYG{p}{)}
      \PYG{k}{if} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{all}\PYG{p}{(}\PYG{n}{current\PYGZus{}policy} \PYG{o}{==} \PYG{n}{new\PYGZus{}policy}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Policy iteration converged at step }\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{current\PYGZus{}policy} \PYG{o}{=} \PYG{n}{new\PYGZus{}policy}
        \PYG{k}{break}
      \PYG{n}{current\PYGZus{}policy} \PYG{o}{=} \PYG{n}{new\PYGZus{}policy}
    \PYG{k}{return} \PYG{n}{new\PYGZus{}policy}

  \PYG{k}{def} \PYG{n+nf}{train\PYGZus{}and\PYGZus{}validate}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} compute optimal policy}
    \PYG{n}{optimal\PYGZus{}policy} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{policy\PYGZus{}iteration}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{ep\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{ep\PYGZus{}steps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{rewards} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
      \PYG{n}{state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
      \PYG{n}{step} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
        \PYG{n}{step} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{optimal\PYGZus{}policy}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}
        \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
      \PYG{n}{ep\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}
      \PYG{n}{ep\PYGZus{}steps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{step}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}steps}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}del\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} destructor}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@2}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k}{class} \PYG{n+nc}{PolicyIterationAgent2}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{max\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{100000}\PYG{p}{,} \PYG{n}{threshold}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}6}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env} \PYG{o}{=} \PYG{n}{env}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}iterations} \PYG{o}{=} \PYG{n}{max\PYGZus{}iterations}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{threshold} \PYG{o}{=} \PYG{n}{threshold}

  \PYG{k}{def} \PYG{n+nf}{policy\PYGZus{}evaluation}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{policy}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{value\PYGZus{}fn} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}
    \PYG{n}{i} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
      \PYG{n}{i} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
      \PYG{n}{prev\PYGZus{}value\PYGZus{}fn} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{n}{value\PYGZus{}fn}\PYG{p}{)}
      \PYG{k}{for} \PYG{n}{state} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{outersum} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{k}{for} \PYG{n}{action} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
          \PYG{n}{q\PYGZus{}value} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}
              \PYG{p}{[}\PYG{n}{trans\PYGZus{}prob} \PYG{o}{*} \PYGZbs{}
              \PYG{p}{(}\PYG{n}{reward} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{prev\PYGZus{}value\PYGZus{}fn}\PYG{p}{[}\PYG{n}{next\PYGZus{}state}\PYG{p}{]}\PYG{p}{)} \PYGZbs{}
              \PYG{k}{for} \PYG{n}{trans\PYGZus{}prob}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYGZbs{}
                                \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{P}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
          \PYG{n}{outersum} \PYG{o}{+}\PYG{o}{=} \PYG{n}{policy}\PYG{p}{[}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{]} \PYG{o}{*} \PYG{n}{q\PYGZus{}value}
        \PYG{n}{value\PYGZus{}fn}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]} \PYG{o}{=} \PYG{n}{outersum}
      \PYG{k}{if} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{fabs}\PYG{p}{(}\PYG{n}{prev\PYGZus{}value\PYGZus{}fn} \PYG{o}{\PYGZhy{}} \PYG{n}{value\PYGZus{}fn}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{threshold}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Value convergences in }\PYG{l+s+si}{\PYGZpc{}d}\PYG{l+s+s1}{ iteration}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{\PYGZpc{}}\PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{break}
    \PYG{k}{return} \PYG{n}{value\PYGZus{}fn}

  \PYG{k}{def} \PYG{n+nf}{policy\PYGZus{}improvement}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{value\PYGZus{}fn}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{q\PYGZus{}value} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{improved\PYGZus{}policy} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{state} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{)}\PYG{p}{:}
      \PYG{k}{for} \PYG{n}{action} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{q\PYGZus{}value}\PYG{p}{[}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}
            \PYG{p}{[}\PYG{n}{trans\PYGZus{}prob} \PYG{o}{*} \PYGZbs{}
            \PYG{p}{(}\PYG{n}{reward} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{value\PYGZus{}fn}\PYG{p}{[}\PYG{n}{next\PYGZus{}state}\PYG{p}{]}\PYG{p}{)} \PYGZbs{}
            \PYG{k}{for} \PYG{n}{trans\PYGZus{}prob}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYGZbs{}
                                \PYG{o+ow}{in} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{P}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
      \PYG{n}{best\PYGZus{}action\PYGZus{}indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{q\PYGZus{}value}\PYG{p}{[}\PYG{n}{state}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]} \PYGZbs{}
                           \PYG{o}{==} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{q\PYGZus{}value}\PYG{p}{[}\PYG{n}{state}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
      \PYG{k}{for} \PYG{n}{index} \PYG{o+ow}{in} \PYG{n}{best\PYGZus{}action\PYGZus{}indices}\PYG{p}{:}
        \PYG{n}{improved\PYGZus{}policy}\PYG{p}{[}\PYG{n}{state}\PYG{p}{,} \PYG{n}{index}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{np}\PYG{o}{.}\PYG{n}{size}\PYG{p}{(}\PYG{n}{best\PYGZus{}action\PYGZus{}indices}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{improved\PYGZus{}policy}


  \PYG{k}{def} \PYG{n+nf}{policy\PYGZus{}iteration}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} start with uniform probability for all actions}
    \PYG{n}{initial\PYGZus{}policy} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mf}{1.0}\PYG{o}{/}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)} \PYG{o}{*} \PYGZbs{}
            \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}states}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}actions}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} \PYGZbs{}pi(s,a)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}iterations}\PYG{p}{)}\PYG{p}{:}
      \PYG{k}{if} \PYG{n}{i} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{current\PYGZus{}policy} \PYG{o}{=} \PYG{n}{initial\PYGZus{}policy}
      \PYG{n}{current\PYGZus{}value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{policy\PYGZus{}evaluation}\PYG{p}{(}\PYG{n}{current\PYGZus{}policy}\PYG{p}{)}
      \PYG{n}{improved\PYGZus{}policy} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{policy\PYGZus{}improvement}\PYG{p}{(}\PYG{n}{current\PYGZus{}value}\PYG{p}{)}
      \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{allclose}\PYG{p}{(}\PYG{n}{current\PYGZus{}policy}\PYG{p}{,} \PYG{n}{improved\PYGZus{}policy}\PYG{p}{,} \PYGZbs{}
                                 \PYG{n}{rtol}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}10}\PYG{p}{,} \PYG{n}{atol}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}15}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Policy Iteration converged in }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ iterations.}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
        \PYG{n}{current\PYGZus{}policy} \PYG{o}{=} \PYG{n}{improved\PYGZus{}policy}
        \PYG{k}{break}
      \PYG{n}{current\PYGZus{}policy} \PYG{o}{=} \PYG{n}{improved\PYGZus{}policy}
    \PYG{k}{return} \PYG{n}{current\PYGZus{}policy}


  \PYG{k}{def} \PYG{n+nf}{train\PYGZus{}and\PYGZus{}validate}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} compute optimal policy}
    \PYG{n}{optimal\PYGZus{}policy} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{policy\PYGZus{}iteration}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{ep\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{ep\PYGZus{}steps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{rewards} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
      \PYG{n}{state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
      \PYG{n}{step} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
        \PYG{n}{step} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{optimal\PYGZus{}policy}\PYG{p}{[}\PYG{n}{state}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
      \PYG{n}{ep\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}
      \PYG{n}{ep\PYGZus{}steps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{step}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}steps}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}del\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} destructor}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single,indent=L]{pytx@PYGpython@default@defaultverb@3}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{c+c1}{\PYGZsh{} stochastic environment}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FrozenLake\PYGZhy{}v1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{map\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{4x4}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
               \PYG{n}{is\PYGZus{}slippery}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rgb\PYGZus{}array}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{state: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Observation space dimension: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action space dimension:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Value Iteration Algorithm:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{agent} \PYG{o}{=} \PYG{n}{ValueIterationAgent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{mean\PYGZus{}steps} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train\PYGZus{}and\PYGZus{}validate}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mean episodic reward: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}rewards}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{      average steps per episode: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}steps}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Policy Iteration Algorithm:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{p\PYGZus{}agent} \PYG{o}{=} \PYG{n}{PolicyIterationAgent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}reward}\PYG{p}{,} \PYG{n}{mean\PYGZus{}steps} \PYG{o}{=} \PYG{n}{p\PYGZus{}agent}\PYG{o}{.}\PYG{n}{train\PYGZus{}and\PYGZus{}validate}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mean rewards: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}reward}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{  Mean steps: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}steps}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single,indent=L]{pytx@PYGpython@default@defaultverb@4}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Taxi\PYGZhy{}v3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rgb\PYGZus{}array}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{state:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{size of state space: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{size of action space: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scores with random policy:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{ep\PYGZus{}steps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
  \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
  \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
  \PYG{n}{step} \PYG{o}{=} \PYG{l+m+mi}{0}
  \PYG{n}{rewards} \PYG{o}{=} \PYG{l+m+mi}{0}
  \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
    \PYG{n}{step} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{n}{action} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{p}{,}\PYG{n}{\PYGZus{}}\PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
    \PYG{n}{rewards} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
    \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
  \PYG{n}{ep\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}
  \PYG{n}{ep\PYGZus{}steps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{step}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mean episodic Reward: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Mean episodic steps: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}steps}\PYG{p}{)}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@5}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Taxi\PYGZhy{}v3}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Policy Iteration:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{agent} \PYG{o}{=} \PYG{n}{PolicyIterationAgent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{mean\PYGZus{}steps}  \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train\PYGZus{}and\PYGZus{}validate}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Avg rewards: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}rewards}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, Avg steps: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}steps}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Value Iteration:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{v\PYGZus{}agent} \PYG{o}{=} \PYG{n}{ValueIterationAgent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{mean\PYGZus{}steps} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train\PYGZus{}and\PYGZus{}validate}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Avg rewards: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}rewards}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, Avg steps: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}steps}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@6}
\PYG{k+kn}{import} \PYG{n+nn}{math}
\PYG{k}{def} \PYG{n+nf}{deg2rad}\PYG{p}{(}\PYG{n}{deg}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{return} \PYG{n}{deg} \PYG{o}{*} \PYG{n}{math}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{180}

\PYG{c+c1}{\PYGZsh{}estimate pi}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{n}{pi\PYGZus{}estimate} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{n} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{10000}\PYG{p}{)}\PYG{p}{:}
  \PYG{n}{s} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} x,y points}
  \PYG{n}{r} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{s}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} r = sqrt(x\PYGZca{}2 + y\PYGZca{}2)}
  \PYG{n}{c} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{r} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{1}\PYG{p}{)}                   \PYG{c+c1}{\PYGZsh{} count points having r \PYGZlt{} 1}
  \PYG{n}{pi\PYGZus{}estimate}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{l+m+mi}{4} \PYG{o}{*} \PYG{n}{c} \PYG{o}{/} \PYG{n}{n}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{}plotting sample points}
\PYG{n}{cin} \PYG{o}{=} \PYG{n}{s}\PYG{p}{[}\PYG{n}{r} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} points inside the circle}
\PYG{n}{cout} \PYG{o}{=} \PYG{n}{s}\PYG{p}{[}\PYG{n}{r} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} points outside the circle}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{cin}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cin}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{points inside circle}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{cout}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cout}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{points outside circle}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} draw the circle}
\PYG{n}{theta} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{deg2rad}\PYG{p}{(}\PYG{l+m+mi}{90}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{radius} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{radius} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{radius} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{theta}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} new plot}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{pi\PYGZus{}estimate}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{estimate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{l+m+mf}{3.14}\PYG{o}{*}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones\PYGZus{}like}\PYG{p}{(}\PYG{n}{pi\PYGZus{}estimate}\PYG{p}{)}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3.14}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Number of samples (n)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Estimate of \PYGZdl{}}\PYG{l+s+s1}{\PYGZbs{}}\PYG{l+s+s1}{pi\PYGZdl{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{best}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@7}
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blackjack\PYGZhy{}v1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rgb\PYGZus{}array}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{state:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}
\PYG{n}{action} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Step function output: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observation space: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{spaces}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action values:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{screen} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{render}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{screen}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@8}
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{defaultdict}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{k}{class} \PYG{n+nc}{MCPAgent}\PYG{p}{:}
  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env} \PYG{o}{=} \PYG{n}{env}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{n\PYGZus{}actions} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Environment: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{sample\PYGZus{}policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{player\PYGZus{}sum}\PYG{p}{,} \PYG{n}{dealer\PYGZus{}show}\PYG{p}{,} \PYG{n}{usable\PYGZus{}ace} \PYG{o}{=} \PYG{n}{obs}
    \PYG{n}{probs} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]} \PYG{k}{if} \PYG{n}{player\PYGZus{}sum} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{18} \PYG{k}{else} \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}
    \PYG{n}{action} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{n}{probs}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{action}

  \PYG{k}{def} \PYG{n+nf}{generate\PYGZus{}episode}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Plays a single episode with a set policy.}
\PYG{l+s+sd}{    Records the state, action and reward for each step}
\PYG{l+s+sd}{    returns all the timesteps for the episode.  \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{episode} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
      \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{sample\PYGZus{}policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
      \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{info}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
      \PYG{n}{episode}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{)}\PYG{p}{)}
      \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
      \PYG{k}{if} \PYG{n}{done}\PYG{p}{:}
        \PYG{k}{break}
    \PYG{k}{return} \PYG{n}{episode}

\PYG{k}{def} \PYG{n+nf}{update\PYGZus{}Q\PYGZus{}first\PYGZus{}visit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{episode}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{,} \PYG{n}{returns\PYGZus{}sum}\PYG{p}{,} \PYG{n}{N}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{G} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{visited\PYGZus{}state} \PYG{o}{=} \PYG{n+nb}{set}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{episode}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} traverse in reverse order}
      \PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward} \PYG{o}{=} \PYG{n}{episode}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
      \PYG{n}{G} \PYG{o}{+}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*}\PYG{o}{*} \PYG{n}{i} \PYG{o}{*} \PYG{n}{reward} \PYG{c+c1}{\PYGZsh{} returns for (s,a)}
      \PYG{k}{if} \PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{visited\PYGZus{}state}\PYG{p}{:}    \PYG{c+c1}{\PYGZsh{} if first visit}
        \PYG{n}{N}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}  \PYG{c+c1}{\PYGZsh{} first visit count}
        \PYG{k}{if} \PYG{n}{method} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
          \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{G}  \PYG{c+c1}{\PYGZsh{} update returns}
          \PYG{n}{Q}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{=} \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{/} \PYG{n}{N}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]}
        \PYG{k}{else}\PYG{p}{:}
          \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{=} \PYG{n}{G} \PYG{c+c1}{\PYGZsh{} update returns}
          \PYG{n}{Q}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{p}{(}\PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{\PYGZhy{}}
                           \PYG{n}{Q}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]}\PYG{p}{)}\PYG{o}{/} \PYG{n}{N}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]}
        \PYG{n}{visited\PYGZus{}state}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}Q\PYGZus{}every\PYGZus{}visit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{episode}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{,} \PYG{n}{returns\PYGZus{}sum}\PYG{p}{,} \PYG{n}{N}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{G} \PYG{o}{=} \PYG{l+m+mi}{0}  \PYG{c+c1}{\PYGZsh{} return}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{episode}\PYG{p}{)}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{s}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{r} \PYG{o}{=} \PYG{n}{episode}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
      \PYG{n}{G} \PYG{o}{+}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*}\PYG{o}{*} \PYG{n}{i} \PYG{o}{*} \PYG{n}{r}  \PYG{c+c1}{\PYGZsh{} returns for (s,a)}
      \PYG{n}{N}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}    \PYG{c+c1}{\PYGZsh{} every\PYGZhy{}visit count}
      \PYG{k}{if} \PYG{n}{method} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
        \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{G}
        \PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{=} \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{/} \PYG{n}{N}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}
      \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{=} \PYG{n}{G}
        \PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{p}{(}\PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{)} \PYG{o}{/} \PYG{n}{N}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}

  \PYG{k}{def} \PYG{n+nf}{mc\PYGZus{}predict}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{100000}\PYG{p}{,} \PYG{n}{first\PYGZus{}visit}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} This plays through several episodes of the game \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{n}{returns\PYGZus{}sum} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{k}{lambda}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{N} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{k}{lambda}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{Q} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{k}{lambda}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
      \PYG{k}{if} \PYG{n}{i} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{1000} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}r}\PYG{l+s+s1}{Episode: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{sys}\PYG{o}{.}\PYG{n}{stdout}\PYG{o}{.}\PYG{n}{flush}\PYG{p}{(}\PYG{p}{)}
      \PYG{n}{episode} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{generate\PYGZus{}episode}\PYG{p}{(}\PYG{p}{)}
      \PYG{k}{if} \PYG{n}{first\PYGZus{}visit}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}Q\PYGZus{}first\PYGZus{}visit}\PYG{p}{(}\PYG{n}{episode}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{,} \PYG{n}{returns\PYGZus{}sum}\PYG{p}{,} \PYG{n}{N}\PYG{p}{,} \PYG{n}{method}\PYG{p}{)}
      \PYG{k}{else}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}Q\PYGZus{}every\PYGZus{}visit}\PYG{p}{(}\PYG{n}{episode}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{,} \PYG{n}{returns\PYGZus{}sum}\PYG{p}{,} \PYG{n}{N}\PYG{p}{,} \PYG{n}{method}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Q}

  \PYG{k}{def} \PYG{n+nf}{QtoV}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZsq{}\PYGZsq{}\PYGZsq{} Converts Q to V \PYGZsq{}\PYGZsq{}\PYGZsq{}}
    \PYG{n}{V} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,} \PYG{p}{(}\PYG{n}{k}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{\PYGZgt{}}\PYG{l+m+mi}{18}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,} \PYG{n}{v}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYGZbs{}
     \PYG{p}{(}\PYG{n}{k}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{18}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{,} \PYG{n}{v}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k}\PYG{p}{,} \PYG{n}{v} \PYG{o+ow}{in} \PYG{n}{Q}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{V}

\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
  \PYG{n}{agent} \PYG{o}{=} \PYG{n}{MCPAgent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{)}
  \PYG{n}{Q} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{mc\PYGZus{}predict}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{500000}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
  \PYG{n}{V} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{QtoV}\PYG{p}{(}\PYG{n}{Q}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@9}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits}\PYG{n+nn}{.}\PYG{n+nn}{mplot3d} \PYG{k+kn}{import} \PYG{n}{Axes3D}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits}\PYG{n+nn}{.}\PYG{n+nn}{axes\PYGZus{}grid1} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}axes\PYGZus{}locatable}

\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}blackjack\PYGZus{}values}\PYG{p}{(}\PYG{n}{V}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}Z}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{usable\PYGZus{}ace}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{,}\PYG{n}{usable\PYGZus{}ace}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n}{V}\PYG{p}{:}
        \PYG{k}{return} \PYG{n}{V}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{,}\PYG{n}{usable\PYGZus{}ace}\PYG{p}{]}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{return} \PYG{l+m+mi}{0}

  \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}figure}\PYG{p}{(}\PYG{n}{usable\PYGZus{}ace}\PYG{p}{,} \PYG{n}{ax}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x\PYGZus{}range} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mi}{22}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}range} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{11}\PYG{p}{)}
    \PYG{n}{X}\PYG{p}{,} \PYG{n}{Y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{x\PYGZus{}range}\PYG{p}{,} \PYG{n}{y\PYGZus{}range}\PYG{p}{)}
    \PYG{n}{Z} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{get\PYGZus{}Z}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{,}\PYG{n}{usable\PYGZus{}ace}\PYG{p}{)} \PYGZbs{}
            \PYG{k}{for} \PYG{n}{x}\PYG{p}{,}\PYG{n}{y} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
    \PYG{n}{surf} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot\PYGZus{}surface}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{,} \PYG{n}{rstride}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYGZbs{}
            \PYG{n}{cstride}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{coolwarm}\PYG{p}{,} \PYG{n}{vmin}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Player}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{s Current Sum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dealer}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{s Showing Card}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{State Value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{view\PYGZus{}init}\PYG{p}{(}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{elev}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{120}\PYG{p}{)}

  \PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{)}
  \PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{121}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Usable Ace}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{n}{get\PYGZus{}figure}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{ax}\PYG{p}{)}
  \PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{122}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No Usable Ace}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{n}{get\PYGZus{}figure}\PYG{p}{(}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{ax}\PYG{p}{)}
  \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

  \PYG{c+c1}{\PYGZsh{} code for plotting policy}
 \PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}blackjack\PYGZus{}policy}\PYG{p}{(}\PYG{n}{policy}\PYG{p}{)}\PYG{p}{:}

  \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}Z}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{usable\PYGZus{}ace}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{,}\PYG{n}{usable\PYGZus{}ace}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n}{policy}\PYG{p}{:}
      \PYG{k}{return} \PYG{n}{policy}\PYG{p}{[}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{,}\PYG{n}{usable\PYGZus{}ace}\PYG{p}{]}
    \PYG{k}{else}\PYG{p}{:}
      \PYG{k}{return} \PYG{l+m+mi}{1}

  \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}figure}\PYG{p}{(}\PYG{n}{usable\PYGZus{}ace}\PYG{p}{,} \PYG{n}{ax}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{x\PYGZus{}range} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{11}\PYG{p}{,} \PYG{l+m+mi}{22}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}range} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{X}\PYG{p}{,} \PYG{n}{Y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{x\PYGZus{}range}\PYG{p}{,} \PYG{n}{y\PYGZus{}range}\PYG{p}{)}
    \PYG{n}{Z} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{n}{get\PYGZus{}Z}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,}\PYG{n}{y}\PYG{p}{,}\PYG{n}{usable\PYGZus{}ace}\PYG{p}{)} \PYG{k}{for} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n}{x\PYGZus{}range}\PYG{p}{]} \PYG{k}{for} \PYG{n}{y} \PYG{o+ow}{in} \PYG{n}{y\PYGZus{}range}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{surf} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{Z}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{get\PYGZus{}cmap}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pastel2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYGZbs{}
                     \PYG{n}{vmin}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{vmax}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{extent}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{10.5}\PYG{p}{,} \PYG{l+m+mf}{21.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{10.5}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(}\PYG{n}{x\PYGZus{}range}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{yticks}\PYG{p}{(}\PYG{n}{y\PYGZus{}range}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{invert\PYGZus{}yaxis}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Player}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{s Current Sum}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Dealer}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{s Showing Card}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{divider} \PYG{o}{=} \PYG{n}{make\PYGZus{}axes\PYGZus{}locatable}\PYG{p}{(}\PYG{n}{ax}\PYG{p}{)}
    \PYG{n}{cax} \PYG{o}{=} \PYG{n}{divider}\PYG{o}{.}\PYG{n}{append\PYGZus{}axes}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{right}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{5}\PYG{l+s+s2}{\PYGZpc{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{pad}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
    \PYG{n}{cbar} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{surf}\PYG{p}{,} \PYG{n}{ticks}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cax}\PYG{o}{=}\PYG{n}{cax}\PYG{p}{)}
    \PYG{n}{cbar}\PYG{o}{.}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticklabels}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{0 (STICK)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1 (HIT)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

  \PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{)}
  \PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{121}\PYG{p}{)}
  \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Usable Ace}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{n}{get\PYGZus{}figure}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{ax}\PYG{p}{)}
  \PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{122}\PYG{p}{)}
  \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{No Usable Ace}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{n}{get\PYGZus{}figure}\PYG{p}{(}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{ax}\PYG{p}{)}
  \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@10}
  \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}Q\PYGZus{}first\PYGZus{}visit}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{episode}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{,} \PYG{n}{returns\PYGZus{}sum}\PYG{p}{,} \PYG{n}{N}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Another implementation of First Visit Monte\PYGZhy{}Carlo update of Q values.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{for} \PYG{n}{s}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n}{episode}\PYG{p}{:}
      \PYG{n}{first\PYGZus{}occurrence\PYGZus{}idx} \PYG{o}{=} \PYG{n+nb}{next}\PYG{p}{(}\PYG{n}{i} \PYGZbs{}
              \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{episode}\PYG{p}{)} \PYG{k}{if} \PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{==} \PYG{n}{s}\PYG{p}{)}
      \PYG{n}{G} \PYG{o}{=} \PYG{n+nb}{sum}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{*} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*}\PYG{o}{*} \PYG{n}{i}\PYG{p}{)} \PYGZbs{}
              \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{x} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{episode}\PYG{p}{[}\PYG{n}{first\PYGZus{}occurrence\PYGZus{}idx}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
      \PYG{n}{N}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}  \PYG{c+c1}{\PYGZsh{} first visit count for (s,a)}
      \PYG{k}{if} \PYG{n}{method} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{:}
        \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{G}
        \PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{=} \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{/} \PYG{n}{N}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}
      \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{=} \PYG{n}{G}
        \PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{p}{(}\PYG{n}{returns\PYGZus{}sum}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{)} \PYG{o}{/} \PYG{n}{N}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@11}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{defaultdict}

\PYG{k}{class} \PYG{n+nc}{MCAgent}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.0001}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{ep\PYGZus{}decay}\PYG{o}{=}\PYG{l+m+mf}{0.9999}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env} \PYG{o}{=} \PYG{n}{env}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{n\PYGZus{}action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha} \PYG{o}{=} \PYG{n}{alpha}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Environment name: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{best\PYGZus{}policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{policy} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{v}\PYG{p}{)}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k}\PYG{p}{,} \PYG{n}{v} \PYG{o+ow}{in} \PYG{n}{Q}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{policy}

  \PYG{k}{def} \PYG{n+nf}{epsilon\PYGZus{}greedy\PYGZus{}policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{,} \PYG{n}{epsilon}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{epsilon}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} explore}
      \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} exploit}
      \PYG{n}{action} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{state}\PYG{p}{,}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{action}

  \PYG{k}{def} \PYG{n+nf}{generate\PYGZus{}episode}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{,} \PYG{n}{epsilon}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
      \PYG{n}{states}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
      \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}greedy\PYGZus{}policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{,} \PYG{n}{epsilon}\PYG{p}{)}
      \PYG{n}{actions}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
      \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{info}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
      \PYG{n}{rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}
      \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
      \PYG{k}{if} \PYG{n}{done}\PYG{p}{:}
        \PYG{k}{break}
    \PYG{k}{return} \PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}Q}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{episode}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{returns} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{episode}
    \PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} traverse in reverse order}
      \PYG{n}{s} \PYG{o}{=} \PYG{n}{states}\PYG{p}{[}\PYG{n}{t}\PYG{p}{]}
      \PYG{n}{a} \PYG{o}{=} \PYG{n}{actions}\PYG{p}{[}\PYG{n}{t}\PYG{p}{]}
      \PYG{n}{r} \PYG{o}{=} \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{t}\PYG{p}{]}
      \PYG{n}{returns} \PYG{o}{+}\PYG{o}{=} \PYG{n}{r} \PYG{o}{*} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*}\PYG{o}{*} \PYG{n}{t}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} discounted rewards}
      \PYG{k}{if} \PYG{n}{s} \PYG{o+ow}{not} \PYG{o+ow}{in} \PYG{n}{states}\PYG{p}{[}\PYG{p}{:}\PYG{n}{t}\PYG{p}{]}\PYG{p}{:}   \PYG{c+c1}{\PYGZsh{} if S is a first visit (last index is ignore)}
        \PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha} \PYG{o}{*} \PYG{p}{(}\PYG{n}{returns} \PYG{o}{\PYGZhy{}} \PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{Q}

  \PYG{k}{def} \PYG{n+nf}{mc\PYGZus{}control}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{500000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{Q} \PYG{o}{=} \PYG{n}{defaultdict}\PYG{p}{(}\PYG{k}{lambda}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{n\PYGZus{}action}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{epsilon} \PYG{o}{=} \PYG{l+m+mf}{1.0}
    \PYG{n}{eps\PYGZus{}min} \PYG{o}{=} \PYG{l+m+mf}{0.0001}
    \PYG{n}{decay} \PYG{o}{=} \PYG{l+m+mf}{0.9999}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
      \PYG{k}{if} \PYG{n}{i} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{1000} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}r}\PYG{l+s+s1}{Episode: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{sys}\PYG{o}{.}\PYG{n}{stdout}\PYG{o}{.}\PYG{n}{flush}\PYG{p}{(}\PYG{p}{)}

      \PYG{n}{episode} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{generate\PYGZus{}episode}\PYG{p}{(}\PYG{n}{Q}\PYG{p}{,} \PYG{n}{epsilon}\PYG{p}{)}
      \PYG{n}{Q} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}Q}\PYG{p}{(}\PYG{n}{episode}\PYG{p}{,} \PYG{n}{Q}\PYG{p}{)}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{epsilon} \PYG{o}{*} \PYG{n}{decay}\PYG{p}{,} \PYG{n}{eps\PYGZus{}min}\PYG{p}{)}
      \PYG{n}{policy} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{best\PYGZus{}policy}\PYG{p}{(}\PYG{n}{Q}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{policy}\PYG{p}{,} \PYG{n}{Q}

  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}delete\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}

\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
  \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Blackjack\PYGZhy{}v1}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
  \PYG{n}{agent} \PYG{o}{=} \PYG{n}{MCAgent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{)}

  \PYG{c+c1}{\PYGZsh{} Learn optimal policy}
  \PYG{n}{policy}\PYG{p}{,} \PYG{n}{Q} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{mc\PYGZus{}control}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{500000}\PYG{p}{)}

  \PYG{c+c1}{\PYGZsh{} Compute value function}
  \PYG{n}{V} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}\PYG{p}{(}\PYG{n}{k}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{v}\PYG{p}{)}\PYG{p}{)} \PYG{k}{for} \PYG{n}{k}\PYG{p}{,} \PYG{n}{v} \PYG{o+ow}{in} \PYG{n}{Q}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

  \PYG{c+c1}{\PYGZsh{} plot Value \PYGZam{} Policy}
  \PYG{n}{plot\PYGZus{}blackjack\PYGZus{}values}\PYG{p}{(}\PYG{n}{V}\PYG{p}{)}
  \PYG{n}{plot\PYGZus{}blackjack\PYGZus{}policy}\PYG{p}{(}\PYG{n}{policy}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@12}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{time}

\PYG{k}{class} \PYG{n+nc}{QLearningAgent}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{fixed\PYGZus{}epsilon}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env} \PYG{o}{=} \PYG{n}{env}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha} \PYG{o}{=} \PYG{n}{alpha}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
    \PYG{k}{if} \PYG{n}{fixed\PYGZus{}epsilon} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{l+m+mf}{1.0}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{eps\PYGZus{}min} \PYG{o}{=} \PYG{l+m+mf}{0.01}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}rate} \PYG{o}{=} \PYG{l+m+mf}{0.999}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}flag} \PYG{o}{=} \PYG{k+kc}{True}
    \PYG{k}{else}\PYG{p}{:}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n}{fixed\PYGZus{}epsilon}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}flag} \PYG{o}{=} \PYG{k+kc}{False}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Environment Name: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RL Agent: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q\PYGZhy{}Learning}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} initialize Q table}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{,}
                                 \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{epsilon\PYGZus{}greedy\PYGZus{}policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{epsilon}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{randvar} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{randvar} \PYG{o}{\PYGZlt{}} \PYG{n}{epsilon}\PYG{p}{:}
      \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} explore}
    \PYG{k}{else}\PYG{p}{:}
      \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} exploit}
      \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} explore}
    \PYG{k}{return} \PYG{n}{action}


  \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}q\PYGZus{}table}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{s}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{r}\PYG{p}{,} \PYG{n}{s\PYGZus{}next}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha} \PYG{o}{*} \PYG{p}{(}\PYG{n}{r} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYGZbs{}
                         \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{s\PYGZus{}next}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
      \PYG{n}{file} \PYG{o}{=} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{w}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{n}{ep\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{start} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{ep\PYGZus{}reward} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{c+c1}{\PYGZsh{} reset the environment for each episode}
      \PYG{n}{state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
      \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} select an action}
        \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}greedy\PYGZus{}policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} obtain rewards}
        \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} update q table}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}q\PYGZus{}table}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} accumulate rewards for the episode}
        \PYG{n}{ep\PYGZus{}reward} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
        \PYG{c+c1}{\PYGZsh{} prepare for next iteration}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
        \PYG{k}{if} \PYG{n}{done}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} end of episode}
          \PYG{n}{ep\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}reward}\PYG{p}{)}
          \PYG{k}{break}
      \PYG{c+c1}{\PYGZsh{}end of while loop}
      \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}flag}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} allow epsilon decay}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}rate}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{eps\PYGZus{}min}\PYG{p}{)}

      \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{file}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{)}\PYG{p}{)}
      \PYG{k}{if} \PYG{n}{i} \PYG{o}{\PYGZpc{}} \PYG{n}{freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}r}\PYG{l+s+s1}{Episode: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{, Average episodic Reward:}\PYG{l+s+si}{\PYGZob{}:.3f\PYGZcb{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYGZbs{}
              \PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{sys}\PYG{o}{.}\PYG{n}{stdout}\PYG{o}{.}\PYG{n}{flush}\PYG{p}{(}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{}end of for loop}
    \PYG{n}{end} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Training time (seconds): }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{end} \PYG{o}{\PYGZhy{}} \PYG{n}{start}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
      \PYG{n}{file}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{validate}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ep\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
      \PYG{n}{ep\PYGZus{}reward} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}greedy\PYGZus{}policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
        \PYG{n}{ep\PYGZus{}reward} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
        \PYG{k}{if} \PYG{n}{done}\PYG{p}{:}
          \PYG{n}{ep\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}reward}\PYG{p}{)}
          \PYG{k}{break}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Test: Average Episodic Reward: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{display\PYGZus{}q\PYGZus{}table}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{} }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{} }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}delete\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@13}
\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FrozenLake\PYGZhy{}v1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{is\PYGZus{}slippery}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{QLearningAgent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{flake\PYGZus{}qlearn.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{validate}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@14}
\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Taxi\PYGZhy{}v3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{is\PYGZus{}slippery}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{QLearningAgent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{20000}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{taxi\PYGZus{}qlearn.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{validate}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@15}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{time}

\PYG{k}{class} \PYG{n+nc}{AgentSARSA}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{fixed\PYGZus{}epsilon}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env} \PYG{o}{=} \PYG{n}{env}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha} \PYG{o}{=} \PYG{n}{alpha} \PYG{c+c1}{\PYGZsh{} learning rate}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma} \PYG{c+c1}{\PYGZsh{} discount factor}
    \PYG{k}{if} \PYG{n}{fixed\PYGZus{}epsilon} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{l+m+mf}{1.0}      \PYG{c+c1}{\PYGZsh{} exploration probability}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{eps\PYGZus{}min} \PYG{o}{=} \PYG{l+m+mf}{0.01}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}rate} \PYG{o}{=} \PYG{l+m+mf}{0.999}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}flag} \PYG{o}{=} \PYG{k+kc}{True}
    \PYG{k}{else}\PYG{p}{:}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n}{fixed\PYGZus{}epsilon}
      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}flag} \PYG{o}{=} \PYG{k+kc}{False}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Environment Name: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RL Agent: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SARSA}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} initialize Q table}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{,}
                              \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}\PYG{p}{)}


  \PYG{k}{def} \PYG{n+nf}{epsilon\PYGZus{}greedy\PYGZus{}policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{epsilon}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{randvar} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{randvar} \PYG{o}{\PYGZlt{}} \PYG{n}{epsilon}\PYG{p}{:}
      \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} explore}
    \PYG{k}{else}\PYG{p}{:}
      \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{state}\PYG{p}{]}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} exploit}
      \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} explore}
    \PYG{k}{return} \PYG{n}{action}


  \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}q\PYGZus{}table}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{s}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{r}\PYG{p}{,} \PYG{n}{s\PYGZus{}next}\PYG{p}{,} \PYG{n}{a\PYGZus{}next}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha} \PYG{o}{*} \PYG{p}{(}\PYG{n}{r} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYGZbs{}
                      \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{s\PYGZus{}next}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a\PYGZus{}next}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{[}\PYG{n}{s}\PYG{p}{]}\PYG{p}{[}\PYG{n}{a}\PYG{p}{]}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
      \PYG{n}{file} \PYG{o}{=} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{w}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{ep\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{start} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{ep\PYGZus{}reward} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{n}{state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{}reset the environment}
      \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}greedy\PYGZus{}policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{)}
      \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} get next\PYGZus{}state and reward}
        \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} get action for next\PYGZus{}state}
        \PYG{n}{next\PYGZus{}action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}greedy\PYGZus{}policy}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} update q table}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}q\PYGZus{}table}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYGZbs{}
                                 \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{next\PYGZus{}action}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} epsodic reward}
        \PYG{n}{ep\PYGZus{}reward} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}

        \PYG{c+c1}{\PYGZsh{} prepare for next iteration}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{next\PYGZus{}action}
        \PYG{k}{if} \PYG{n}{done}\PYG{p}{:}
          \PYG{n}{ep\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}reward}\PYG{p}{)}
          \PYG{k}{break}
      \PYG{c+c1}{\PYGZsh{}end of while loop}
      \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}flag}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} allow epsilon decay}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{decay\PYGZus{}rate}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{eps\PYGZus{}min}\PYG{p}{)}

      \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{file}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{)}\PYG{p}{)}
      \PYG{k}{if} \PYG{n}{i} \PYG{o}{\PYGZpc{}} \PYG{n}{freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}r}\PYG{l+s+s1}{Episode: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{/}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{, Average episodic Reward:}\PYG{l+s+si}{\PYGZob{}:.3f\PYGZcb{}}\PYG{l+s+s1}{.}\PYG{l+s+s1}{\PYGZsq{}}\PYGZbs{}
              \PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{n}{sys}\PYG{o}{.}\PYG{n}{stdout}\PYG{o}{.}\PYG{n}{flush}\PYG{p}{(}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{}end of for loop}
    \PYG{n}{end} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{ Training Time: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{end}\PYG{o}{\PYGZhy{}}\PYG{n}{start}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
      \PYG{n}{file}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{validate}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{ep\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
      \PYG{n}{state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
      \PYG{n}{ep\PYGZus{}reward} \PYG{o}{=} \PYG{l+m+mi}{0}
      \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}greedy\PYGZus{}policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
        \PYG{n}{ep\PYGZus{}reward} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
        \PYG{k}{if} \PYG{n}{done}\PYG{p}{:}
          \PYG{n}{ep\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}reward}\PYG{p}{)}
          \PYG{k}{break}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Average Episodic Reward: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf}{display\PYGZus{}q\PYGZus{}table}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{} }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{Q}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{ \PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{} }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

  \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}delete\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@16}
\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Taxi\PYGZhy{}v3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{AgentSARSA}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{20000}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{taxi\PYGZus{}sarsa.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{validate}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@17}
\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FrozenLake\PYGZhy{}v1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{is\PYGZus{}slippery}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{AgentSARSA}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{flake\PYGZus{}sarsa.tsv}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{validate}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@18}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k}{class} \PYG{n+nc}{ReplayBuffer}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{capacity}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYG{o}{=} \PYG{n}{capacity}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{object}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{idx} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{o}{=} \PYG{k+kc}{False}

    \PYG{k}{def} \PYG{n+nf}{add}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{experience}\PYG{p}{:}\PYG{n+nb}{tuple}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{idx}\PYG{p}{]} \PYG{o}{=} \PYG{n}{experience}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{idx} \PYG{o}{=} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{idx} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZpc{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity}
        \PYG{c+c1}{\PYGZsh{} set this flag if buffer is full}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{o+ow}{or} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{idx} \PYG{o}{==} \PYG{l+m+mi}{0}

    \PYG{k}{def} \PYG{n+nf}{sample}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYGZbs{}
                                    \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{k}{else} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{idx}\PYG{p}{,}
                                    \PYG{n}{size}\PYG{o}{=}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}
        \PYG{n}{batch} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{p}{[}\PYG{n}{indices}\PYG{p}{]}
        \PYG{k}{return} \PYG{n}{batch}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}getitem\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{index}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{index} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{0} \PYG{o+ow}{and} \PYG{n}{index} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYGZbs{}
                        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{k}{else} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{idx}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} sanity check}
            \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{p}{[}\PYG{n}{index}\PYG{p}{]}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Index is out of range}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}len\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} return the current length of buffer}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{k}{else} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{idx}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@19}
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{random}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{import} \PYG{n+nn}{keras}
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{layers} \PYG{k+kn}{import} \PYG{n}{Dense}
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{optimizers} \PYG{k+kn}{import} \PYG{n}{Adam}
\PYG{k+kn}{from} \PYG{n+nn}{keras}\PYG{n+nn}{.}\PYG{n+nn}{models} \PYG{k+kn}{import} \PYG{n}{Sequential}

\PYG{k}{class} \PYG{n+nc}{DQNAgent}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{:} \PYG{n+nb}{tuple}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{:} \PYG{n+nb}{int}\PYG{p}{,}
                \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{2000}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{24}\PYG{p}{,}
                \PYG{n}{ddqn\PYGZus{}flag}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{n\PYGZus{}actions}   \PYG{c+c1}{\PYGZsh{} number discrete actions}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{ddqn} \PYG{o}{=} \PYG{n}{ddqn\PYGZus{}flag}   \PYG{c+c1}{\PYGZsh{} choose between DQN \PYGZam{} DDQN}

        \PYG{c+c1}{\PYGZsh{} hyper\PYGZhy{}parameters for DQN}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{l+m+mf}{0.99}    \PYG{c+c1}{\PYGZsh{} discount factor}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{l+m+mf}{1.0}    \PYG{c+c1}{\PYGZsh{} exploration rate \PYGZhy{} epsilon\PYGZhy{}greedy policy}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}decay} \PYG{o}{=} \PYG{l+m+mf}{0.999}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}min} \PYG{o}{=} \PYG{l+m+mf}{0.01}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n}{batch\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer\PYGZus{}size} \PYG{o}{=} \PYG{n}{buffer\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{train\PYGZus{}start} \PYG{o}{=} \PYG{l+m+mi}{1000}  \PYG{c+c1}{\PYGZsh{} minimum buffer size to start training}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{learning\PYGZus{}rate} \PYG{o}{=} \PYG{l+m+mf}{0.001} \PYG{c+c1}{\PYGZsh{} learning rate for the Deep Network}

        \PYG{c+c1}{\PYGZsh{} create a replay buffer to store experiences}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory} \PYG{o}{=} \PYG{n}{ReplayBuffer}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer\PYGZus{}size}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} create main model \PYGZam{} target model   \PYGZhy{} DDQN Architecture}
        \PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{model}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{clone\PYGZus{}model}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} initialize target model}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}model}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}


    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{keras}\PYG{o}{.}\PYG{n}{Sequential}\PYG{p}{(}\PYG{p}{[}
            \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{,} \PYG{n}{input\PYGZus{}shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,}
                              \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                              \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{he\PYGZus{}uniform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{24}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                              \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{he\PYGZus{}uniform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{linear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                              \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{he\PYGZus{}uniform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{p}{]}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{loss}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{optimizer}\PYG{o}{=}\PYG{n}{Adam}\PYG{p}{(}
                                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{learning\PYGZus{}rate}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}target\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{tau}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{store\PYGZus{}experience}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}target\PYGZus{}q\PYGZus{}value}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{s\PYGZus{}next}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}epsilon}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{save\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{filename}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{save\PYGZus{}weights}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{load\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{filename}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{load\PYGZus{}weights}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@20}
\PYG{k}{class} \PYG{n+nc}{DQNAgent}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{epsilon}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} epsilon\PYGZhy{}greedy policy}
        \PYG{k}{if} \PYG{n}{epsilon} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{epsilon} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}   \PYG{c+c1}{\PYGZsh{} decaying epsilon}
        \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{epsilon}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} explore}
            \PYG{k}{return} \PYG{n}{random}\PYG{o}{.}\PYG{n}{randrange}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{q\PYGZus{}value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{verbose}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
            \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{q\PYGZus{}value}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@21}
\PYG{k}{class} \PYG{n+nc}{DQNAgent}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}target\PYGZus{}q\PYGZus{}value}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} batch input}
        \PYG{n}{q\PYGZus{}value\PYGZus{}ns} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{verbose}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Q(s\PYGZsq{}, :)}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{ddqn}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{}\PYGZsh{} DDQN algorithm}
            \PYG{c+c1}{\PYGZsh{} primary model is used for action selection: a = arg max Q(s,a)}
            \PYG{n}{max\PYGZus{}actions} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{q\PYGZus{}value\PYGZus{}ns}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} use target model for action evaluation: Q\PYGZsq{}(s\PYGZsq{},:)}
            \PYG{n}{target\PYGZus{}q\PYGZus{}values\PYGZus{}ns} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}
                                             \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{verbose}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} Q\PYGZsq{}(s\PYGZsq{}, argmax(Q(s,a)))}
            \PYG{n}{max\PYGZus{}q\PYGZus{}values} \PYG{o}{=} \PYG{n}{target\PYGZus{}q\PYGZus{}values\PYGZus{}ns}\PYG{p}{[}
                        \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{target\PYGZus{}q\PYGZus{}values\PYGZus{}ns}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{max\PYGZus{}actions}\PYG{p}{]}
        \PYG{k}{else}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} DQN}
            \PYG{n}{max\PYGZus{}q\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{amax}\PYG{p}{(}\PYG{n}{q\PYGZus{}values\PYGZus{}ns}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{max\PYGZus{}q\PYGZus{}values}

    \PYG{k}{def} \PYG{n+nf}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{train\PYGZus{}start}\PYG{p}{:}
            \PYG{k}{return}
        \PYG{c+c1}{\PYGZsh{} sample experiences from replay buffer}
        \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n+nb}{min}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{mini\PYGZus{}batch} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} unwrapping mini\PYGZus{}batch tuple}
        \PYG{n}{states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{dones} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{mini\PYGZus{}batch}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{states}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
            \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}
            \PYG{n}{next\PYGZus{}states}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}
            \PYG{n}{dones}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}

        \PYG{n}{q\PYGZus{}values\PYGZus{}cs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{verbose}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{max\PYGZus{}q\PYGZus{}values\PYGZus{}ns} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}target\PYGZus{}q\PYGZus{}value}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} compute target q value}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{done} \PYG{o}{=} \PYG{n}{dones}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{bool}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{reward} \PYG{o}{=} \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{k}{if} \PYG{n}{done}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} terminal state}
                \PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{=} \PYG{n}{reward}
            \PYG{k}{else}\PYG{p}{:}
                \PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{=} \PYG{n}{reward} \PYG{o}{+} \PYGZbs{}
                              \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{max\PYGZus{}q\PYGZus{}values\PYGZus{}ns}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}

        \PYG{c+c1}{\PYGZsh{} train the Q network}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{)}\PYG{p}{,}
                      \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,}
                      \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                      \PYG{n}{verbose}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} decay epsilon over time}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}epsilon}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}epsilon}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{\PYGZgt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}min}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{*}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon\PYGZus{}decay}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@22}
\PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{,} \PYG{n}{train\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                        \PYG{n}{copy\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{file} \PYG{o}{=} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{}averaging factor \PYGZhy{} choose between soft \PYGZam{} hard update}
    \PYG{n}{tau} \PYG{o}{=} \PYG{l+m+mf}{0.1} \PYG{k}{if} \PYG{n}{copy\PYGZus{}freq} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{10} \PYG{k}{else} \PYG{l+m+mf}{1.0}
    \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{avg\PYGZus{}score}\PYG{p}{,} \PYG{n}{avg100\PYGZus{}score} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
  \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{for} \PYG{n}{e} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
        \PYG{n}{ep\PYGZus{}reward} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{t} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
            \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
            \PYG{c+c1}{\PYGZsh{} take action}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{get\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} collect reward \PYGZam{} transition to next state}
            \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} reward engineering}
            \PYG{c+c1}{\PYGZsh{} discourages premature termination}
            \PYG{n}{reward} \PYG{o}{=} \PYG{n}{reward} \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{done} \PYG{k}{else} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}

            \PYG{c+c1}{\PYGZsh{}store experience}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{store\PYGZus{}experience}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYGZbs{}
                                 \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}

            \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
            \PYG{n}{ep\PYGZus{}reward} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
            \PYG{n}{t} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}

            \PYG{c+c1}{\PYGZsh{} train}
            \PYG{k}{if} \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{\PYGZpc{}} \PYG{n}{train\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                \PYG{n}{agent}\PYG{o}{.}\PYG{n}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} update the target model}
            \PYG{k}{if} \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{\PYGZpc{}} \PYG{n}{copy\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                \PYG{n}{agent}\PYG{o}{.}\PYG{n}{update\PYGZus{}target\PYGZus{}model}\PYG{p}{(}\PYG{n}{tau}\PYG{o}{=}\PYG{n}{tau}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} while loop ends here}
        \PYG{k}{if} \PYG{n}{e} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{100} \PYG{o+ow}{and} \PYG{n}{t} \PYG{o}{\PYGZgt{}} \PYG{n}{best\PYGZus{}score}\PYG{p}{:}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{save\PYGZus{}model}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{best\PYGZus{}model.weights.h5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
            \PYG{n}{best\PYGZus{}score}\PYG{o}{=}\PYG{n}{t}
        \PYG{n}{scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{t}\PYG{p}{)}
        \PYG{n}{avg\PYGZus{}score}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{avg100\PYGZus{}score}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{file}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{t}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{                              }\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
            \PYG{n}{file}\PYG{o}{.}\PYG{n}{flush}\PYG{p}{(}\PYG{p}{)}
            \PYG{n}{os}\PYG{o}{.}\PYG{n}{fsync}\PYG{p}{(}\PYG{n}{file}\PYG{o}{.}\PYG{n}{fileno}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{e} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{20} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, ep\PYGZus{}reward:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{t}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, avg\PYGZus{}ep\PYGZus{}reward: }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{                                      }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} end of for loop}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{end of training}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{file}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@23}
    \PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}

    \PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} create gym environment}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CartPole\PYGZhy{}v0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{n\PYGZus{}actions} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}

    \PYG{c+c1}{\PYGZsh{} create DQN Agent}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{DQNAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{,}
    \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{2000}\PYG{p}{,}
    \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{24}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} train the agent}
    \PYG{n}{train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{copy\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}
    \PYG{n}{filename}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{cp\PYGZus{}dqn.txt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@24}
    \PYG{k}{class} \PYG{n+nc}{SumTree}\PYG{p}{(}\PYG{n+nb}{object}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Here we initialize the tree with all nodes = 0,}
    \PYG{c+c1}{\PYGZsh{} and initialize the data with all values = 0}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{capacity}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Number of leaf nodes (final nodes) that contains experiences}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYG{o}{=} \PYG{n}{capacity}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data\PYGZus{}pointer} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{o}{=} \PYG{k+kc}{False}   \PYG{c+c1}{\PYGZsh{} indicates if the buffer is full}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{capacity} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} contains priorities}
    \PYG{c+c1}{\PYGZsh{} Contains the experiences (so the size of data is capacity)}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{capacity}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{object}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{add}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{priority}\PYG{p}{,} \PYG{n}{data}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} data is stored at the leaf of the tree from index: n\PYGZhy{}1 to 2*n\PYGZhy{}1}
    \PYG{n}{tree\PYGZus{}index} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data\PYGZus{}pointer} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}
    \PYG{c+c1}{\PYGZsh{} Update data frame}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data\PYGZus{}pointer}\PYG{p}{]} \PYG{o}{=} \PYG{n}{data}
    \PYG{c+c1}{\PYGZsh{} Update the leaf}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{tree\PYGZus{}index}\PYG{p}{,} \PYG{n}{priority}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Add 1 to data\PYGZus{}pointer}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data\PYGZus{}pointer} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{c+c1}{\PYGZsh{} If above capacity, go back to first index to overwrite}
    \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data\PYGZus{}pointer} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity}\PYG{p}{:}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data\PYGZus{}pointer} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{o}{=} \PYG{k+kc}{True}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}len\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} returns the size of data buffer only}
    \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{k}{else} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data\PYGZus{}pointer}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}getitem\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{index}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} return data and priority at index i}
    \PYG{k}{if} \PYG{n}{index} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{0} \PYG{o+ow}{and} \PYG{n}{index} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYGZbs{}
    \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{full} \PYG{k}{else} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data\PYGZus{}pointer}\PYG{p}{:}
    \PYG{n}{tree\PYGZus{}idx} \PYG{o}{=} \PYG{n}{index} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}
    \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{n}{index}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{n}{tree\PYGZus{}idx}\PYG{p}{]}
    \PYG{k}{else}\PYG{p}{:}
    \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index out of range}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


    \PYG{k}{def} \PYG{n+nf}{update}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{tree\PYGZus{}index}\PYG{p}{,} \PYG{n}{priority}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Change = new priority score \PYGZhy{} former priority score}
    \PYG{n}{change} \PYG{o}{=} \PYG{n}{priority} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{n}{tree\PYGZus{}index}\PYG{p}{]}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{n}{tree\PYGZus{}index}\PYG{p}{]} \PYG{o}{=} \PYG{n}{priority}

    \PYG{c+c1}{\PYGZsh{} then propagate the change through tree}
    \PYG{c+c1}{\PYGZsh{} this method is faster than the recursive loop}
    \PYG{k}{while} \PYG{n}{tree\PYGZus{}index} \PYG{o}{!=} \PYG{l+m+mi}{0}\PYG{p}{:}
    \PYG{n}{tree\PYGZus{}index} \PYG{o}{=} \PYG{p}{(}\PYG{n}{tree\PYGZus{}index} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{/}\PYG{o}{/} \PYG{l+m+mi}{2}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{n}{tree\PYGZus{}index}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{change}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}leaf}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{v}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{parent\PYGZus{}index} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{while} \PYG{k+kc}{True}\PYG{p}{:}
    \PYG{n}{left\PYGZus{}child\PYGZus{}index} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{parent\PYGZus{}index} \PYG{o}{+} \PYG{l+m+mi}{1}
    \PYG{n}{right\PYGZus{}child\PYGZus{}index} \PYG{o}{=} \PYG{n}{left\PYGZus{}child\PYGZus{}index} \PYG{o}{+} \PYG{l+m+mi}{1}

    \PYG{c+c1}{\PYGZsh{} If we reach bottom, end the search}
    \PYG{k}{if} \PYG{n}{left\PYGZus{}child\PYGZus{}index} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{leaf\PYGZus{}index} \PYG{o}{=} \PYG{n}{parent\PYGZus{}index}
    \PYG{k}{break}
    \PYG{k}{else}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} downward search, always search for a higher priority node}
    \PYG{k}{if} \PYG{n}{v} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{n}{left\PYGZus{}child\PYGZus{}index}\PYG{p}{]}\PYG{p}{:}
    \PYG{n}{parent\PYGZus{}index} \PYG{o}{=} \PYG{n}{left\PYGZus{}child\PYGZus{}index}
    \PYG{k}{else}\PYG{p}{:}
    \PYG{n}{v} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{n}{left\PYGZus{}child\PYGZus{}index}\PYG{p}{]}
    \PYG{n}{parent\PYGZus{}index} \PYG{o}{=} \PYG{n}{right\PYGZus{}child\PYGZus{}index}

    \PYG{n}{data\PYGZus{}index} \PYG{o}{=} \PYG{n}{leaf\PYGZus{}index} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{capacity} \PYG{o}{+} \PYG{l+m+mi}{1}
    \PYG{k}{return} \PYG{n}{leaf\PYGZus{}index}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{n}{leaf\PYGZus{}index}\PYG{p}{]}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{data}\PYG{p}{[}\PYG{n}{data\PYGZus{}index}\PYG{p}{]}

    \PYG{n+nd}{@property}
    \PYG{k}{def} \PYG{n+nf}{total\PYGZus{}priority}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} Returns the root node}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@25}
    \PYG{k}{class} \PYG{n+nc}{STBuffer}\PYG{p}{(}\PYG{n+nb}{object}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} stored as ( state, action, reward, next\PYGZus{}state ) in SumTree}
    \PYG{n}{PER\PYGZus{}e} \PYG{o}{=} \PYG{l+m+mf}{0.01}  \PYG{c+c1}{\PYGZsh{} avoid some experiences to have 0 probability}
    \PYG{n}{PER\PYGZus{}a} \PYG{o}{=} \PYG{l+m+mf}{0.6}  \PYG{c+c1}{\PYGZsh{}  control randomness in stochastic prioritization}
    \PYG{n}{PER\PYGZus{}b} \PYG{o}{=} \PYG{l+m+mf}{0.4}  \PYG{c+c1}{\PYGZsh{} importance\PYGZhy{}sampling, from initial value increasing to 1}
    \PYG{n}{PER\PYGZus{}b\PYGZus{}increment\PYGZus{}per\PYGZus{}sampling} \PYG{o}{=} \PYG{l+m+mf}{0.001}
    \PYG{n}{absolute\PYGZus{}error\PYGZus{}upper} \PYG{o}{=} \PYG{l+m+mf}{1.}  \PYG{c+c1}{\PYGZsh{} clipped abs error}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{capacity}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Making the tree}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree} \PYG{o}{=} \PYG{n}{SumTree}\PYG{p}{(}\PYG{n}{capacity}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{add}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{experience}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Find the max priority of leaf nodes}
    \PYG{n}{max\PYGZus{}priority} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{capacity}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} If the max priority = 0 we can\PYGZsq{}t put priority = 0}
    \PYG{c+c1}{\PYGZsh{} since this experience will never have a chance to be selected}
    \PYG{c+c1}{\PYGZsh{} So we use a minimum priority}
    \PYG{k}{if} \PYG{n}{max\PYGZus{}priority} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
    \PYG{n}{max\PYGZus{}priority} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{absolute\PYGZus{}error\PYGZus{}upper}
    \PYG{c+c1}{\PYGZsh{} set the priority for new experience}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{n}{max\PYGZus{}priority}\PYG{p}{,} \PYG{n}{experience}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{sample}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Create a minibatch array that will contains the minibatch}
    \PYG{n}{minibatch} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{b\PYGZus{}idx} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{int32}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} array to store sample priorities}
    \PYG{n}{priorities} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n}\PYG{p}{,}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Calculate the priority segment}
    \PYG{c+c1}{\PYGZsh{} we divide the Range[0, ptotal] into n ranges}
    \PYG{n}{priority\PYGZus{}segment} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{total\PYGZus{}priority} \PYG{o}{/} \PYG{n}{n}  \PYG{c+c1}{\PYGZsh{} priority segment}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} A value is uniformly sample from each range}
    \PYG{n}{a}\PYG{p}{,} \PYG{n}{b} \PYG{o}{=} \PYG{n}{priority\PYGZus{}segment} \PYG{o}{*} \PYG{n}{i}\PYG{p}{,} \PYG{n}{priority\PYGZus{}segment} \PYG{o}{*} \PYG{p}{(}\PYG{n}{i} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{value} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{n}{a}\PYG{p}{,} \PYG{n}{b}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Experience that correspond to each value is retrieved}
    \PYG{n}{index}\PYG{p}{,} \PYG{n}{priority}\PYG{p}{,} \PYG{n}{data} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{get\PYGZus{}leaf}\PYG{p}{(}\PYG{n}{value}\PYG{p}{)}
    \PYG{n}{b\PYGZus{}idx}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{=} \PYG{n}{index}
    \PYG{n}{priorities}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{priority}   \PYG{c+c1}{\PYGZsh{} experimental}
    \PYG{n}{minibatch}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}\PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{b\PYGZus{}idx}\PYG{p}{,} \PYG{n}{minibatch}\PYG{p}{,}

    \PYG{k}{def} \PYG{n+nf}{batch\PYGZus{}update}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{tree\PYGZus{}idx}\PYG{p}{,} \PYG{n}{abs\PYGZus{}errors}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{abs\PYGZus{}errors} \PYG{o}{+}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{PER\PYGZus{}e}  \PYG{c+c1}{\PYGZsh{} convert to abs and avoid 0}
    \PYG{n}{clipped\PYGZus{}errors} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{minimum}\PYG{p}{(}\PYG{n}{abs\PYGZus{}errors}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{absolute\PYGZus{}error\PYGZus{}upper}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} stochastic prioritization}
    \PYG{n}{ps} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{power}\PYG{p}{(}\PYG{n}{clipped\PYGZus{}errors}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{PER\PYGZus{}a}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} values between 0 and 1}
    \PYG{c+c1}{\PYGZsh{} convert priorities into probabilities}
    \PYG{n}{prob} \PYG{o}{=} \PYG{n}{ps} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{ps}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} experimental}
    \PYG{c+c1}{\PYGZsh{} importance sampling weights: iw = [1 / ( N * P)]\PYGZca{}b}
    \PYG{n}{is\PYGZus{}wts} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{power}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{prob}\PYG{p}{)} \PYG{o}{*} \PYG{n}{prob}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{PER\PYGZus{}b}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{ti}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{n}{iw} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{tree\PYGZus{}idx}\PYG{p}{,} \PYG{n}{ps}\PYG{p}{,} \PYG{n}{is\PYGZus{}wts}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{new\PYGZus{}p} \PYG{o}{=} \PYG{n}{p} \PYG{o}{*} \PYG{n}{iw}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{o}{.}\PYG{n}{update}\PYG{p}{(}\PYG{n}{ti}\PYG{p}{,} \PYG{n}{new\PYGZus{}p}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} gradually increase PER\PYGZus{}b for more focus on high\PYGZhy{}error experience}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{PER\PYGZus{}b} \PYG{o}{=} \PYG{n+nb}{min}\PYG{p}{(}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{PER\PYGZus{}b} \PYG{o}{+} \PYGZbs{}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{PER\PYGZus{}b\PYGZus{}increment\PYGZus{}per\PYGZus{}sampling}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}len\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}getitem\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{index}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{tree}\PYG{p}{[}\PYG{n}{index}\PYG{p}{]}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@26}
    \PYG{k}{class} \PYG{n+nc}{DQNPERAgent}\PYG{p}{(}\PYG{n}{DQNAgent}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{:} \PYG{n+nb}{tuple}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{:} \PYG{n+nb}{int}\PYG{p}{,}
    \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{2000}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{24}\PYG{p}{,}
    \PYG{n}{ddqn\PYGZus{}flag}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{buffer\PYGZus{}size}\PYG{p}{,}
    \PYG{c+c1}{\PYGZsh{} uses a sumtree Buffer}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory} \PYG{o}{=} \PYG{n}{STBuffer}\PYG{p}{(}\PYG{n}{capacity}\PYG{o}{=}\PYG{n}{buffer\PYGZus{}size}\PYG{p}{)}


    \PYG{k}{def} \PYG{n+nf}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{train\PYGZus{}start}\PYG{p}{:}
    \PYG{k}{return}
    \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n+nb}{min}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{tree\PYGZus{}idx}\PYG{p}{,} \PYG{n}{mini\PYGZus{}batch} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}
    \PYG{n}{states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{next\PYGZus{}states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{actions} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{dones} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{mini\PYGZus{}batch}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{states}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
    \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}
    \PYG{n}{next\PYGZus{}states}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}
    \PYG{n}{dones}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}  \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}
    \PYG{n}{q\PYGZus{}values\PYGZus{}cs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{verbose}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{q\PYGZus{}values\PYGZus{}cs\PYGZus{}old} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{)}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} deep copy}
    \PYG{n}{max\PYGZus{}q\PYGZus{}values\PYGZus{}ns} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}target\PYGZus{}q\PYGZus{}value}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Q\PYGZhy{}learning updates}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{action} \PYG{o}{=} \PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} check}
    \PYG{n}{done} \PYG{o}{=} \PYG{n}{dones}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{bool}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} check}
    \PYG{n}{reward} \PYG{o}{=} \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} check}
    \PYG{k}{if} \PYG{n}{done}\PYG{p}{:}
    \PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{=} \PYG{n}{reward}
    \PYG{k}{else}\PYG{p}{:}
    \PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{n}{action}\PYG{p}{]} \PYG{o}{=} \PYG{n}{reward} \PYG{o}{+} \PYGZbs{}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{max\PYGZus{}q\PYGZus{}values\PYGZus{}ns}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
    \PYG{c+c1}{\PYGZsh{} update experience priorities}
    \PYG{n}{indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{int32}\PYG{p}{)}
    \PYG{n}{actions} \PYG{o}{=} \PYG{n}{actions}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
    \PYG{n}{absolute\PYGZus{}errors} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{abs}\PYG{p}{(}\PYG{n}{q\PYGZus{}values\PYGZus{}cs\PYGZus{}old}\PYG{p}{[}\PYG{n}{indices}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYGZbs{}
    \PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{[}\PYG{n}{indices}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{]}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} update sample priorities}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{memory}\PYG{o}{.}\PYG{n}{batch\PYGZus{}update}\PYG{p}{(}\PYG{n}{tree\PYGZus{}idx}\PYG{p}{,} \PYG{n}{absolute\PYGZus{}errors}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} train the Q network}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{q\PYGZus{}values\PYGZus{}cs}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n}{batch\PYGZus{}size}\PYG{p}{,}
    \PYG{n}{epochs} \PYG{o}{=} \PYG{l+m+mi}{1}\PYG{p}{,}
    \PYG{n}{verbose} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} decay epsilon over time}
    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}epsilon}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@27}
    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{,}
    \PYG{n}{train\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{copy\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{wtfile\PYGZus{}prefix}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
    \PYG{n}{file} \PYG{o}{=} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{wtfile\PYGZus{}prefix} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
    \PYG{n}{wt\PYGZus{}filename} \PYG{o}{=} \PYG{n}{wtfile\PYGZus{}prefix} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}best\PYGZus{}model.weights.h5}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{k}{else}\PYG{p}{:}
    \PYG{n}{wt\PYGZus{}filename} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{best\PYGZus{}model.weights.h5}\PYG{l+s+s1}{\PYGZsq{}}

    \PYG{c+c1}{\PYGZsh{} choose between soft \PYGZam{} hard target update}
    \PYG{n}{tau} \PYG{o}{=} \PYG{l+m+mf}{0.1} \PYG{k}{if} \PYG{n}{copy\PYGZus{}freq} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{10} \PYG{k}{else} \PYG{l+m+mf}{1.0}
    \PYG{n}{max\PYGZus{}steps} \PYG{o}{=} \PYG{l+m+mi}{200}
    \PYG{n}{car\PYGZus{}positions} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{scores}\PYG{p}{,} \PYG{n}{avg\PYGZus{}scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{for} \PYG{n}{e} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} make observation}
    \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{state} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
    \PYG{n}{ep\PYGZus{}reward} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{t} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{max\PYGZus{}pos} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{99.0}
    \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
    \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
    \PYG{c+c1}{\PYGZsh{} take action using epsilon\PYGZhy{}greedy policy}
    \PYG{n}{action} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{get\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} transition to next state}
    \PYG{c+c1}{\PYGZsh{} and collect reward from the environment}
    \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
    \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} (\PYGZhy{}1, 4)}
    \PYG{c+c1}{\PYGZsh{} reward engineering \PYGZhy{} important step}
    \PYG{k}{if} \PYG{n}{next\PYGZus{}state}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mf}{0.5}\PYG{p}{:}
    \PYG{n}{reward} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{200}
    \PYG{k}{else}\PYG{p}{:}
    \PYG{n}{reward} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{o}{*}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYGZbs{}
    \PYG{o}{+} \PYG{l+m+mi}{3}\PYG{o}{*}\PYG{n+nb}{abs}\PYG{p}{(}\PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} track maximum car position}
    \PYG{k}{if} \PYG{n}{next\PYGZus{}state}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{\PYGZgt{}} \PYG{n}{max\PYGZus{}pos}\PYG{p}{:}
    \PYG{n}{max\PYGZus{}pos} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

    \PYG{c+c1}{\PYGZsh{} store experience in replay buffer}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{store\PYGZus{}experience}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}
    \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
    \PYG{n}{ep\PYGZus{}reward} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
    \PYG{n}{t} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}

    \PYG{c+c1}{\PYGZsh{} train}
    \PYG{k}{if} \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{\PYGZpc{}} \PYG{n}{train\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} update target model}
    \PYG{k}{if} \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{\PYGZpc{}} \PYG{n}{copy\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{update\PYGZus{}target\PYGZus{}model}\PYG{p}{(}\PYG{n}{tau}\PYG{o}{=}\PYG{n}{tau}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{done} \PYG{o+ow}{and} \PYG{n}{t} \PYG{o}{\PYGZlt{}} \PYG{n}{max\PYGZus{}steps}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Successfully solved the problem in }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{ episodes. }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{    max\PYGZus{}pos:}\PYG{l+s+si}{\PYGZob{}:.2f\PYGZcb{}}\PYG{l+s+s1}{, steps: }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{e}\PYG{p}{,} \PYG{n}{max\PYGZus{}pos}\PYG{p}{,} \PYG{n}{t}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{agent}\PYG{o}{.}\PYG{n}{save\PYGZus{}model}\PYG{p}{(}\PYG{n}{wt\PYGZus{}filename}\PYG{p}{)}

    \PYG{k}{if} \PYG{n}{t} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{max\PYGZus{}steps}\PYG{p}{:}
    \PYG{k}{break}
    \PYG{c+c1}{\PYGZsh{} episode ends here}
    \PYG{n}{car\PYGZus{}positions}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{state}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}reward}\PYG{p}{)}
    \PYG{n}{avg\PYGZus{}scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
    \PYG{n}{file}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}reward}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{    }\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{max\PYGZus{}pos}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{t}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
    \PYG{n}{file}\PYG{o}{.}\PYG{n}{flush}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{os}\PYG{o}{.}\PYG{n}{fsync}\PYG{p}{(}\PYG{n}{file}\PYG{o}{.}\PYG{n}{fileno}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} write to the file immediately}
    \PYG{c+c1}{\PYGZsh{}print on console}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}r}\PYG{l+s+s1}{e:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, ep\PYGZus{}reward: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}reward}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, avg\PYGZus{}ep\PYGZus{}reward: }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{    }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, ep\PYGZus{}steps: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{t}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, max\PYGZus{}pos: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{max\PYGZus{}pos}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{sys}\PYG{o}{.}\PYG{n}{stdout}\PYG{o}{.}\PYG{n}{flush}\PYG{p}{(}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{End of training}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{file}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@28}
    \PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
    \PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
    \PYG{k+kn}{import} \PYG{n+nn}{keras}

    \PYG{c+c1}{\PYGZsh{} create a gym environment}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{MountainCar\PYGZhy{}v0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rgb\PYGZus{}array}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{n\PYGZus{}actions} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observation shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action size: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Max episodic steps: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Create a model}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{keras}\PYG{o}{.}\PYG{n}{Sequential}\PYG{p}{(}\PYG{p}{[}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{input\PYGZus{}shape}\PYG{o}{=}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{60}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{linear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{p}{]}\PYG{p}{)}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{loss}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}  \PYGZbs{}
    \PYG{n}{optimizer}\PYG{o}{=}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{)}\PYG{p}{)}


    \PYG{c+c1}{\PYGZsh{} create a DQN PER Agent}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{DQNPERAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{,}
    \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{20000}\PYG{p}{,}
    \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{64}\PYG{p}{,}
    \PYG{n}{model}\PYG{o}{=}\PYG{n}{model}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} train the agent}
    \PYG{n}{train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{copy\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,} \PYGZbs{}
    \PYG{n}{filename}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mc\PYGZus{}dqn\PYGZus{}per.txt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@29}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{c+c1}{\PYGZsh{} create an atari environment}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE/MsPacman\PYGZhy{}v5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{grayscale}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
               \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rgb\PYGZus{}array}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape of action space: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape of observation space: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} initialize environment and make observation}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{shape of x: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{imshow}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}   \PYG{c+c1}{\PYGZsh{} visualize observation}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axis}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{off}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{obs\PYGZus{}shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Max Environment Steps:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observation space low: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{low}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observation space high:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{high}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@30}
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{deque}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{from} \PYG{n+nn}{gym}\PYG{n+nn}{.}\PYG{n+nn}{spaces} \PYG{k+kn}{import} \PYG{n}{Box}

\PYG{k}{class} \PYG{n+nc}{FrameStack}\PYG{p}{(}\PYG{n}{gym}\PYG{o}{.}\PYG{n}{Wrapper}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{    Wrapper that stacks observations from the environment}
\PYG{l+s+sd}{    into a single observation. This wrapper keeps a rolling buffer}
\PYG{l+s+sd}{    of the most recent frames and stacks them}
\PYG{l+s+sd}{    together as a new observation.}
\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{n}{num\PYGZus{}stacked\PYGZus{}frames}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Args:}
\PYG{l+s+sd}{          env: The environment to wrap.}
\PYG{l+s+sd}{          num\PYGZus{}stacked\PYGZus{}frames: The number of frames to stack.}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n+nb}{super}\PYG{p}{(}\PYG{n}{FrameStack}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{env}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}stacked\PYGZus{}frames} \PYG{o}{=} \PYG{n}{num\PYGZus{}stacked\PYGZus{}frames}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frames} \PYG{o}{=} \PYG{n}{deque}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{n}{maxlen}\PYG{o}{=}\PYG{n}{num\PYGZus{}stacked\PYGZus{}frames}\PYG{p}{)}
        \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
        \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{2}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} convert (H, W) to (H, W, D)}
            \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Modify the observation space to accommodate stacked frames}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space} \PYG{o}{=} \PYG{n}{Box}\PYG{p}{(}
            \PYG{n}{low}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{high}\PYG{o}{=}\PYG{l+m+mi}{255}\PYG{p}{,}
            \PYG{n}{shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
                     \PYG{n}{obs\PYGZus{}shape}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}stacked\PYGZus{}frames}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{dtype}
        \PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{reset}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Resets the environment and fills the frame buffer}
\PYG{l+s+sd}{        with initial observations.}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n}{observation} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{n}{observation}\PYG{p}{)}\PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{2}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} convert (H, W) to (H, W, D)}
            \PYG{n}{observation} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{observation}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{num\PYGZus{}stacked\PYGZus{}frames}\PYG{p}{)}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frames}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{observation}\PYG{p}{)}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}stack\PYGZus{}frames}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{step}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{action}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Steps through the environment and stacks}
\PYG{l+s+sd}{            the new observation with previous ones.}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n}{observation}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{info}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
        \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{n}{observation}\PYG{p}{)}\PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{2}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} convert (H, W) to (H, W, D)}
            \PYG{n}{observation} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{observation}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frames}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{observation}\PYG{p}{)}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}stack\PYGZus{}frames}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{info}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}stack\PYGZus{}frames}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Stacks frames from the buffer into a single observation.}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{frames}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@31}
\PYG{k+kn}{import} \PYG{n+nn}{cv2}
\PYG{k+kn}{import} \PYG{n+nn}{sys}

\PYG{k}{class} \PYG{n+nc}{DQNAtariAgent}\PYG{p}{(}\PYG{n}{DQNPERAgent}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{:} \PYG{n+nb}{tuple}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{:} \PYG{n+nb}{int}\PYG{p}{,}
                        \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{2000}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{24}\PYG{p}{,}
                        \PYG{n}{ddqn\PYGZus{}flag}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{per\PYGZus{}flag}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{per\PYGZus{}flag} \PYG{o}{=} \PYG{n}{per\PYGZus{}flag}

        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{per\PYGZus{}flag}\PYG{p}{:}
            \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{buffer\PYGZus{}size}\PYG{p}{,}
                        \PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{ddqn\PYGZus{}flag}\PYG{p}{,} \PYG{n}{model}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{DQNAgent}\PYG{o}{.}\PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{buffer\PYGZus{}size}\PYG{p}{,}
                              \PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{ddqn\PYGZus{}flag}\PYG{p}{,} \PYG{n}{model}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{per\PYGZus{}flag}\PYG{p}{:}
            \PYG{n+nb}{super}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{DQNAgent}\PYG{o}{.}\PYG{n}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}


    \PYG{k}{def} \PYG{n+nf}{preprocess}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{observation}\PYG{p}{,} \PYG{n}{x\PYGZus{}crop}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{172}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y\PYGZus{}crop}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{assert} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{3}\PYG{p}{,} \PYGZbs{}
                      \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Observation must have 3 dimension (H, W, C)}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{n}{output\PYGZus{}shape} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} all but last (H, W)}
        \PYG{c+c1}{\PYGZsh{} crop image}
        \PYG{k}{if} \PYG{n}{x\PYGZus{}crop} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{y\PYGZus{}crop} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{xlow}\PYG{p}{,} \PYG{n}{xhigh} \PYG{o}{=} \PYG{n}{x\PYGZus{}crop}
            \PYG{n}{ylow}\PYG{p}{,} \PYG{n}{yhigh} \PYG{o}{=} \PYG{n}{y\PYGZus{}crop}
            \PYG{n}{observation} \PYG{o}{=} \PYG{n}{observation}\PYG{p}{[}\PYG{n}{xlow}\PYG{p}{:}\PYG{n}{xhigh}\PYG{p}{,} \PYG{n}{ylow}\PYG{p}{:}\PYG{n}{yhigh}\PYG{p}{]}
        \PYG{k}{elif} \PYG{n}{x\PYGZus{}crop} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{y\PYGZus{}crop} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{xlow}\PYG{p}{,} \PYG{n}{xhigh} \PYG{o}{=} \PYG{n}{x\PYGZus{}crop}
            \PYG{n}{observation} \PYG{o}{=} \PYG{n}{observation}\PYG{p}{[}\PYG{n}{xlow}\PYG{p}{:}\PYG{n}{xhigh}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
        \PYG{k}{elif} \PYG{n}{x\PYGZus{}crop} \PYG{o+ow}{is} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{y\PYGZus{}crop} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
                \PYG{n}{ylow}\PYG{p}{,} \PYG{n}{yhigh} \PYG{o}{=} \PYG{n}{y\PYGZus{}crop}
                \PYG{n}{observation} \PYG{o}{=} \PYG{n}{observation}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{ylow}\PYG{p}{:}\PYG{n}{yhigh}\PYG{p}{]}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{observation} \PYG{o}{=} \PYG{n}{observation}

        \PYG{c+c1}{\PYGZsh{} resize image}
        \PYG{n}{observation} \PYG{o}{=} \PYG{n}{cv2}\PYG{o}{.}\PYG{n}{resize}\PYG{p}{(}\PYG{n}{observation}\PYG{p}{,} \PYG{n}{output\PYGZus{}shape}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} normalize image}
        \PYG{n}{observation} \PYG{o}{=} \PYG{n}{observation} \PYG{o}{/} \PYG{l+m+mf}{255.}  \PYG{c+c1}{\PYGZsh{} normalize between 0 \PYGZam{} 1}
        \PYG{k}{return} \PYG{n}{observation}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{env}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{,}
          \PYG{n}{train\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{copy\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{wtfile\PYGZus{}prefix}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}

        \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{file} \PYG{o}{=} \PYG{n+nb}{open}\PYG{p}{(}\PYG{n}{filename}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{w}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{wtfile\PYGZus{}prefix} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{wt\PYGZus{}filename} \PYG{o}{=} \PYG{n}{wtfile\PYGZus{}prefix} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}best\PYGZus{}model.weights.h5}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{wt\PYGZus{}filename} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{best\PYGZus{}model.weights.h5}\PYG{l+s+s1}{\PYGZsq{}}

        \PYG{n}{tau} \PYG{o}{=} \PYG{l+m+mf}{0.01} \PYG{k}{if} \PYG{n}{copy\PYGZus{}freq} \PYG{o}{\PYGZlt{}} \PYG{l+m+mi}{10} \PYG{k}{else} \PYG{l+m+mf}{1.0}

        \PYG{n}{best\PYGZus{}score}\PYG{p}{,} \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}
        \PYG{n}{scores}\PYG{p}{,} \PYG{n}{avg\PYGZus{}scores}\PYG{p}{,} \PYG{n}{avg100\PYGZus{}scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
        \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{k}{for} \PYG{n}{e} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} with framestack wrapper}
            \PYG{c+c1}{\PYGZsh{}state = env.reset()[0] \PYGZsh{} without framestack wrapper}
            \PYG{n}{state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{preprocess}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
            \PYG{n}{state} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
            \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
            \PYG{n}{ep\PYGZus{}reward} \PYG{o}{=} \PYG{l+m+mi}{0}
            \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
                \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
                \PYG{c+c1}{\PYGZsh{} take action}
                \PYG{n}{action} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{get\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
                \PYG{c+c1}{\PYGZsh{} collect reward}
                \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
                \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{preprocess}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} (H, W, C)}
                \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} (B, H, W, C)}
                \PYG{c+c1}{\PYGZsh{} store experiences in eplay buffer}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{store\PYGZus{}experience}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,}
                                          \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}
                \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
                \PYG{n}{ep\PYGZus{}reward} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
                \PYG{c+c1}{\PYGZsh{} train}
                \PYG{k}{if} \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{\PYGZpc{}} \PYG{n}{train\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} update target model}
                \PYG{k}{if} \PYG{n}{global\PYGZus{}step\PYGZus{}cnt} \PYG{o}{\PYGZpc{}} \PYG{n}{copy\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
                    \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}target\PYGZus{}model}\PYG{p}{(}\PYG{n}{tau}\PYG{o}{=}\PYG{n}{tau}\PYG{p}{)}
                \PYG{c+c1}{\PYGZsh{} end of while\PYGZhy{}loop}
            \PYG{k}{if} \PYG{n}{ep\PYGZus{}reward} \PYG{o}{\PYGZgt{}} \PYG{n}{best\PYGZus{}score}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{save\PYGZus{}model}\PYG{p}{(}\PYG{n}{wt\PYGZus{}filename}\PYG{p}{)}
                \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{n}{ep\PYGZus{}reward}
            \PYG{n}{scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}reward}\PYG{p}{)}
            \PYG{n}{avg\PYGZus{}scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{avg100\PYGZus{}scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
            \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
                \PYG{n}{file}\PYG{o}{.}\PYG{n}{write}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}reward}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{                                 }\PYG{l+s+se}{\PYGZbs{}t}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
                \PYG{n}{file}\PYG{o}{.}\PYG{n}{flush}\PYG{p}{(}\PYG{p}{)}
                \PYG{n}{os}\PYG{o}{.}\PYG{n}{fsync}\PYG{p}{(}\PYG{n}{file}\PYG{o}{.}\PYG{n}{fileno}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}r}\PYG{l+s+s1}{e:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, ep\PYGZus{}reward: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}reward}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{                        avg\PYGZus{}ep\PYGZus{}reward: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
            \PYG{n}{sys}\PYG{o}{.}\PYG{n}{stdout}\PYG{o}{.}\PYG{n}{flush}\PYG{p}{(}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} end of for loop}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{End of training}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{filename} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{file}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@32}
\PYG{c+c1}{\PYGZsh{} create an instance of gym environment}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE/MsPacman\PYGZhy{}v5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{grayscale}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
                              \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rgb\PYGZus{}array}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Stack the frames using Wrapper}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{FrameStack}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{num\PYGZus{}stacked\PYGZus{}frames}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{observation shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}

\PYG{n}{n\PYGZus{}actions} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action space dimension: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} All images will be resized to this size}
\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{84}\PYG{p}{,} \PYG{l+m+mi}{84}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} create a sequential model for Q Network}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{keras}\PYG{o}{.}\PYG{n}{Sequential}\PYG{p}{(}\PYG{p}{[}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Conv2D}\PYG{p}{(}\PYG{l+m+mi}{32}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{strides}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{n}{padding}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{he\PYGZus{}uniform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                          \PYG{n}{input\PYGZus{}shape}\PYG{o}{=}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{MaxPooling2D}\PYG{p}{(}\PYG{n}{pool\PYGZus{}size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Conv2D}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{strides}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{padding}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{same}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
               \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{he\PYGZus{}uniform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{MaxPooling2D}\PYG{p}{(}\PYG{n}{pool\PYGZus{}size}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Flatten}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                        \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{he\PYGZus{}uniform}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
      \PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{linear}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
  \PYG{p}{]}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{compile}\PYG{p}{(}\PYG{n}{loss}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{optimizer}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{adam}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create DQN PER Agent}
\PYG{n}{agent} \PYG{o}{=} \PYG{n}{DQNAtariAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{,}
                      \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{60000}\PYG{p}{,}
                      \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{64}\PYG{p}{,}
                      \PYG{n}{model}\PYG{o}{=}\PYG{n}{model}\PYG{p}{,} \PYG{n}{per\PYGZus{}flag}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Train the agent}
\PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{400}\PYG{p}{,} \PYG{n}{train\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,}
               \PYG{n}{copy\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{filename}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pacman\PYGZus{}dqn\PYGZus{}per.txt}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@33}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k}{as} \PYG{n+nn}{tf}
\PYG{k}{class} \PYG{n+nc}{PolicyNetwork}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{0.0001}\PYG{p}{,}
                        \PYG{n}{fc1\PYGZus{}dim}\PYG{o}{=}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{fc2\PYGZus{}dim}\PYG{o}{=}\PYG{l+m+mi}{256}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fc1\PYGZus{}dim} \PYG{o}{=} \PYG{n}{fc1\PYGZus{}dim}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fc2\PYGZus{}dim} \PYG{o}{=} \PYG{n}{fc2\PYGZus{}dim}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{n\PYGZus{}actions} \PYG{o}{=} \PYG{n}{n\PYGZus{}actions}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{lr}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{inputs} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{fc1} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fc1\PYGZus{}dim}\PYG{p}{,}
                              \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{)}
        \PYG{n}{fc2} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{fc2\PYGZus{}dim}\PYG{p}{,}
                              \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{fc1}\PYG{p}{)}
        \PYG{n}{outputs} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{n\PYGZus{}actions}\PYG{p}{,}
                              \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softmax}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{fc2}\PYG{p}{)}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{p}{,} \PYG{n}{outputs}\PYG{p}{,}
                              \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{policy\PYGZus{}network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{pi} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{pi}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@34}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow\PYGZus{}probability} \PYG{k}{as} \PYG{n+nn}{tfp}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k}{class} \PYG{n+nc}{REINFORCEAgent}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.0005}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{alpha}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{n\PYGZus{}actions} \PYG{o}{=} \PYG{n}{n\PYGZus{}actions}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{states} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actions} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{c+c1}{\PYGZsh{} create policy network}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{policy} \PYG{o}{=} \PYG{n}{PolicyNetwork}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{choose\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{obs}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{probs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
        \PYG{n}{action\PYGZus{}probs} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{probs}\PYG{p}{)}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{action\PYGZus{}probs}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{action}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

    \PYG{k}{def} \PYG{n+nf}{store\PYGZus{}transitions}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{states}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actions}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{calculate\PYGZus{}return}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{G} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{G\PYGZus{}sum} \PYG{o}{=} \PYG{l+m+mi}{0}
            \PYG{k}{for} \PYG{n}{k} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{t}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{G\PYGZus{}sum} \PYG{o}{+}\PYG{o}{=} \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{k}\PYG{p}{]} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma}
            \PYG{n}{G}\PYG{p}{[}\PYG{n}{t}\PYG{p}{]} \PYG{o}{=} \PYG{n}{G\PYGZus{}sum}
        \PYG{k}{return} \PYG{n}{G}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actions}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{rewards}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} compute returns}
        \PYG{n}{G} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{calculate\PYGZus{}return}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} optimize with gradient ascent}
        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
            \PYG{n}{loss} \PYG{o}{=} \PYG{l+m+mi}{0}
            \PYG{k}{for} \PYG{n}{idx}\PYG{p}{,} \PYG{p}{(}\PYG{n}{g}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{G}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{states}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
                \PYG{n}{state} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
                \PYG{n}{probs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
                \PYG{n}{action\PYGZus{}probs} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{probs}\PYG{p}{)}
                \PYG{n}{log\PYGZus{}prob} \PYG{o}{=} \PYG{n}{action\PYGZus{}probs}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{[}\PYG{n}{idx}\PYG{p}{]}\PYG{p}{)}
                \PYG{n}{loss} \PYG{o}{+}\PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{g} \PYG{o}{*} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n}{log\PYGZus{}prob}\PYG{p}{)}
        \PYG{n}{trainable\PYGZus{}params} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{policy}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}
        \PYG{n}{gradient} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{loss}\PYG{p}{,}  \PYG{n}{trainable\PYGZus{}params}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{policy}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{gradient}\PYG{p}{,} \PYGZbs{}
                                             \PYG{n}{trainable\PYGZus{}params}\PYG{p}{)}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} empty the buffer}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{states} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actions} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@35}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k}{def} \PYG{n+nf}{train\PYGZus{}agent\PYGZus{}for\PYGZus{}env}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{scores}\PYG{p}{,} \PYG{n}{avgscores}\PYG{p}{,} \PYG{n}{avg100scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{e} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
        \PYG{n}{score} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{choose\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{store\PYGZus{}transitions}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{)}
            \PYG{n}{score} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
            \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
        \PYG{c+c1}{\PYGZsh{} end of while loop}
        \PYG{c+c1}{\PYGZsh{} train the agent at the end of each episode}
        \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{score}\PYG{p}{)}
        \PYG{n}{avgscores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{avg100scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{e} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{100}\PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{episode:}\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{, score: }\PYG{l+s+si}{\PYGZob{}:.2f\PYGZcb{}}\PYG{l+s+s1}{, avgscore: }\PYG{l+s+si}{\PYGZob{}:.2f\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{               avg100score: }\PYG{l+s+si}{\PYGZob{}:.2f\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{e}\PYG{p}{,} \PYG{n}{score}\PYG{p}{,} \PYGZbs{}
                  \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} end of for\PYGZhy{}loop}
    \PYG{n}{file}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@36}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{c+c1}{\PYGZsh{} instantiate a gym environment}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CartPole\PYGZhy{}v0}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observation shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action Size: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Max Episode steps: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} create an RL agent}
\PYG{n}{agent} \PYG{o}{=} \PYG{n}{REINFORCEAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} train the RL agent on}
\PYG{n}{train\PYGZus{}agent\PYGZus{}for\PYGZus{}env}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{2000}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@37}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{LunarLander\PYGZhy{}v2}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{continuous}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
\PYG{c+c1}{\PYGZsh{} create a RL agent}
\PYG{n}{agent} \PYG{o}{=} \PYG{n}{REINFORCEAgent2}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Train for the environment}
\PYG{n}{train\PYGZus{}agent\PYGZus{}for\PYGZus{}env}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{4000}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@38}
\PYG{k}{class} \PYG{n+nc}{OUActionNoise}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{mean}\PYG{p}{,} \PYG{n}{std\PYGZus{}deviation}\PYG{p}{,} \PYG{n}{theta}\PYG{o}{=}\PYG{l+m+mf}{0.15}\PYG{p}{,}
                              \PYG{n}{dt}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}2}\PYG{p}{,} \PYG{n}{x\PYGZus{}initial}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{theta} \PYG{o}{=} \PYG{n}{theta}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mean} \PYG{o}{=} \PYG{n}{mean}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{std\PYGZus{}dev} \PYG{o}{=} \PYG{n}{std\PYGZus{}deviation}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dt} \PYG{o}{=} \PYG{n}{dt}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x\PYGZus{}initial} \PYG{o}{=} \PYG{n}{x\PYGZus{}initial}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{p}{(}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x\PYGZus{}prev}
            \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{theta} \PYG{o}{*} \PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mean} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x\PYGZus{}prev}\PYG{p}{)} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dt}
            \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{std\PYGZus{}dev} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{dt}\PYG{p}{)} \PYG{o}{*} \PYGZbs{}
                     \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mean}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}
        \PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Store x into x\PYGZus{}prev}
        \PYG{c+c1}{\PYGZsh{} Makes next noise dependent on current one}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x\PYGZus{}prev} \PYG{o}{=} \PYG{n}{x}
        \PYG{k}{return} \PYG{n}{x}

    \PYG{k}{def} \PYG{n+nf}{reset}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x\PYGZus{}initial} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x\PYGZus{}prev} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x\PYGZus{}initial}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{x\PYGZus{}prev} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@39}
\PYG{k}{class} \PYG{n+nc}{Actor}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.0003}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{learning\PYGZus{}rate}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}
        \PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{model}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{clone\PYGZus{}model}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} target shares same weights as the primary model in the beginning}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{initializer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{RandomUniform}\PYG{p}{(}
                                    \PYG{n}{minval}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{maxval}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
        \PYG{n}{s\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{fc1} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                   \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{initializer}\PYG{p}{)}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{)}
        \PYG{n}{fc2} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                   \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{initializer}\PYG{p}{)}\PYG{p}{(}\PYG{n}{fc1}\PYG{p}{)}
        \PYG{n}{a\PYGZus{}out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tanh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                     \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{initializer}\PYG{p}{)}\PYG{p}{(}\PYG{n}{fc2}\PYG{p}{)}
        \PYG{n}{a\PYGZus{}out} \PYG{o}{=} \PYG{n}{a\PYGZus{}out} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{,} \PYG{n}{a\PYGZus{}out}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{actor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{target}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{states}
        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{target}\PYG{p}{:}
            \PYG{n}{pi} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{pi} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{pi}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}target}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{tau}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{model\PYGZus{}weights} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{target\PYGZus{}weights} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} update weights layer\PYGZhy{}by\PYGZhy{}layer using Polyak Averaging}
        \PYG{n}{new\PYGZus{}weights} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{for} \PYG{n}{w}\PYG{p}{,} \PYG{n}{w\PYGZus{}dash} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{model\PYGZus{}weights}\PYG{p}{,} \PYG{n}{target\PYGZus{}weights}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{new\PYGZus{}w} \PYG{o}{=} \PYG{n}{tau} \PYG{o}{*} \PYG{n}{w} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{tau}\PYG{p}{)} \PYG{o}{*} \PYG{n}{w\PYGZus{}dash}
            \PYG{n}{new\PYGZus{}weights}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{new\PYGZus{}w}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}\PYG{n}{new\PYGZus{}weights}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{critic}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
            \PYG{n}{actor\PYGZus{}weights} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}
            \PYG{n}{actions} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}values} \PYG{o}{=} \PYG{n}{critic}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} \PYGZhy{}ve value is used to maximize the function}
            \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{critic\PYGZus{}values}\PYG{p}{)}
        \PYG{n}{actor\PYGZus{}grad} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{actor\PYGZus{}loss}\PYG{p}{,} \PYG{n}{actor\PYGZus{}weights}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grad}\PYG{p}{,} \PYG{n}{actor\PYGZus{}weights}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{actor\PYGZus{}loss}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@40}
\PYG{k}{class} \PYG{n+nc}{Critic}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.0003}\PYG{p}{,}
                \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,}
                \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{learning\PYGZus{}rate}

        \PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{model}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{clone\PYGZus{}model}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} target shares same weights as the main model in the beginning}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{s\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{s\PYGZus{}out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{16}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{)}
        \PYG{n}{s\PYGZus{}out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{32}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{s\PYGZus{}out}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} action as input}
        \PYG{n}{a\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{p}{(}\PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{p}{)}\PYG{p}{)}
        \PYG{n}{a\PYGZus{}out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{32}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{a\PYGZus{}input}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} concat [s, a]}
        \PYG{n}{concat} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Concatenate}\PYG{p}{(}\PYG{p}{)}\PYG{p}{(}\PYG{p}{[}\PYG{n}{s\PYGZus{}out}\PYG{p}{,} \PYG{n}{a\PYGZus{}out}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{concat}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{n}{net\PYGZus{}out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} output is the Q\PYGZhy{}value output}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{s\PYGZus{}input}\PYG{p}{,} \PYG{n}{a\PYGZus{}input}\PYG{p}{]}\PYG{p}{,}
                                      \PYG{n}{outputs}\PYG{o}{=}\PYG{n}{net\PYGZus{}out}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{critic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{target}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{target}\PYG{p}{:}
            \PYG{n}{value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{p}{[}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{]}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target}\PYG{p}{(}\PYG{p}{[}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{]}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{value}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}target}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{tau}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{model\PYGZus{}weights} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{target\PYGZus{}weights} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} update weights layer\PYGZhy{}by\PYGZhy{}layer using Polyak Averaging}
        \PYG{n}{new\PYGZus{}weights} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{for} \PYG{n}{w}\PYG{p}{,} \PYG{n}{w\PYGZus{}dash} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{model\PYGZus{}weights}\PYG{p}{,} \PYG{n}{target\PYGZus{}weights}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{new\PYGZus{}w} \PYG{o}{=} \PYG{n}{tau} \PYG{o}{*} \PYG{n}{w} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{tau}\PYG{p}{)} \PYG{o}{*} \PYG{n}{w\PYGZus{}dash}
            \PYG{n}{new\PYGZus{}weights}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{new\PYGZus{}w}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}\PYG{n}{new\PYGZus{}weights}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{,} \PYG{n}{actor}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
            \PYG{n}{critic\PYGZus{}weights} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}
            \PYG{n}{target\PYGZus{}actions} \PYG{o}{=} \PYG{n}{actor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{target}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
            \PYG{n}{target\PYGZus{}q\PYGZus{}values} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target}\PYG{p}{(}\PYG{p}{[}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{target\PYGZus{}actions}\PYG{p}{]}\PYG{p}{)}
            \PYG{n}{y} \PYG{o}{=} \PYG{n}{rewards} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1}\PYG{o}{\PYGZhy{}}\PYG{n}{dones}\PYG{p}{)} \PYG{o}{*} \PYG{n}{target\PYGZus{}q\PYGZus{}values}
            \PYG{n}{q\PYGZus{}values} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{p}{[}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{]}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{y} \PYG{o}{\PYGZhy{}} \PYG{n}{q\PYGZus{}values}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{critic\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{critic\PYGZus{}loss}\PYG{p}{,} \PYG{n}{critic\PYGZus{}weights}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}grads}\PYG{p}{,} \PYG{n}{critic\PYGZus{}weights}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{critic\PYGZus{}loss}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@41}
\PYG{k}{class} \PYG{n+nc}{DDPGAgent}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                 \PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n}{buffer\PYGZus{}capacity}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}lower\PYGZus{}bound}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                \PYG{n}{lr\PYGZus{}a}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,} \PYG{n}{lr\PYGZus{}c}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,}
                 \PYG{n}{noise\PYGZus{}std}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,}
                \PYG{n}{actor\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                \PYG{n}{critic\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer\PYGZus{}capacity} \PYG{o}{=} \PYG{n}{buffer\PYGZus{}capacity}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n}{batch\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}lower\PYGZus{}bound} \PYG{o}{=} \PYG{n}{action\PYGZus{}lower\PYGZus{}bound}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}a} \PYG{o}{=} \PYG{n}{lr\PYGZus{}a}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}c} \PYG{o}{=} \PYG{n}{lr\PYGZus{}c}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{noise\PYGZus{}std} \PYG{o}{=} \PYG{n}{noise\PYGZus{}std}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor} \PYG{o}{=} \PYG{n}{Actor}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                           \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}a}\PYG{p}{,}
                          \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{,}
                          \PYG{n}{model}\PYG{o}{=}\PYG{n}{actor\PYGZus{}model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic} \PYG{o}{=} \PYG{n}{Critic}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                            \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}c}\PYG{p}{,}
                            \PYG{n}{gamma}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma}\PYG{p}{,}
                            \PYG{n}{model}\PYG{o}{=}\PYG{n}{critic\PYGZus{}model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer} \PYG{o}{=} \PYG{n}{ReplayBuffer}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer\PYGZus{}capacity}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}noise} \PYG{o}{=} \PYG{n}{OUActionNoise}\PYG{p}{(}\PYG{n}{mean}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,}
                 \PYG{n}{std\PYGZus{}deviation}\PYG{o}{=}\PYG{n+nb}{float}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{noise\PYGZus{}std}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{noise} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}noise}\PYG{p}{(}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} add noise to action}
        \PYG{n}{sampled\PYGZus{}action} \PYG{o}{=} \PYG{n}{action}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)} \PYG{o}{+} \PYG{n}{noise}
        \PYG{c+c1}{\PYGZsh{} check action bounds}
        \PYG{n}{valid\PYGZus{}action} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{clip}\PYG{p}{(}\PYG{n}{sampled\PYGZus{}action}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}lower\PYGZus{}bound}\PYG{p}{,}
                                           \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{valid\PYGZus{}action}

    \PYG{k}{def} \PYG{n+nf}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{:}
            \PYG{k}{return}
        \PYG{n}{mini\PYGZus{}batch} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}
        \PYG{n}{states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{*}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{dones} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{mini\PYGZus{}batch}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{states}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
            \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}
            \PYG{n}{next\PYGZus{}states}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}
            \PYG{n}{dones} \PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}
        \PYG{n}{states} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}states} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{dones} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{dones}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{a\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{p}{)}
        \PYG{n}{c\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYGZbs{}
                           \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{a\PYGZus{}loss}\PYG{p}{,} \PYG{n}{c\PYGZus{}loss}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}targets}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{tau\PYGZus{}a}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{tau\PYGZus{}c}\PYG{o}{=}\PYG{l+m+mf}{0.02}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{update\PYGZus{}target}\PYG{p}{(}\PYG{n}{tau\PYGZus{}a}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{update\PYGZus{}target}\PYG{p}{(}\PYG{n}{tau\PYGZus{}c}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@42}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{import} \PYG{n+nn}{os}
\PYG{k+kn}{import} \PYG{n+nn}{sys}
\PYG{k}{def} \PYG{n+nf}{solve\PYGZus{}problem}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{tau\PYGZus{}a}\PYG{p}{,} \PYG{n}{tau\PYGZus{}c} \PYG{o}{=} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{l+m+mf}{0.01}
    \PYG{n}{scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{e} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{ep\PYGZus{}score} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
        \PYG{n}{steps} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
            \PYG{n}{tf\PYGZus{}state} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} take action}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{policy}\PYG{p}{(}\PYG{n}{tf\PYGZus{}state}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} make transition and receive reward}
            \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} store experience in the replay buffer}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{buffer}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{ep\PYGZus{}score} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
            \PYG{n}{total\PYGZus{}steps} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
            \PYG{n}{steps} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
            \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
            \PYG{c+c1}{\PYGZsh{} train the agent}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{experience\PYGZus{}replay}\PYG{p}{(}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} update target models}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{update\PYGZus{}targets}\PYG{p}{(}\PYG{n}{tau\PYGZus{}a}\PYG{p}{,} \PYG{n}{tau\PYGZus{}c}\PYG{p}{)}
            \PYG{k}{if} \PYG{n}{steps} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{200}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} terminate the episode}
                \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{True}
        \PYG{c+c1}{\PYGZsh{} end of while loop}
        \PYG{n}{scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}score}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{e} \PYG{o}{\PYGZpc{}} \PYG{l+m+mi}{20} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{episode: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}score}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{            avg\PYGZus{}score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{            avg50score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{50}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} end of for loop}
    \PYG{n}{file}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@43}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{c+c1}{\PYGZsh{} create an environment}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pendulum\PYGZhy{}v1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{g}\PYG{o}{=}\PYG{l+m+mf}{9.81}\PYG{p}{,} \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rgb\PYGZus{}array}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{action\PYGZus{}ub} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{high}
\PYG{n}{action\PYGZus{}lb} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{low}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observation shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Max episodic Steps: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action space bounds: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{action\PYGZus{}ub}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{action\PYGZus{}lb}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} create an agent}
\PYG{n}{agent} \PYG{o}{=} \PYG{n}{DDPGAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                  \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{buffer\PYGZus{}capacity}\PYG{o}{=}\PYG{l+m+mi}{20000}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{l+m+mf}{2.0}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}lower\PYGZus{}bound}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{2.0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} solve a problem}
\PYG{n}{solve\PYGZus{}problem}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@44}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow\PYGZus{}probability} \PYG{k}{as} \PYG{n+nn}{tfp}
\PYG{k}{class} \PYG{n+nc}{PPOActor}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.0003}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                 \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{lmbda}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{kl\PYGZus{}target}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,}
                 \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{entropy\PYGZus{}coeff}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,}
                 \PYG{n}{critic\PYGZus{}loss\PYGZus{}coeff}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,}
                 \PYG{n}{grad\PYGZus{}clip}\PYG{o}{=}\PYG{l+m+mf}{10.0}\PYG{p}{,}
                 \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}ub} \PYG{o}{=} \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{learning\PYGZus{}rate}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n}{epsilon} \PYG{c+c1}{\PYGZsh{} clip on ratio}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lam} \PYG{o}{=} \PYG{n}{lmbda} \PYG{c+c1}{\PYGZsh{} required for penalty method}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{beta} \PYG{o}{=} \PYG{n}{beta} \PYG{c+c1}{\PYGZsh{} kl penalty coefficient}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{entropy\PYGZus{}coeff} \PYG{o}{=} \PYG{n}{entropy\PYGZus{}coeff}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{c\PYGZus{}loss\PYGZus{}coeff} \PYG{o}{=} \PYG{n}{critic\PYGZus{}loss\PYGZus{}coeff}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{method} \PYG{o}{=} \PYG{n}{method} \PYG{c+c1}{\PYGZsh{} choose between \PYGZsq{}clip\PYGZsq{} and \PYGZsq{}penalty\PYGZsq{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kl\PYGZus{}target} \PYG{o}{=} \PYG{n}{kl\PYGZus{}target}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kl\PYGZus{}value} \PYG{o}{=} \PYG{l+m+mi}{0} \PYG{c+c1}{\PYGZsh{} to store most recent kld value}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip} \PYG{o}{=} \PYG{n}{grad\PYGZus{}clip} \PYG{c+c1}{\PYGZsh{} applying gradient clipping}
        \PYG{c+c1}{\PYGZsh{} create actor model}
        \PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{clone\PYGZus{}model}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} additional parameters}
        \PYG{n}{logstd} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYGZbs{}
                                             \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{logstd} \PYG{o}{=} \PYG{n}{logstd}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{logstd}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{last\PYGZus{}init} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{random\PYGZus{}uniform\PYGZus{}initializer}\PYG{p}{(}\PYG{n}{minval}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{maxval}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{)}
        \PYG{n}{state\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{state\PYGZus{}input}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{net\PYGZus{}out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tanh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                       \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{last\PYGZus{}init}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{net\PYGZus{}out} \PYG{o}{=} \PYG{n}{net\PYGZus{}out} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}ub}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{state\PYGZus{}input}\PYG{p}{,} \PYG{n}{net\PYGZus{}out}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{actor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} input is a tensor}
        \PYG{n}{mean} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{std} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{logstd}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{mean}\PYG{p}{,} \PYG{n}{std} \PYG{c+c1}{\PYGZsh{} return tensor}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state\PYGZus{}batch}\PYG{p}{,} \PYG{n}{action\PYGZus{}batch}\PYG{p}{,} \PYG{n}{advantages}\PYG{p}{,} \PYG{n}{old\PYGZus{}pi}\PYG{p}{,} \PYG{n}{c\PYGZus{}loss}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
            \PYG{n}{mean} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{state\PYGZus{}batch}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{std} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{logstd}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{pi} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{mean}\PYG{p}{,} \PYG{n}{std}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} r = pi/pi\PYGZus{}old}
            \PYG{n}{ratio} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{pi}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n}{action\PYGZus{}batch}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZhy{}}
                          \PYG{n}{old\PYGZus{}pi}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n}{action\PYGZus{}batch}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
            \PYG{k}{if} \PYG{n}{ratio}\PYG{o}{.}\PYG{n}{ndim} \PYG{o}{\PYGZgt{}} \PYG{n}{advantages}\PYG{o}{.}\PYG{n}{ndim}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} match shapes}
                \PYG{n}{ratio} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{ratio}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n}{surr\PYGZus{}obj} \PYG{o}{=} \PYG{n}{ratio} \PYG{o}{*} \PYG{n}{advantages} \PYG{c+c1}{\PYGZsh{} surrogate objective function}
            \PYG{c+c1}{\PYGZsh{} current kl divergence (kld) value}
            \PYG{n}{kld} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{kl\PYGZus{}divergence}\PYG{p}{(}\PYG{n}{old\PYGZus{}pi}\PYG{p}{,} \PYG{n}{pi}\PYG{p}{)}
            \PYG{k}{if} \PYG{n}{kld}\PYG{o}{.}\PYG{n}{ndim} \PYG{o}{\PYGZgt{}} \PYG{n}{advantages}\PYG{o}{.}\PYG{n}{ndim}\PYG{p}{:}
                \PYG{n}{kld} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{kld}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{)}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kl\PYGZus{}value} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{kld}\PYG{p}{)}
            \PYG{n}{entropy} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{pi}\PYG{o}{.}\PYG{n}{entropy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} entropy}
            \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{method} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{penalty}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
                \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{surr\PYGZus{}obj} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{beta} \PYG{o}{*} \PYG{n}{kld}\PYG{p}{)}\PYG{p}{)}
            \PYG{k}{elif} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{method} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
                \PYG{n}{l\PYGZus{}clip} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}
                        \PYG{n}{tf}\PYG{o}{.}\PYG{n}{minimum}\PYG{p}{(}\PYG{n}{surr\PYGZus{}obj}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}value}\PYG{p}{(}\PYG{n}{ratio}\PYG{p}{,}
                        \PYG{l+m+mf}{1.}\PYG{o}{\PYGZhy{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{,} \PYG{l+m+mf}{1.}\PYG{o}{+}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{)} \PYG{o}{*} \PYG{n}{advantages}\PYG{p}{)}\PYG{p}{)}
                \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{n}{l\PYGZus{}clip} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{c\PYGZus{}loss\PYGZus{}coeff} \PYG{o}{*} \PYG{n}{c\PYGZus{}loss}  \PYG{o}{+} \PYGZbs{}
                              \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{entropy\PYGZus{}coeff} \PYG{o}{*} \PYG{n}{entropy}\PYG{p}{)}
            \PYG{k}{else}\PYG{p}{:}
                \PYG{k}{raise} \PYG{n+ne}{ValueError}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{invalid option for PPO method}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
            \PYG{n}{actor\PYGZus{}weights} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}
            \PYG{n}{actor\PYGZus{}grad} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{actor\PYGZus{}loss}\PYG{p}{,} \PYG{n}{actor\PYGZus{}weights}\PYG{p}{)}
            \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
                \PYG{n}{actor\PYGZus{}grad} \PYG{o}{=} \PYG{p}{[}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}value}\PYG{p}{(}\PYG{n}{grad}\PYG{p}{,}\PYGZbs{}
                     \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip}\PYG{p}{)}\PYGZbs{}
                                    \PYG{k}{for} \PYG{n}{grad} \PYG{o+ow}{in} \PYG{n}{actor\PYGZus{}grad}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{}outside gradient tape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grad}\PYG{p}{,} \PYG{n}{actor\PYGZus{}weights}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{actor\PYGZus{}loss}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}beta}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} update beta after each epoch}
        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kl\PYGZus{}value} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kl\PYGZus{}target} \PYG{o}{/} \PYG{l+m+mf}{1.5}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{beta} \PYG{o}{/}\PYG{o}{=} \PYG{l+m+mf}{2.}
        \PYG{k}{elif} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kl\PYGZus{}value} \PYG{o}{\PYGZgt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kl\PYGZus{}target} \PYG{o}{*} \PYG{l+m+mf}{1.5}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{beta} \PYG{o}{*}\PYG{o}{=} \PYG{l+m+mf}{2.}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@45}
\PYG{k}{class} \PYG{n+nc}{PPOCritic}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.0003}\PYG{p}{,}
                 \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,}
                 \PYG{n}{grad\PYGZus{}clip} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,}
                 \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{learning\PYGZus{}rate}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip} \PYG{o}{=} \PYG{n}{grad\PYGZus{}clip}
        \PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{clone\PYGZus{}model}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} input is a tensor}
        \PYG{n}{value} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{value}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{state\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{relu}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{state\PYGZus{}input}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{relu}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{64}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{relu}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{n}{net\PYGZus{}out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Outputs single value for give state\PYGZhy{}action}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{n}{state\PYGZus{}input}\PYG{p}{,} \PYG{n}{outputs}\PYG{o}{=}\PYG{n}{net\PYGZus{}out}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state\PYGZus{}batch}\PYG{p}{,} \PYG{n}{disc\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
            \PYG{n}{critic\PYGZus{}weights} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}
            \PYG{n}{critic\PYGZus{}value} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{state\PYGZus{}batch}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}
                        \PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{disc\PYGZus{}rewards} \PYG{o}{\PYGZhy{}} \PYG{n}{critic\PYGZus{}value}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}grad} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{critic\PYGZus{}loss}\PYG{p}{,} \PYG{n}{critic\PYGZus{}weights}\PYG{p}{)}
            \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
                \PYG{n}{critic\PYGZus{}grad} \PYG{o}{=} \PYG{p}{[}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}value}\PYG{p}{(}\PYG{n}{grad}\PYG{p}{,} \PYGZbs{}
                     \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip}\PYG{p}{)} \PYGZbs{}
                                    \PYG{k}{for} \PYG{n}{grad} \PYG{o+ow}{in} \PYG{n}{critic\PYGZus{}grad}\PYG{p}{]}
        \PYG{c+c1}{\PYGZsh{} outside the gradient tape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}grad}\PYG{p}{,} \PYG{n}{critic\PYGZus{}weights}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{critic\PYGZus{}loss}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@46}
\PYG{k}{class} \PYG{n+nc}{PPOAgent}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                 \PYG{n}{lr\PYGZus{}a}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,} \PYG{n}{lr\PYGZus{}c}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,}
                 \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,}            \PYG{c+c1}{\PYGZsh{} discount factor}
                 \PYG{n}{lmbda}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{,}            \PYG{c+c1}{\PYGZsh{} required for GAE}
                 \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}              \PYG{c+c1}{\PYGZsh{} KL penalty coefficient}
                 \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,}           \PYG{c+c1}{\PYGZsh{} action clip boundary}
                 \PYG{n}{kl\PYGZus{}target}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}        \PYG{c+c1}{\PYGZsh{} required for KL penalty method}
                 \PYG{n}{entropy\PYGZus{}coeff}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}     \PYG{c+c1}{\PYGZsh{} entropy coefficient}
                 \PYG{n}{c\PYGZus{}loss\PYGZus{}coeff}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}      \PYG{c+c1}{\PYGZsh{} critic loss coefficient}
                 \PYG{n}{grad\PYGZus{}clip}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                 \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}         \PYG{c+c1}{\PYGZsh{} choose between \PYGZsq{}clip\PYGZsq{} \PYGZam{} \PYGZsq{}penalty\PYGZsq{}}
                 \PYG{n}{actor\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                 \PYG{n}{critic\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ppo}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor\PYGZus{}lr} \PYG{o}{=} \PYG{n}{lr\PYGZus{}a}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr} \PYG{o}{=} \PYG{n}{lr\PYGZus{}c}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n}{batch\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma} \PYG{c+c1}{\PYGZsh{} discount factor}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon} \PYG{o}{=} \PYG{n}{epsilon} \PYG{c+c1}{\PYGZsh{} clip boundary for prob ratio}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lmbda} \PYG{o}{=} \PYG{n}{lmbda} \PYG{c+c1}{\PYGZsh{} required for GAE}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{initial\PYGZus{}beta} \PYG{o}{=} \PYG{n}{beta} \PYG{c+c1}{\PYGZsh{} required for penalty method}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kl\PYGZus{}target} \PYG{o}{=} \PYG{n}{kl\PYGZus{}target} \PYG{c+c1}{\PYGZsh{} required for updating beta}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{method} \PYG{o}{=} \PYG{n}{method} \PYG{c+c1}{\PYGZsh{} choose between \PYGZsq{}clip\PYGZsq{} \PYGZam{} \PYGZsq{}penalty\PYGZsq{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{c\PYGZus{}loss\PYGZus{}coeff} \PYG{o}{=} \PYG{n}{c\PYGZus{}loss\PYGZus{}coeff}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{entropy\PYGZus{}coeff} \PYG{o}{=} \PYG{n}{entropy\PYGZus{}coeff}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip} \PYG{o}{=} \PYG{n}{grad\PYGZus{}clip} \PYG{c+c1}{\PYGZsh{} apply gradient clipping}
        \PYG{c+c1}{\PYGZsh{} Actor Model}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor} \PYG{o}{=} \PYG{n}{PPOActor}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                              \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor\PYGZus{}lr}\PYG{p}{,}
                              \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{,}
                              \PYG{n}{epsilon}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{epsilon}\PYG{p}{,}
                              \PYG{n}{lmbda}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lmbda}\PYG{p}{,}
                              \PYG{n}{kl\PYGZus{}target}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{kl\PYGZus{}target}\PYG{p}{,}
                              \PYG{n}{beta}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{initial\PYGZus{}beta}\PYG{p}{,}
                              \PYG{n}{entropy\PYGZus{}coeff}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{entropy\PYGZus{}coeff}\PYG{p}{,}
                              \PYG{n}{critic\PYGZus{}loss\PYGZus{}coeff}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{c\PYGZus{}loss\PYGZus{}coeff}\PYG{p}{,}
                              \PYG{n}{method}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{method}\PYG{p}{,}
                              \PYG{n}{grad\PYGZus{}clip}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip}\PYG{p}{,}
                              \PYG{n}{model}\PYG{o}{=}\PYG{n}{actor\PYGZus{}model}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Critic Model}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic} \PYG{o}{=} \PYG{n}{PPOCritic}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                               \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr}\PYG{p}{,}
                                \PYG{n}{gamma}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma}\PYG{p}{,}
                                \PYG{n}{grad\PYGZus{}clip}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip}\PYG{p}{,}
                                \PYG{n}{model}\PYG{o}{=}\PYG{n}{critic\PYGZus{}model}\PYG{p}{)}


    \PYG{k}{def} \PYG{n+nf}{policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{greedy}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{tf\PYGZus{}state} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{mean}\PYG{p}{,} \PYG{n}{std} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{p}{(}\PYG{n}{tf\PYGZus{}state}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{greedy}\PYG{p}{:}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{mean}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{pi} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{mean}\PYG{p}{,} \PYG{n}{std}\PYG{p}{)}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{pi}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{action}\PYG{p}{,} \PYG{n}{shape}\PYG{o}{=}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{p}{)}\PYG{p}{)}
        \PYG{n}{valid\PYGZus{}action} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}value}\PYG{p}{(}\PYG{n}{action}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{,}
                                        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{valid\PYGZus{}action}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{states} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}states} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{dones} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{dones}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} compute advantage \PYGZam{} discounted returns}
        \PYG{n}{target\PYGZus{}values}\PYG{p}{,} \PYG{n}{advantages} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{compute\PYGZus{}advantages}\PYG{p}{(}
                              \PYG{n}{states}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{)}
        \PYG{n}{target\PYGZus{}values} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{target\PYGZus{}values}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{advantages} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{advantages}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} current action probability distribution}
        \PYG{n}{mean}\PYG{p}{,} \PYG{n}{std} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
        \PYG{n}{pi} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{mean}\PYG{p}{,} \PYG{n}{std}\PYG{p}{)}
        \PYG{n}{n\PYGZus{}split} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)} \PYG{o}{/}\PYG{o}{/} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}
        \PYG{k}{assert} \PYG{n}{n\PYGZus{}split} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{buffer length must be greater than batch\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{n}{indexes} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{n\PYGZus{}split}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n+nb}{int}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} training}
        \PYG{n}{a\PYGZus{}loss\PYGZus{}list}\PYG{p}{,} \PYG{n}{c\PYGZus{}loss\PYGZus{}list}\PYG{p}{,} \PYG{n}{kl\PYGZus{}list} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{epochs}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{shuffle}\PYG{p}{(}\PYG{n}{indexes}\PYG{p}{)}
            \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{indexes}\PYG{p}{:}
                \PYG{n}{old\PYGZus{}pi} \PYG{o}{=} \PYG{n}{pi}\PYG{p}{[}\PYG{n}{i} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{:} \PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{]}
                \PYG{n}{s\PYGZus{}split} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{gather}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{indices}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}
                  \PYG{n}{i} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
                \PYG{n}{a\PYGZus{}split} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{gather}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{,} \PYG{n}{indices}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}
                  \PYG{n}{i} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
                \PYG{n}{tv\PYGZus{}split} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{gather}\PYG{p}{(}\PYG{n}{target\PYGZus{}values}\PYG{p}{,} \PYG{n}{indices}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}
                  \PYG{n}{i} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
                \PYG{n}{adv\PYGZus{}split} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{gather}\PYG{p}{(}\PYG{n}{advantages}\PYG{p}{,} \PYG{n}{indices}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}
                  \PYG{n}{i} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{p}{(}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
                \PYG{c+c1}{\PYGZsh{} update critic}
                \PYG{n}{cl} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{s\PYGZus{}split}\PYG{p}{,} \PYG{n}{tv\PYGZus{}split}\PYG{p}{)}
                \PYG{n}{c\PYGZus{}loss\PYGZus{}list}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{cl}\PYG{p}{)}
                \PYG{c+c1}{\PYGZsh{} update actor}
                \PYG{n}{al} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{s\PYGZus{}split}\PYG{p}{,} \PYG{n}{a\PYGZus{}split}\PYG{p}{,} \PYGZbs{}
                                       \PYG{n}{adv\PYGZus{}split}\PYG{p}{,} \PYG{n}{old\PYGZus{}pi}\PYG{p}{,} \PYG{n}{cl}\PYG{p}{)}
                \PYG{n}{a\PYGZus{}loss\PYGZus{}list}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{al}\PYG{p}{)}
                \PYG{n}{kl\PYGZus{}list}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{kl\PYGZus{}value}\PYG{p}{)}
            \PYG{c+c1}{\PYGZsh{} update lambda once in each epoch}
            \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{method} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{penalty}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{update\PYGZus{}beta}\PYG{p}{(}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} end of epoch loop}
        \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{a\PYGZus{}loss\PYGZus{}list}\PYG{p}{)}
        \PYG{n}{critic\PYGZus{}loss} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{c\PYGZus{}loss\PYGZus{}list}\PYG{p}{)}
        \PYG{n}{kld\PYGZus{}mean} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{kl\PYGZus{}list}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{actor\PYGZus{}loss}\PYG{p}{,} \PYG{n}{critic\PYGZus{}loss}\PYG{p}{,} \PYG{n}{kld\PYGZus{}mean}

    \PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}advantages}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} input/output are tensors}
        \PYG{n}{s\PYGZus{}values} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
        \PYG{n}{ns\PYGZus{}values} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} advantage should have same shape as that of values}
        \PYG{n}{adv} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{s\PYGZus{}values}\PYG{p}{)}
        \PYG{n}{returns} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros\PYGZus{}like}\PYG{p}{(}\PYG{n}{s\PYGZus{}values}\PYG{p}{)}
        \PYG{n}{discount} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma}
        \PYG{n}{lmbda} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lmbda}
        \PYG{n}{returns\PYGZus{}current} \PYG{o}{=} \PYG{n}{ns\PYGZus{}values}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{c+c1}{\PYGZsh{} last value}
        \PYG{n}{g} \PYG{o}{=} \PYG{l+m+mi}{0} \PYG{c+c1}{\PYGZsh{} GAE}
        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{gamma} \PYG{o}{=} \PYG{n}{discount} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mf}{1.} \PYG{o}{\PYGZhy{}} \PYG{n}{dones}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}
            \PYG{n}{td\PYGZus{}error} \PYG{o}{=} \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{+} \PYG{n}{gamma} \PYG{o}{*} \PYG{n}{ns\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{s\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
            \PYG{n}{g} \PYG{o}{=} \PYG{n}{td\PYGZus{}error} \PYG{o}{+} \PYG{n}{gamma} \PYG{o}{*} \PYG{n}{lmbda} \PYG{o}{*} \PYG{n}{g}
            \PYG{n}{returns\PYGZus{}current} \PYG{o}{=} \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{+} \PYG{n}{gamma} \PYG{o}{*} \PYG{n}{returns\PYGZus{}current}
            \PYG{n}{adv}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{g}
            \PYG{n}{returns}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{returns\PYGZus{}current}
        \PYG{n}{adv} \PYG{o}{=} \PYG{p}{(}\PYG{n}{adv} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{adv}\PYG{p}{)}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{adv}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}10}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{returns}\PYG{p}{,} \PYG{n}{adv}

    \PYG{n+nd}{@property}
    \PYG{k}{def} \PYG{n+nf}{penalty\PYGZus{}coefficient}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} returns penalty coefficienty}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{beta}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@47}
\PYG{k}{def} \PYG{n+nf}{collect\PYGZus{}trajectories}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{tmax}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{max\PYGZus{}steps}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{states}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{actions} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{dones} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{ep\PYGZus{}count} \PYG{o}{=} \PYG{l+m+mi}{0}        \PYG{c+c1}{\PYGZsh{} episode count}
    \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{step} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{tmax}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{step} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
        \PYG{n}{states}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
        \PYG{n}{actions}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}states}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{)}
        \PYG{n}{rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}
        \PYG{n}{dones}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{done}\PYG{p}{)}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
        \PYG{k}{if} \PYG{n}{max\PYGZus{}steps} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{step} \PYG{o}{\PYGZgt{}} \PYG{n}{max\PYGZus{}steps}\PYG{p}{:}
            \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{True}
        \PYG{k}{if} \PYG{n}{done}\PYG{p}{:}
            \PYG{n}{ep\PYGZus{}count} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
            \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{step} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{return} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{,} \PYG{n}{ep\PYGZus{}count}

 \PYG{n}{ppo\PYGZus{}train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}buffer\PYGZus{}len}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{max\PYGZus{}seasons}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}
             \PYG{n}{max\PYGZus{}steps}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Environment name: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RL Agent name:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}
    \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}
    \PYG{n}{season\PYGZus{}scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{total\PYGZus{}ep\PYGZus{}cnt} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{for} \PYG{n}{s} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}seasons}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} collect trajectories}
        \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{,} \PYG{n}{ep\PYGZus{}count} \PYG{o}{=} \PYGZbs{}
            \PYG{n}{collect\PYGZus{}trajectories}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{tmax}\PYG{o}{=}\PYG{n}{max\PYGZus{}buffer\PYGZus{}len}\PYG{p}{,} \PYGZbs{}
                                             \PYG{n}{max\PYGZus{}steps}\PYG{o}{=}\PYG{n}{max\PYGZus{}steps}\PYG{p}{)}
        \PYG{n}{total\PYGZus{}ep\PYGZus{}cnt} \PYG{o}{+}\PYG{o}{=} \PYG{p}{(}\PYG{n}{ep\PYGZus{}count}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} train the agent}
        \PYG{n}{a\PYGZus{}loss}\PYG{p}{,} \PYG{n}{c\PYGZus{}loss}\PYG{p}{,} \PYG{n}{kld\PYGZus{}value} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,}
                          \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{n}{epochs}\PYG{p}{)}
        \PYG{n}{season\PYGZus{}score} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n}{ep\PYGZus{}count} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}
        \PYG{n}{season\PYGZus{}scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{season\PYGZus{}score}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{season\PYGZus{}score} \PYG{o}{\PYGZgt{}} \PYG{n}{best\PYGZus{}score}\PYG{p}{:}
            \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{n}{season\PYGZus{}score}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{save\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{stop\PYGZus{}score} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{season\PYGZus{}score} \PYG{o}{\PYGZgt{}} \PYG{n}{stop\PYGZus{}score}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Problem is solved in }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{s}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ seasons}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{                           or }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{total\PYGZus{}ep\PYGZus{}cnt}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ episodes.}\PYG{l+s+s1}{\PYGZsq{}} \PYG{p}{)}
            \PYG{k}{break}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{season: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{s}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, episodes: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{total\PYGZus{}ep\PYGZus{}cnt}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{               season\PYGZus{}score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{season\PYGZus{}score}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{,}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{               avg\PYGZus{}ep\PYGZus{}reward: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{season\PYGZus{}scores}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{,}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{               best\PYGZus{}score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{best\PYGZus{}score}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@48}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pendulum\PYGZhy{}v1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{action\PYGZus{}ub} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{high}
\PYG{n}{action\PYGZus{}lb} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{low}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observation shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Max episodic Steps: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action space bounds: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{action\PYGZus{}ub}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{action\PYGZus{}lb}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} create a ppo agent}
\PYG{n}{ppo\PYGZus{}agent} \PYG{o}{=} \PYG{n}{PPOAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                     \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{]}\PYG{p}{,}
                     \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{n}{action\PYGZus{}ub}\PYG{p}{,}
                     \PYG{n}{entropy\PYGZus{}coeff}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,}
                     \PYG{n}{c\PYGZus{}loss\PYGZus{}coeff}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,}
                     \PYG{n}{kl\PYGZus{}target}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}
                     \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}
                     \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{lmbda}\PYG{o}{=}\PYG{l+m+mf}{0.95}\PYG{p}{,}
                    \PYG{n}{grad\PYGZus{}clip}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                    \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} train the agent}
\PYG{n}{ppo\PYGZus{}train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{ppo\PYGZus{}agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}buffer\PYGZus{}len}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{,}
      \PYG{n}{max\PYGZus{}seasons}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{epochs}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{max\PYGZus{}steps}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@49}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LunarLander\PYGZhy{}v2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{continuous}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\PYG{n}{action\PYGZus{}ub} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{high}
\PYG{n}{action\PYGZus{}lb} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{low}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{environment name: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observation shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action shape: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Max episodic Steps: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Action space bounds: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{p}{(}\PYG{n}{action\PYGZus{}ub}\PYG{p}{,} \PYG{n}{action\PYGZus{}lb}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}actor\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{s\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{512}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{512}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{a} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{tanh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{actor\PYGZus{}network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{model}

\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}critic\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{s\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{512}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{512}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{v} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{,} \PYG{n}{v}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{critic\PYGZus{}network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{model}

\PYG{n}{a\PYGZus{}model} \PYG{o}{=} \PYG{n}{create\PYGZus{}actor\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
\PYG{n}{c\PYGZus{}model} \PYG{o}{=} \PYG{n}{create\PYGZus{}critic\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}

 \PYG{n}{CFG} \PYG{o}{=} \PYG{n+nb}{dict}\PYG{p}{(}
    \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,}
    \PYG{n}{entropy\PYGZus{}coeff} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,}   \PYG{c+c1}{\PYGZsh{} required for CLIP method}
    \PYG{n}{c\PYGZus{}loss\PYGZus{}coeff} \PYG{o}{=} \PYG{l+m+mf}{0.0}\PYG{p}{,}    \PYG{c+c1}{\PYGZsh{} required for CLIP method}
    \PYG{n}{grad\PYGZus{}clip} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{,}
    \PYG{n}{method} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} choose between \PYGZsq{}clip\PYGZsq{} or \PYGZsq{}penalty\PYGZsq{}}
    \PYG{n}{kl\PYGZus{}target} \PYG{o}{=} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} required for penalty method}
    \PYG{n}{beta} \PYG{o}{=} \PYG{l+m+mf}{0.01}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} required for penalty method}
    \PYG{n}{epsilon} \PYG{o}{=} \PYG{l+m+mf}{0.3}\PYG{p}{,}  \PYG{c+c1}{\PYGZsh{} required for clip method}
    \PYG{n}{gamma} \PYG{o}{=} \PYG{l+m+mf}{0.99}\PYG{p}{,}
    \PYG{n}{lam} \PYG{o}{=} \PYG{l+m+mf}{0.95}\PYG{p}{,}     \PYG{c+c1}{\PYGZsh{} used for GAE}
    \PYG{n}{buffer\PYGZus{}capacity} \PYG{o}{=} \PYG{l+m+mi}{20000}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} next try with 50000}
    \PYG{n}{lr\PYGZus{}a} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,}
    \PYG{n}{lr\PYGZus{}c} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,}
    \PYG{n}{training\PYGZus{}epochs}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}
\PYG{p}{)}

\PYG{n}{agent} \PYG{o}{=} \PYG{n}{PPOAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                     \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{20000}\PYG{p}{,}
                     \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{n}{action\PYGZus{}ub}\PYG{p}{,}
                     \PYG{n}{entropy\PYGZus{}coeff}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,}
                     \PYG{n}{c\PYGZus{}loss\PYGZus{}coeff}\PYG{o}{=}\PYG{l+m+mf}{0.0}\PYG{p}{,}
                     \PYG{n}{kl\PYGZus{}target}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}
                     \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{beta}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,}
                     \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{n}{lmbda}\PYG{o}{=}\PYG{l+m+mf}{0.95}\PYG{p}{,}
                    \PYG{n}{grad\PYGZus{}clip}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{clip}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                    \PYG{n}{lr\PYGZus{}a}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,} \PYG{n}{lr\PYGZus{}c}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,}
                    \PYG{n}{actor\PYGZus{}model}\PYG{o}{=}\PYG{n}{a\PYGZus{}model}\PYG{p}{,}
                    \PYG{n}{critic\PYGZus{}model}\PYG{o}{=}\PYG{n}{c\PYGZus{}model}\PYG{p}{)}

\PYG{n}{ppo\PYGZus{}train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{ppo\PYGZus{}agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}buffer\PYGZus{}len}\PYG{o}{=}\PYG{n}{CFG}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{buffer\PYGZus{}capacity}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
              \PYG{n}{max\PYGZus{}seasons}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,}
             \PYG{n}{epochs}\PYG{o}{=}\PYG{n}{CFG}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{batch\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
             \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{max\PYGZus{}steps}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,}
             \PYG{n}{wandb\PYGZus{}log}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@50}
\PYG{k}{class} \PYG{n+nc}{Actor}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
	\PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}4}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
		\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
		\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}size}
		\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{lr}
		\PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
		\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
		\PYG{k}{else}\PYG{p}{:}
		\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{model}
		\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}
               \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{p}{)}


	\PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} outputs action probabilities}
		\PYG{n}{sinput} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
		\PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{512}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
		   \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{sinput}\PYG{p}{)}
		\PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{512}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
		   \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
		\PYG{n}{aout} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,}
         \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softmax}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
		   \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
		\PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{sinput}\PYG{p}{,} \PYG{n}{aout}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{actor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
		\PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
		\PYG{k}{return} \PYG{n}{model}

	\PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{)}\PYG{p}{:}
		\PYG{c+c1}{\PYGZsh{} returns action probabilities for each state}
		\PYG{n}{pi} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}\PYG{p}{)}
		\PYG{k}{return} \PYG{n}{pi}

	\PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}actor\PYGZus{}loss}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{td\PYGZus{}error}\PYG{p}{)}\PYG{p}{:}
		\PYG{n}{pi} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
		\PYG{n}{action\PYGZus{}dist} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}
            \PYG{n}{probs}\PYG{o}{=}\PYG{n}{pi}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
		\PYG{n}{log\PYGZus{}prob} \PYG{o}{=} \PYG{n}{action\PYGZus{}dist}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{)}
		\PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{log\PYGZus{}prob} \PYG{o}{*} \PYG{n}{td\PYGZus{}error}
		\PYG{k}{return} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{actor\PYGZus{}loss}\PYG{p}{)}				
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@51}
\PYG{k}{class} \PYG{n+nc}{Critic}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,}
                \PYG{n}{lr} \PYG{o}{=} \PYG{l+m+mf}{1e\PYGZhy{}4}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{lr}
        \PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{model}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{p}{)}


    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} returns V(s)}
        \PYG{n}{sinput} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{sinput}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{vout} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=} \PYG{n}{sinput}\PYG{p}{,} \PYG{n}{outputs}\PYG{o}{=}\PYG{n}{vout}\PYG{p}{,}
                                      \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{critic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} returns V(s) for each state}
        \PYG{n}{value} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{value}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@52}
\PYG{k}{class} \PYG{n+nc}{ACAgent}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                \PYG{n}{lr\PYGZus{}a}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}4}\PYG{p}{,} \PYG{n}{lr\PYGZus{}c}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}4}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,}
                \PYG{n}{a\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{c\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}a} \PYG{o}{=} \PYG{n}{lr\PYGZus{}a}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}c} \PYG{o}{=} \PYG{n}{lr\PYGZus{}c}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{actor\PYGZhy{}critic}\PYG{l+s+s1}{\PYGZsq{}}

        \PYG{c+c1}{\PYGZsh{} actor model}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor} \PYG{o}{=} \PYG{n}{Actor}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                          \PYG{n}{lr}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}a}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{a\PYGZus{}model}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} critic model}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic} \PYG{o}{=} \PYG{n}{Critic}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}c}\PYG{p}{,}
                             \PYG{n}{gamma}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{c\PYGZus{}model}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}
            \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{pi} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} action probabilities}
        \PYG{n}{action\PYGZus{}dist} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}
                              \PYG{n}{probs}\PYG{o}{=}\PYG{n}{pi}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{action\PYGZus{}dist}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{action}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}


    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}
               \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{action}\PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{action}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{reward} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}state} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}
               \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{,}
               \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{done} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{done}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape1}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape2}\PYG{p}{:}
            \PYG{n}{value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{p}{(}\PYG{n}{next\PYGZus{}state}\PYG{p}{)}
            \PYG{n}{td\PYGZus{}target} \PYG{o}{=} \PYG{n}{reward} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{next\PYGZus{}value} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{done}\PYG{p}{)}
            \PYG{n}{td\PYGZus{}error} \PYG{o}{=} \PYG{n}{td\PYGZus{}target} \PYG{o}{\PYGZhy{}} \PYG{n}{value}
            \PYG{n}{a\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{compute\PYGZus{}actor\PYGZus{}loss}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{td\PYGZus{}error}\PYG{p}{)}
            \PYG{n}{c\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{td\PYGZus{}target} \PYG{o}{\PYGZhy{}} \PYG{n}{value}\PYG{p}{)}\PYG{p}{)}

        \PYG{n}{actor\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape1}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{a\PYGZus{}loss}\PYG{p}{,}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}
        \PYG{n}{critic\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape2}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{c\PYGZus{}loss}\PYG{p}{,}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grads}\PYG{p}{,}
                              \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}grads}\PYG{p}{,}
                           \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{a\PYGZus{}loss}\PYG{p}{,} \PYG{n}{c\PYGZus{}loss}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@53}
\PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{,} \PYG{n}{log\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}
            \PYG{n}{min\PYGZus{}score}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{max\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,}
             \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{)}\PYG{p}{:}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Environment name: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{id}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RL Agent name:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}

    \PYG{k}{assert} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{p}{,} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{spaces}\PYG{o}{.}\PYG{n}{Discrete}\PYG{p}{)}\PYG{p}{,}\PYGZbs{}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{AC Agent only for discrete action spaces}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{n}{ep\PYGZus{}scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}
    \PYG{k}{for} \PYG{n}{e} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{ep\PYGZus{}score} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{a\PYGZus{}losses}\PYG{p}{,} \PYG{n}{c\PYGZus{}losses} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
            \PYG{n}{a\PYGZus{}loss}\PYG{p}{,} \PYG{n}{c\PYGZus{}loss} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,}
                                 \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}
            \PYG{n}{a\PYGZus{}losses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{a\PYGZus{}loss}\PYG{p}{)}
            \PYG{n}{c\PYGZus{}losses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{c\PYGZus{}loss}\PYG{p}{)}
            \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
            \PYG{n}{ep\PYGZus{}score} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
        \PYG{c+c1}{\PYGZsh{} while loop ends here}
        \PYG{n}{ep\PYGZus{}scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}score}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{e} \PYG{o}{\PYGZpc{}} \PYG{n}{log\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, ep\PYGZus{}score:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}score}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{,}
                     \PYG{n}{avg\PYGZus{}ep\PYGZus{}score}\PYG{p}{:}\PYG{p}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}scores}\PYG{p}{)}\PYG{p}{:}\PYG{l+m+mf}{.2}\PYG{n}{f}\PYG{p}{\PYGZcb{}}\PYG{p}{,}\PYGZbs{}
                        \PYG{n}{avg100score}\PYG{p}{:}\PYG{p}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}\PYG{l+m+mf}{.2}\PYG{n}{f}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYGZbs{}
                           \PYG{n}{best\PYGZus{}score}\PYG{p}{:}\PYG{p}{\PYGZob{}}\PYG{n}{best\PYGZus{}score}\PYG{p}{:}\PYG{l+m+mf}{.2}\PYG{n}{f}\PYG{p}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{)}

        \PYG{k}{if} \PYG{n}{ep\PYGZus{}score} \PYG{o}{\PYGZgt{}} \PYG{n}{best\PYGZus{}score}\PYG{p}{:}
            \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{n}{ep\PYGZus{}score}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{save\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Best Score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}score}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, episode: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{. Model saved.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{n}{stop\PYGZus{}score}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{The problem is solved in }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{ episodes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{e}\PYG{p}{)}\PYG{p}{)}
            \PYG{k}{break}
    \PYG{c+c1}{\PYGZsh{} for loop ends here}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@54}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{from} \PYG{n+nn}{actor\PYGZus{}critic} \PYG{k+kn}{import} \PYG{n}{ACAgent}
\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CartPole\PYGZhy{}v1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Observation shape: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action Size: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Max Episode steps: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} create an RL agent}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{ACAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} train the RL agent on}
    \PYG{n}{train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,} \PYG{n}{log\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}
             \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{499}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@55}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{from} \PYG{n+nn}{actor\PYGZus{}critic} \PYG{k+kn}{import} \PYG{n}{ACAgent}
\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LunarLander\PYGZhy{}v3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Observation shape: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action Size: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Max Episode steps: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} create an RL agent}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{ACAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} train the RL agent on}
    \PYG{n}{train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,} \PYG{n}{min\PYGZus{}score}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{log\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}
             \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@56}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow\PYGZus{}probability} \PYG{k}{as} \PYG{n+nn}{tfp}
\PYG{k}{class} \PYG{n+nc}{A2CAgent}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                 \PYG{n}{lr\PYGZus{}a}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}4}\PYG{p}{,} \PYG{n}{lr\PYGZus{}c}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,} \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,}
                 \PYG{n}{entropy\PYGZus{}beta}\PYG{o}{=}\PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{grad\PYGZus{}clip\PYGZus{}norm}\PYG{o}{=}\PYG{l+m+mf}{5.0}\PYG{p}{,}
                 \PYG{n}{a\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{c\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}a} \PYG{o}{=} \PYG{n}{lr\PYGZus{}a}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}c} \PYG{o}{=} \PYG{n}{lr\PYGZus{}c}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{entropy\PYGZus{}beta} \PYG{o}{=} \PYG{n}{entropy\PYGZus{}beta}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip\PYGZus{}norm} \PYG{o}{=} \PYG{n}{grad\PYGZus{}clip\PYGZus{}norm}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{A2C\PYGZus{}v2}\PYG{l+s+s1}{\PYGZsq{}}

        \PYG{c+c1}{\PYGZsh{} create actor and critic networks}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor} \PYG{o}{=} \PYG{n}{Actor}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}a}\PYG{p}{,}
                           \PYG{n}{model}\PYG{o}{=}\PYG{n}{a\PYGZus{}model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic} \PYG{o}{=} \PYG{n}{Critic}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{lr}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr\PYGZus{}c}\PYG{p}{,}
                             \PYG{n}{gamma}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{c\PYGZus{}model}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}
            \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{,}
               \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{pi} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
        \PYG{n}{pi\PYGZus{}np} \PYG{o}{=} \PYG{n}{pi}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{action\PYGZus{}probs} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}\PYG{n}{probs}\PYG{o}{=}\PYG{n}{pi}\PYG{p}{)}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{action\PYGZus{}probs}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{action}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}advantages}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{values}\PYG{p}{,} \PYG{n}{next\PYGZus{}values}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{advantages} \PYG{o}{=} \PYG{n}{rewards} \PYG{o}{+} \PYGZbs{}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{next\PYGZus{}values} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mf}{1.0} \PYG{o}{\PYGZhy{}} \PYG{n}{dones}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{values}
        \PYG{c+c1}{\PYGZsh{} normalize advantages}
        \PYG{n}{advantages} \PYG{o}{=} \PYG{p}{(}\PYG{n}{advantages} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{advantages}\PYG{p}{)}\PYG{p}{)}
                     \PYG{o}{/} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{std}\PYG{p}{(}\PYG{n}{advantages}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}8}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{advantages}

    \PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}actor\PYGZus{}loss}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{advantages}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{probs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
        \PYG{n}{action\PYGZus{}dist} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}
                           \PYG{n}{probs}\PYG{o}{=}\PYG{n}{probs}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{log\PYGZus{}probs} \PYG{o}{=} \PYG{n}{action\PYGZus{}dist}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{)}
        \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{log\PYGZus{}probs} \PYG{o}{*} \PYG{n}{advantages}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} entropy regularization}
        \PYG{n}{entropy} \PYG{o}{=} \PYG{n}{action\PYGZus{}dist}\PYG{o}{.}\PYG{n}{entropy}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{actor\PYGZus{}loss} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{entropy\PYGZus{}beta} \PYG{o}{*} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{entropy}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{actor\PYGZus{}loss}


    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{int32}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{dones} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{dones}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}

        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape1}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape2}\PYG{p}{:}
            \PYG{n}{values} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}values} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{)}
            \PYG{n}{advantages} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{compute\PYGZus{}advantages}\PYG{p}{(}
                        \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{values}\PYG{p}{,} \PYG{n}{next\PYGZus{}values}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{)}
            \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{compute\PYGZus{}actor\PYGZus{}loss}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,}
                                             \PYG{n}{actions}\PYG{p}{,}\PYG{n}{advantages}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{advantages}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} compute gradients}
        \PYG{n}{actor\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape1}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{actor\PYGZus{}loss}\PYG{p}{,}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}
        \PYG{n}{critic\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape2}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{critic\PYGZus{}loss}\PYG{p}{,}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} apply gradients to the models}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}
            \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}
            \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{actor\PYGZus{}loss}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{critic\PYGZus{}loss}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@57}
\PYG{k}{class} \PYG{n+nc}{A2CAgent}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{init}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{policy}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}discounted\PYGZus{}rewards}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{discounted\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{n}{sum\PYGZus{}rewards} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{k}{for} \PYG{n}{r} \PYG{o+ow}{in} \PYG{n+nb}{reversed}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{sum\PYGZus{}rewards} \PYG{o}{=} \PYG{n}{r} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{n}{sum\PYGZus{}rewards}
            \PYG{n}{discounted\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{insert}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{sum\PYGZus{}rewards}\PYG{p}{)}
        \PYG{n}{discounted\PYGZus{}rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{discounted\PYGZus{}rewards}\PYG{p}{)}
        \PYG{n}{states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{int32}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{discounted\PYGZus{}rewards}


    \PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}actor\PYGZus{}loss}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{td\PYGZus{}error}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{probs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
        \PYG{n}{action\PYGZus{}dist} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Categorical}\PYG{p}{(}
               \PYG{n}{probs}\PYG{o}{=}\PYG{n}{probs}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{log\PYGZus{}probs} \PYG{o}{=} \PYG{n}{action\PYGZus{}dist}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{)}
        \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{log\PYGZus{}probs} \PYG{o}{*} \PYG{n}{td\PYGZus{}error}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} entropy regularization}
        \PYG{n}{entropy} \PYG{o}{=} \PYG{n}{action\PYGZus{}dist}\PYG{o}{.}\PYG{n}{entropy}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{actor\PYGZus{}loss} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{entropy\PYGZus{}beta} \PYG{o}{*} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{entropy}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{actor\PYGZus{}loss}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{discnt\PYGZus{}rewards} \PYG{o}{=} \PYGZbs{}
              \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{compute\PYGZus{}discounted\PYGZus{}rewards}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{)}
        \PYG{n}{discnt\PYGZus{}rewards} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}
                           \PYG{n}{discnt\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}

        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape1}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape2}\PYG{p}{:}
            \PYG{n}{values} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
            \PYG{n}{td\PYGZus{}error} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{subtract}\PYG{p}{(}\PYG{n}{discnt\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{values}\PYG{p}{)}
            \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{compute\PYGZus{}actor\PYGZus{}loss}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,}
                                                \PYG{n}{actions}\PYG{p}{,} \PYG{n}{td\PYGZus{}error}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{td\PYGZus{}error}\PYG{p}{)}\PYG{p}{)}

        \PYG{n}{actor\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape1}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{actor\PYGZus{}loss}\PYG{p}{,}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}
        \PYG{n}{critic\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape2}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{critic\PYGZus{}loss}\PYG{p}{,}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}
                  \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}
                  \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{actor\PYGZus{}loss}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{critic\PYGZus{}loss}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@58}
\PYG{k}{def} \PYG{n+nf}{a2c\PYGZus{}train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{,} \PYG{n}{log\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,}
               \PYG{n}{max\PYGZus{}score}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{min\PYGZus{}score}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
               \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Environment name: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{id}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RL Agent name:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}

    \PYG{k}{assert} \PYG{n+nb}{isinstance}\PYG{p}{(}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{p}{,} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{spaces}\PYG{o}{.}\PYG{n}{Discrete}\PYG{p}{)}\PYG{p}{,}\PYGZbs{}
                \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{A2C Agent only for discrete action spaces}\PYG{l+s+s2}{\PYGZdq{}}

    \PYG{n}{ep\PYGZus{}scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}
    \PYG{k}{for} \PYG{n}{e} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
        \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{ep\PYGZus{}score} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
            \PYG{n}{action} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
            \PYG{n}{rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}
            \PYG{n}{states}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
            \PYG{n}{actions}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
            \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
            \PYG{n}{ep\PYGZus{}score} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}

            \PYG{k}{if} \PYG{n}{max\PYGZus{}score} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and}
                     \PYG{n}{ep\PYGZus{}score} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{max\PYGZus{}score}\PYG{p}{:}
                \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{True}
            \PYG{k}{if} \PYG{n}{min\PYGZus{}score} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and}
                     \PYG{n}{ep\PYGZus{}score} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{min\PYGZus{}score}\PYG{p}{:}
                \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{True}

            \PYG{k}{if} \PYG{n}{done}\PYG{p}{:}
                \PYG{n}{ep\PYGZus{}scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}score}\PYG{p}{)}
                \PYG{c+c1}{\PYGZsh{} train the agent}
                \PYG{n}{a\PYGZus{}loss}\PYG{p}{,} \PYG{n}{c\PYGZus{}loss} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} while loop ends here}

        \PYG{k}{if} \PYG{n}{e} \PYG{o}{\PYGZpc{}} \PYG{n}{log\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, ep\PYGZus{}score:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}score}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{,}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{              avg\PYGZus{}ep\PYGZus{}score:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}scores}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{,}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{               avg100score:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{                best\PYGZus{}score:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{best\PYGZus{}score}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{ep\PYGZus{}score} \PYG{o}{\PYGZgt{}} \PYG{n}{best\PYGZus{}score}\PYG{p}{:}
            \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{n}{ep\PYGZus{}score}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{save\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Best Score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}score}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, episode: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{. Model saved.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} for loop ends here}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@59}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{c+c1}{\PYGZsh{} create actor \PYGZam{} Critic models}
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}actor\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{s\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{a} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softmax}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{actor\PYGZus{}network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{model}

\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}critic\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{s\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{v} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{,} \PYG{n}{v}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{critic\PYGZus{}network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{model}

\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Create Gym environment}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LunarLander\PYGZhy{}v3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rgb\PYGZus{}array}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Observation shape: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action Size: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Max Episode steps: }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{p}{)}

    \PYG{n}{actor\PYGZus{}net} \PYG{o}{=} \PYG{n}{create\PYGZus{}actor\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
    \PYG{n}{critic\PYGZus{}net} \PYG{o}{=} \PYG{n}{create\PYGZus{}critic\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} create an RL agent}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{A2CAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                   \PYG{n}{a\PYGZus{}model}\PYG{o}{=}\PYG{n}{actor\PYGZus{}net}\PYG{p}{,}  \PYG{n}{c\PYGZus{}model}\PYG{o}{=}\PYG{n}{critic\PYGZus{}net}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} train the RL agent on}
    \PYG{n}{a2c\PYGZus{}train}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,} \PYG{n}{log\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,}
         \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{max\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{min\PYGZus{}score}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{300}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@60}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{multiprocessing}
\PYG{k+kn}{import} \PYG{n+nn}{time}
\PYG{k+kn}{import} \PYG{n+nn}{gc}
\PYG{k+kn}{import} \PYG{n+nn}{queue}
\PYG{k+kn}{from} \PYG{n+nn}{a2c} \PYG{k+kn}{import} \PYG{n}{A2CAgent}
\PYG{k+kn}{from} \PYG{n+nn}{collections} \PYG{k+kn}{import} \PYG{n}{deque}

\PYG{c+c1}{\PYGZsh{} Disable GPU for this script}
\PYG{n}{os}\PYG{o}{.}\PYG{n}{environ}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CUDA\PYGZus{}VISIBLE\PYGZus{}DEVICES}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}1}\PYG{l+s+s1}{\PYGZsq{}}

\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k}{as} \PYG{n+nn}{tf}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow\PYGZus{}probability} \PYG{k}{as} \PYG{n+nn}{tfp}

\PYG{c+c1}{\PYGZsh{} Worker function for each process}
\PYG{k}{def} \PYG{n+nf}{worker}\PYG{p}{(}\PYG{n}{worker\PYGZus{}id}\PYG{p}{,} \PYG{n}{global\PYGZus{}weights\PYGZus{}queue}\PYG{p}{,}
           \PYG{n}{gradients\PYGZus{}queue}\PYG{p}{,} \PYG{n}{save\PYGZus{}request\PYGZus{}queue}\PYG{p}{,} \PYG{n}{env\PYGZus{}id}\PYG{p}{,}
           \PYG{n}{create\PYGZus{}actor\PYGZus{}func}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
           \PYG{n}{create\PYGZus{}critic\PYGZus{}func}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
           \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,} \PYG{n}{max\PYGZus{}score} \PYG{o}{=} \PYG{l+m+mi}{500}\PYG{p}{,}
           \PYG{n}{min\PYGZus{}score} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{max\PYGZus{}steps} \PYG{o}{=} \PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} Set random seed for reproducibility in each process}
    \PYG{n}{tf}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{set\PYGZus{}seed}\PYG{p}{(}\PYG{n}{worker\PYGZus{}id} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{n}{worker\PYGZus{}id} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} create environment}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{n}{env\PYGZus{}id}\PYG{p}{)}
    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}

    \PYG{c+c1}{\PYGZsh{} Create local network and environment}
    \PYG{k}{if} \PYG{n}{create\PYGZus{}actor\PYGZus{}func} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{actor} \PYG{o}{=} \PYG{n}{create\PYGZus{}actor\PYGZus{}func}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{actor} \PYG{o}{=} \PYG{k+kc}{None}

    \PYG{k}{if} \PYG{n}{create\PYGZus{}critic\PYGZus{}func} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{critic} \PYG{o}{=} \PYG{n}{create\PYGZus{}critic\PYGZus{}func}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{critic} \PYG{o}{=} \PYG{k+kc}{None}
    \PYG{n}{local\PYGZus{}network} \PYG{o}{=} \PYG{n}{A2CAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                             \PYG{n}{a\PYGZus{}model}\PYG{o}{=}\PYG{n}{actor}\PYG{p}{,} \PYG{n}{c\PYGZus{}model}\PYG{o}{=}\PYG{n}{critic}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} collect experience tuple and compute gradients}
    \PYG{c+c1}{\PYGZsh{} for the local network}
    \PYG{n}{episode} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{ep\PYGZus{}scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}
    \PYG{k}{for} \PYG{n}{episode} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{max\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{weights\PYGZus{}updated} \PYG{o}{=} \PYG{k+kc}{False}
        \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} Retry up to 3 times}
            \PYG{k}{try}\PYG{p}{:}
                \PYG{n}{global\PYGZus{}weights} \PYG{o}{=} \PYG{n}{global\PYGZus{}weights\PYGZus{}queue}\PYG{o}{.}\PYG{n}{get\PYGZus{}nowait}\PYG{p}{(}\PYG{p}{)}
                \PYG{n}{local\PYGZus{}network}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}\PYG{o}{*}\PYG{n}{global\PYGZus{}weights}\PYG{p}{)}
                \PYG{n}{weights\PYGZus{}updated} \PYG{o}{=} \PYG{k+kc}{True}
                \PYG{k}{break} \PYG{c+c1}{\PYGZsh{} retrieval success}
            \PYG{k}{except} \PYG{n}{queue}\PYG{o}{.}\PYG{n}{Empty}\PYG{p}{:}
                \PYG{n}{time}\PYG{o}{.}\PYG{n}{sleep}\PYG{p}{(}\PYG{l+m+mf}{0.1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Brief pause before retry}
        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{weights\PYGZus{}updated}\PYG{p}{:}
            \PYG{k}{continue} \PYG{c+c1}{\PYGZsh{} Continue with current weights if queue is empty}

        \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{episode\PYGZus{}reward} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
        \PYG{n}{step} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{states} \PYG{o}{=} \PYG{n}{deque}\PYG{p}{(}\PYG{n}{maxlen}\PYG{o}{=}\PYG{n}{max\PYGZus{}steps} \PYGZbs{}
               \PYG{k}{if} \PYG{n}{max\PYGZus{}steps} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{k}{else} \PYG{l+m+mi}{1000}\PYG{p}{)}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{deque}\PYG{p}{(}\PYG{n}{maxlen}\PYG{o}{=}\PYG{n}{max\PYGZus{}steps} \PYGZbs{}
               \PYG{k}{if} \PYG{n}{max\PYGZus{}steps} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{k}{else} \PYG{l+m+mi}{1000}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{deque}\PYG{p}{(}\PYG{n}{maxlen}\PYG{o}{=}\PYG{n}{max\PYGZus{}steps} \PYGZbs{}
               \PYG{k}{if} \PYG{n}{max\PYGZus{}steps} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{k}{else} \PYG{l+m+mi}{1000}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Collect trajectory}
        \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done}\PYG{p}{:}
            \PYG{n}{action} \PYG{o}{=} \PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{local\PYGZus{}network}\PYG{o}{.}\PYG{n}{policy}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{truncated}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
            \PYG{n}{done} \PYG{o}{=} \PYG{n}{done} \PYG{o+ow}{or} \PYG{n}{truncated}

            \PYG{n}{states}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} type: ignore}
            \PYG{n}{actions}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
            \PYG{n}{rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}

            \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
            \PYG{n}{episode\PYGZus{}reward} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}

            \PYG{n}{step} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}

            \PYG{k}{if} \PYG{n}{max\PYGZus{}score} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{episode\PYGZus{}reward} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{max\PYGZus{}score}\PYG{p}{:}
                \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{True}
            \PYG{k}{if} \PYG{n}{min\PYGZus{}score} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{episode\PYGZus{}reward} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{min\PYGZus{}score}\PYG{p}{:}
                \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{True}
            \PYG{k}{if} \PYG{n}{max\PYGZus{}steps} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{step} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{max\PYGZus{}steps}\PYG{p}{:}
                \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{True}

        \PYG{c+c1}{\PYGZsh{} end of episode}
        \PYG{n}{ep\PYGZus{}scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{episode\PYGZus{}reward}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} update the local network}
        \PYG{n}{a\PYGZus{}loss}\PYG{p}{,} \PYG{n}{c\PYGZus{}loss}\PYG{p}{,} \PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n}{critic\PYGZus{}grads} \PYG{o}{=} \PYGZbs{}
               \PYG{n}{local\PYGZus{}network}\PYG{o}{.}\PYG{n}{compute\PYGZus{}gradients}\PYG{p}{(}
            \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}
        \PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Update global network}
        \PYG{k}{try}\PYG{p}{:}
            \PYG{n}{gradients\PYGZus{}queue}\PYG{o}{.}\PYG{n}{put\PYGZus{}nowait}\PYG{p}{(}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n}{critic\PYGZus{}grads}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{except} \PYG{n}{queue}\PYG{o}{.}\PYG{n}{Full}\PYG{p}{:}
            \PYG{k}{continue} \PYG{c+c1}{\PYGZsh{} queue full, skip updated}


        \PYG{k}{if} \PYG{n}{episode\PYGZus{}reward} \PYG{o}{\PYGZgt{}} \PYG{n}{best\PYGZus{}score}\PYG{p}{:}
            \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{n}{episode\PYGZus{}reward}

        \PYG{k}{try}\PYG{p}{:}
            \PYG{n}{save\PYGZus{}request\PYGZus{}queue}\PYG{o}{.}\PYG{n}{put\PYGZus{}nowait}\PYG{p}{(}
                  \PYG{p}{(}\PYG{n}{worker\PYGZus{}id}\PYG{p}{,} \PYG{n}{episode}\PYG{p}{,} \PYG{n}{episode\PYGZus{}reward}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{except} \PYG{n}{queue}\PYG{o}{.}\PYG{n}{Full}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} skip save request}
            \PYG{k}{continue}

        \PYG{c+c1}{\PYGZsh{} free memory}
        \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{backend}\PYG{o}{.}\PYG{n}{clear\PYGZus{}session}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Clear TensorFlow session}
        \PYG{n}{states}\PYG{o}{.}\PYG{n}{clear}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{actions}\PYG{o}{.}\PYG{n}{clear}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{rewards}\PYG{o}{.}\PYG{n}{clear}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{gc}\PYG{o}{.}\PYG{n}{collect}\PYG{p}{(}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Collect garbage to free memory}
    \PYG{c+c1}{\PYGZsh{} end of for\PYGZhy{}loop}
    \PYG{n}{env}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@61}
\PYG{k}{def} \PYG{n+nf}{run\PYGZus{}workers}\PYG{p}{(}\PYG{n}{env\PYGZus{}name}\PYG{p}{,} \PYG{n}{max\PYGZus{}num\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,}
         \PYG{n}{wandb\PYGZus{}log}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{max\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{min\PYGZus{}score}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{200}\PYG{p}{,}
         \PYG{n}{max\PYGZus{}steps}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,}
         \PYG{n}{create\PYGZus{}actor\PYGZus{}func}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
         \PYG{n}{create\PYGZus{}critic\PYGZus{}func}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Set random seed for reproducibility}
    \PYG{n}{tf}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{set\PYGZus{}seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
    \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

    \PYG{n}{N\PYGZus{}WORKERS} \PYG{o}{=} \PYG{n+nb}{min}\PYG{p}{(}\PYG{n}{multiprocessing}\PYG{o}{.}\PYG{n}{cpu\PYGZus{}count}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{max\PYGZus{}num\PYGZus{}workers}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{N\PYGZus{}WORKERS:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{N\PYGZus{}WORKERS}\PYG{p}{)}


    \PYG{c+c1}{\PYGZsh{} Initialize environment to get state and action sizes}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{n}{env\PYGZus{}name}\PYG{p}{)}

    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{n}
    \PYG{n}{env\PYGZus{}id} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{id}
    \PYG{n}{env}\PYG{o}{.}\PYG{n}{close}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{if} \PYG{n}{create\PYGZus{}actor\PYGZus{}func} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{a\PYGZus{}model} \PYG{o}{=} \PYG{n}{create\PYGZus{}actor\PYGZus{}func}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{a\PYGZus{}model} \PYG{o}{=} \PYG{k+kc}{None}
    \PYG{k}{if} \PYG{n}{create\PYGZus{}critic\PYGZus{}func} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{c\PYGZus{}model} \PYG{o}{=} \PYG{n}{create\PYGZus{}critic\PYGZus{}func}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{n}{c\PYGZus{}model} \PYG{o}{=} \PYG{k+kc}{None}

    \PYG{c+c1}{\PYGZsh{} Initialize global network}
    \PYG{n}{global\PYGZus{}network} \PYG{o}{=} \PYG{n}{A2CAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}size}\PYG{p}{,}
                              \PYG{n}{a\PYGZus{}model}\PYG{o}{=}\PYG{n}{a\PYGZus{}model}\PYG{p}{,} \PYG{n}{c\PYGZus{}model}\PYG{o}{=}\PYG{n}{c\PYGZus{}model}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Create a queue to share weights between processes}
    \PYG{n}{manager} \PYG{o}{=} \PYG{n}{multiprocessing}\PYG{o}{.}\PYG{n}{Manager}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{global\PYGZus{}weights\PYGZus{}queue} \PYG{o}{=} \PYG{n}{manager}\PYG{o}{.}\PYG{n}{Queue}\PYG{p}{(}\PYG{n}{maxsize}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{N\PYGZus{}WORKERS}\PYG{p}{)}
    \PYG{n}{gradients\PYGZus{}queue} \PYG{o}{=} \PYG{n}{manager}\PYG{o}{.}\PYG{n}{Queue}\PYG{p}{(}\PYG{n}{maxsize}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{N\PYGZus{}WORKERS}\PYG{p}{)}
    \PYG{n}{save\PYGZus{}request\PYGZus{}queue} \PYG{o}{=} \PYG{n}{manager}\PYG{o}{.}\PYG{n}{Queue}\PYG{p}{(}\PYG{n}{maxsize}\PYG{o}{=}\PYG{n}{N\PYGZus{}WORKERS}\PYG{p}{)}
    \PYG{n}{save\PYGZus{}lock} \PYG{o}{=} \PYG{n}{manager}\PYG{o}{.}\PYG{n}{Lock}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} initial weights for the global network}
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{o}{*}\PYG{n}{N\PYGZus{}WORKERS}\PYG{p}{)}\PYG{p}{:}
        \PYG{c+c1}{\PYGZsh{} Initialize queue with the global network\PYGZsq{}s weights}
        \PYG{n}{global\PYGZus{}weights\PYGZus{}queue}\PYG{o}{.}\PYG{n}{put}\PYG{p}{(}\PYG{n}{global\PYGZus{}network}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}


    \PYG{c+c1}{\PYGZsh{} Create and start worker processes}
    \PYG{n}{processes} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{N\PYGZus{}WORKERS}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{p} \PYG{o}{=} \PYG{n}{multiprocessing}\PYG{o}{.}\PYG{n}{Process}\PYG{p}{(}
            \PYG{n}{target}\PYG{o}{=}\PYG{n}{worker}\PYG{p}{,}
            \PYG{n}{args}\PYG{o}{=}\PYG{p}{(}\PYG{n}{i}\PYG{p}{,} \PYG{n}{global\PYGZus{}weights\PYGZus{}queue}\PYG{p}{,} \PYG{n}{gradients\PYGZus{}queue}\PYG{p}{,}
                  \PYG{n}{save\PYGZus{}request\PYGZus{}queue}\PYG{p}{,} \PYG{n}{env\PYGZus{}id}\PYG{p}{,}
                  \PYG{n}{create\PYGZus{}actor\PYGZus{}func}\PYG{p}{,} \PYG{n}{create\PYGZus{}critic\PYGZus{}func}\PYG{p}{,}
                  \PYG{n}{max\PYGZus{}episodes}\PYG{p}{,}
                  \PYG{n}{max\PYGZus{}score}\PYG{p}{,} \PYG{n}{min\PYGZus{}score}\PYG{p}{,} \PYG{n}{max\PYGZus{}steps}\PYG{p}{,} \PYG{n}{wandb\PYGZus{}log}\PYG{p}{)}
        \PYG{p}{)}
        \PYG{n}{p}\PYG{o}{.}\PYG{n}{start}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{processes}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{p}\PYG{p}{)}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Started worker }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{p}\PYG{o}{.}\PYG{n}{name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{try}\PYG{p}{:}
        \PYG{n}{last\PYGZus{}refill\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{best\PYGZus{}reward} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}
        \PYG{k}{while}\PYG{p}{(}\PYG{n+nb}{any}\PYG{p}{(}\PYG{n}{p}\PYG{o}{.}\PYG{n}{is\PYGZus{}alive}\PYG{p}{(}\PYG{p}{)} \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{processes}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{gradients} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
            \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{N\PYGZus{}WORKERS}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{gradients\PYGZus{}queue}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                    \PYG{n}{gradients}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{gradients\PYGZus{}queue}\PYG{o}{.}\PYG{n}{get}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

            \PYG{k}{if} \PYG{n}{gradients}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} process gradients if available}
                \PYG{c+c1}{\PYGZsh{} Filter out None gradients}
                \PYG{n}{valid\PYGZus{}gradients} \PYG{o}{=} \PYG{p}{[}
                    \PYG{p}{(}\PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n}{critic\PYGZus{}grads}\PYG{p}{)}
                    \PYG{k}{for} \PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n}{critic\PYGZus{}grads} \PYG{o+ow}{in} \PYG{n}{gradients}
                    \PYG{k}{if} \PYG{n}{actor\PYGZus{}grads} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{o+ow}{and} \PYG{n}{critic\PYGZus{}grads} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}
                \PYG{p}{]}
                \PYG{c+c1}{\PYGZsh{} Only proceed if there are valid gradients}
                \PYG{k}{if} \PYG{n}{valid\PYGZus{}gradients}\PYG{p}{:}
                    \PYG{n}{actor\PYGZus{}grads\PYGZus{}avg} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
                    \PYG{n}{critic\PYGZus{}grads\PYGZus{}avg} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
                    \PYG{k}{for} \PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n}{critic\PYGZus{}grads} \PYG{o+ow}{in} \PYG{n}{valid\PYGZus{}gradients}\PYG{p}{:}
                        \PYG{k}{if} \PYG{o+ow}{not} \PYG{n}{actor\PYGZus{}grads\PYGZus{}avg}\PYG{p}{:}
                            \PYG{n}{actor\PYGZus{}grads\PYGZus{}avg} \PYG{o}{=} \PYG{p}{[}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{g}\PYG{p}{)}\PYGZbs{}
                                    \PYG{k}{for} \PYG{n}{g} \PYG{o+ow}{in} \PYG{n}{actor\PYGZus{}grads}\PYG{p}{]}
                            \PYG{n}{critic\PYGZus{}grads\PYGZus{}avg} \PYG{o}{=} \PYG{p}{[}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{g}\PYG{p}{)}\PYGZbs{}
                                    \PYG{k}{for} \PYG{n}{g} \PYG{o+ow}{in} \PYG{n}{critic\PYGZus{}grads}\PYG{p}{]}
                        \PYG{k}{else}\PYG{p}{:}
                            \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{g} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grads}\PYG{p}{)}\PYG{p}{:}
                                \PYG{n}{actor\PYGZus{}grads\PYGZus{}avg}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}
                                    \PYG{n}{actor\PYGZus{}grads\PYGZus{}avg}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{g}\PYG{p}{)}\PYG{p}{)}
                            \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{g} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{critic\PYGZus{}grads}\PYG{p}{)}\PYG{p}{:}
                                \PYG{n}{critic\PYGZus{}grads\PYGZus{}avg}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}
                                    \PYG{n}{critic\PYGZus{}grads\PYGZus{}avg}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{g}\PYG{p}{)}\PYG{p}{)}

                    \PYG{n}{n} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{valid\PYGZus{}gradients}\PYG{p}{)}
                    \PYG{n}{actor\PYGZus{}grads\PYGZus{}avg} \PYG{o}{=} \PYG{p}{[}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{truediv}\PYG{p}{(}\PYG{n}{g}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)} \PYGZbs{}
                                       \PYG{k}{for} \PYG{n}{g} \PYG{o+ow}{in} \PYG{n}{actor\PYGZus{}grads\PYGZus{}avg}\PYG{p}{]}
                    \PYG{n}{critic\PYGZus{}grads\PYGZus{}avg} \PYG{o}{=} \PYG{p}{[}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{truediv}\PYG{p}{(}\PYG{n}{g}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)} \PYGZbs{}
                                       \PYG{k}{for} \PYG{n}{g} \PYG{o+ow}{in} \PYG{n}{critic\PYGZus{}grads\PYGZus{}avg}\PYG{p}{]}

                    \PYG{n}{global\PYGZus{}network}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grads\PYGZus{}avg}\PYG{p}{,}
                                                \PYG{n}{critic\PYGZus{}grads\PYGZus{}avg}\PYG{p}{)}

                \PYG{c+c1}{\PYGZsh{} Synchronize global weights with all workers}
                \PYG{n}{updated\PYGZus{}weights} \PYG{o}{=} \PYG{n}{global\PYGZus{}network}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}
                \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{N\PYGZus{}WORKERS}\PYG{p}{)}\PYG{p}{:}
                    \PYG{k}{try}\PYG{p}{:}
                        \PYG{c+c1}{\PYGZsh{} Synchronize global weights with all workers}
                        \PYG{n}{global\PYGZus{}weights\PYGZus{}queue}\PYG{o}{.}\PYG{n}{put\PYGZus{}nowait}\PYG{p}{(}\PYG{n}{updated\PYGZus{}weights}\PYG{p}{)}
                    \PYG{k}{except} \PYG{n}{queue}\PYG{o}{.}\PYG{n}{Full}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} skip synchronization}
                        \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{global\PYGZus{}weights\PYGZus{}queue}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                            \PYG{k}{try}\PYG{p}{:}
                                \PYG{n}{global\PYGZus{}weights\PYGZus{}queue}\PYG{o}{.}\PYG{n}{get\PYGZus{}nowait}\PYG{p}{(}\PYG{p}{)}
                            \PYG{k}{except} \PYG{n}{queue}\PYG{o}{.}\PYG{n}{Empty}\PYG{p}{:}
                                \PYG{k}{break}
                        \PYG{n}{global\PYGZus{}weights\PYGZus{}queue}\PYG{o}{.}\PYG{n}{put\PYGZus{}nowait}\PYG{p}{(}\PYG{n}{updated\PYGZus{}weights}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} periodically refill the global weights queue}
            \PYG{k}{if} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{last\PYGZus{}refill\PYGZus{}time} \PYG{o}{\PYGZgt{}} \PYG{l+m+mi}{5}\PYG{p}{:}
                \PYG{k}{try}\PYG{p}{:}
                    \PYG{n}{global\PYGZus{}weights\PYGZus{}queue}\PYG{o}{.}\PYG{n}{put\PYGZus{}nowait}\PYG{p}{(}
                              \PYG{n}{global\PYGZus{}network}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
                    \PYG{n}{last\PYGZus{}refill\PYGZus{}time} \PYG{o}{=} \PYG{n}{time}\PYG{o}{.}\PYG{n}{time}\PYG{p}{(}\PYG{p}{)}
                \PYG{k}{except} \PYG{n}{queue}\PYG{o}{.}\PYG{n}{Full}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} skip refill}
                    \PYG{k}{continue}

            \PYG{c+c1}{\PYGZsh{} saving weights}
            \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{save\PYGZus{}request\PYGZus{}queue}\PYG{o}{.}\PYG{n}{empty}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
                \PYG{k}{try}\PYG{p}{:}
                    \PYG{n}{worker\PYGZus{}id}\PYG{p}{,} \PYG{n}{episode}\PYG{p}{,} \PYG{n}{episode\PYGZus{}reward} \PYG{o}{=} \PYGZbs{}
                              \PYG{n}{save\PYGZus{}request\PYGZus{}queue}\PYG{o}{.}\PYG{n}{get\PYGZus{}nowait}\PYG{p}{(}\PYG{p}{)}
                    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Worker: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{worker\PYGZus{}id}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{,}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                           episode: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{episode}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{,  }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                           reward: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{episode\PYGZus{}reward}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
                    \PYG{k}{if} \PYG{n}{episode\PYGZus{}reward} \PYG{o}{\PYGZgt{}} \PYG{n}{best\PYGZus{}reward}\PYG{p}{:}
                        \PYG{n}{best\PYGZus{}reward} \PYG{o}{=} \PYG{n}{episode\PYGZus{}reward}
                        \PYG{k}{with} \PYG{n}{save\PYGZus{}lock}\PYG{p}{:}
                            \PYG{k}{try}\PYG{p}{:}
                                \PYG{n}{global\PYGZus{}network}\PYG{o}{.}\PYG{n}{save\PYGZus{}weights}\PYG{p}{(}
                                    \PYG{n}{actor\PYGZus{}wt\PYGZus{}file}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{a3c\PYGZus{}actor.weights.h5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                    \PYG{n}{critic\PYGZus{}wt\PYGZus{}file}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{a3c\PYGZus{}critic.weights.h5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                                \PYG{p}{)}
                                \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Best Score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{best\PYGZus{}reward}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{. }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                                                Saved weights!}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
                            \PYG{k}{except} \PYG{n+ne}{Exception} \PYG{k}{as} \PYG{n}{e}\PYG{p}{:}
                                \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Error saving weights: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
                \PYG{k}{except} \PYG{n}{queue}\PYG{o}{.}\PYG{n}{Empty}\PYG{p}{:}
                    \PYG{k}{break}
    \PYG{k}{except} \PYG{n+ne}{Exception} \PYG{k}{as} \PYG{n}{e}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{An error occurred: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{k}{finally}\PYG{p}{:}
        \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Shutting down workers...}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{}Wait for all processes to complete}
        \PYG{k}{for} \PYG{n}{p} \PYG{o+ow}{in} \PYG{n}{processes}\PYG{p}{:}
            \PYG{n}{p}\PYG{o}{.}\PYG{n}{join}\PYG{p}{(}\PYG{p}{)}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Worker }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{p}\PYG{o}{.}\PYG{n}{name}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ has finished.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@62}
\PYG{k}{class} \PYG{n+nc}{A2CAgent}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{discnt\PYGZus{}rewards} \PYG{o}{=} \PYGZbs{}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{compute\PYGZus{}discounted\PYGZus{}rewards}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{)}

        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape1}\PYG{p}{,} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape2}\PYG{p}{:}
            \PYG{n}{values} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
            \PYG{n}{td\PYGZus{}error} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{subtract}\PYG{p}{(}\PYG{n}{discnt\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{values}\PYG{p}{)}
            \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{compute\PYGZus{}actor\PYGZus{}loss}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,}\PYGZbs{}
                                       \PYG{n}{actions}\PYG{p}{,} \PYG{n}{td\PYGZus{}error}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{td\PYGZus{}error}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} compute gradients}
        \PYG{n}{actor\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape1}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{actor\PYGZus{}loss}\PYG{p}{,}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}
        \PYG{n}{critic\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape2}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{critic\PYGZus{}loss}\PYG{p}{,}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} clip gradients}
        \PYG{n}{actor\PYGZus{}grads} \PYG{o}{=} \PYG{p}{[}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}norm}\PYG{p}{(}\PYG{n}{grad}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip\PYGZus{}norm}\PYG{p}{)} \PYGZbs{}
                  \PYG{k}{if} \PYG{n}{grad} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{k}{else} \PYG{k+kc}{None} \PYG{k}{for} \PYG{n}{grad} \PYG{o+ow}{in} \PYG{n}{actor\PYGZus{}grads} \PYG{p}{]}
        \PYG{n}{critic\PYGZus{}grads} \PYG{o}{=} \PYG{p}{[}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}norm}\PYG{p}{(}\PYG{n}{grad}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{grad\PYGZus{}clip\PYGZus{}norm}\PYG{p}{)} \PYGZbs{}
                  \PYG{k}{if} \PYG{n}{grad} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None} \PYG{k}{else} \PYG{k+kc}{None} \PYG{k}{for} \PYG{n}{grad} \PYG{o+ow}{in} \PYG{n}{critic\PYGZus{}grads}\PYG{p}{]}

        \PYG{c+c1}{\PYGZsh{} Check for NaN gradients}
        \PYG{n}{actor\PYGZus{}has\PYGZus{}nan} \PYG{o}{=} \PYG{n+nb}{any}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}any}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{is\PYGZus{}nan}\PYG{p}{(}\PYG{n}{grad}\PYG{p}{)}\PYG{p}{)} \PYGZbs{}
                        \PYG{k}{for} \PYG{n}{grad} \PYG{o+ow}{in} \PYG{n}{actor\PYGZus{}grads} \PYG{k}{if} \PYG{n}{grad} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{)}
        \PYG{n}{critic\PYGZus{}has\PYGZus{}nan} \PYG{o}{=} \PYG{n+nb}{any}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}any}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{is\PYGZus{}nan}\PYG{p}{(}\PYG{n}{grad}\PYG{p}{)}\PYG{p}{)} \PYGZbs{}
                        \PYG{k}{for} \PYG{n}{grad} \PYG{o+ow}{in} \PYG{n}{critic\PYGZus{}grads} \PYG{k}{if} \PYG{n}{grad} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{)}


        \PYG{k}{if} \PYG{n}{actor\PYGZus{}has\PYGZus{}nan} \PYG{o+ow}{or} \PYG{n}{critic\PYGZus{}has\PYGZus{}nan}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{NaN gradients detected! }\PYG{l+s+s2}{\PYGZdq{}}
                  \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Actor NaN: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{actor\PYGZus{}has\PYGZus{}nan}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s2}{                           Critic NaN: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{critic\PYGZus{}has\PYGZus{}nan}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, }\PYG{l+s+s2}{\PYGZdq{}}
            \PYG{c+c1}{\PYGZsh{} Skip sending gradients to avoid corrupting global network}
            \PYG{k}{return} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}\PYG{p}{,} \PYG{k+kc}{None}

        \PYG{k}{return} \PYG{n}{actor\PYGZus{}loss}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{critic\PYGZus{}loss}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYGZbs{}
               \PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n}{critic\PYGZus{}grads}

    \PYG{k}{def} \PYG{n+nf}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n}{critic\PYGZus{}grads}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}
            \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}
            \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{get\PYGZus{}weights}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYGZbs{}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{set\PYGZus{}weights}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{actor\PYGZus{}weights}\PYG{p}{,} \PYG{n}{critic\PYGZus{}weights}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}\PYG{n}{actor\PYGZus{}weights}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}\PYG{n}{critic\PYGZus{}weights}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@63}
\PYG{k+kn}{from} \PYG{n+nn}{a3c} \PYG{k+kn}{import} \PYG{n}{run\PYGZus{}workers}
\PYG{k+kn}{import} \PYG{n+nn}{multiprocessing}
\PYG{k+kn}{import} \PYG{n+nn}{tensorflow} \PYG{k}{as} \PYG{n+nn}{tf}

\PYG{c+c1}{\PYGZsh{} create actor \PYGZam{} Critic models}
\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}actor\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{n\PYGZus{}actions}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{s\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{a} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n}{n\PYGZus{}actions}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{softmax}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{actor\PYGZus{}network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{model}

\PYG{k}{def} \PYG{n+nf}{create\PYGZus{}critic\PYGZus{}model}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{s\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{)}
    \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{128}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{v} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{s\PYGZus{}input}\PYG{p}{,} \PYG{n}{v}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{critic\PYGZus{}network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{model}

\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:}
    \PYG{n}{multiprocessing}\PYG{o}{.}\PYG{n}{set\PYGZus{}start\PYGZus{}method}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spawn}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{run\PYGZus{}workers}\PYG{p}{(}
        \PYG{n}{env\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LunarLander\PYGZhy{}v3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
        \PYG{n}{max\PYGZus{}num\PYGZus{}workers}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{,}
        \PYG{n}{max\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,}
        \PYG{n}{max\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,}
        \PYG{n}{min\PYGZus{}score}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{200}\PYG{p}{,}
        \PYG{n}{max\PYGZus{}steps}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,}
        \PYG{n}{create\PYGZus{}actor\PYGZus{}func}\PYG{o}{=}\PYG{n}{create\PYGZus{}actor\PYGZus{}model}\PYG{p}{,}
        \PYG{n}{create\PYGZus{}critic\PYGZus{}func}\PYG{o}{=}\PYG{n}{create\PYGZus{}critic\PYGZus{}model}\PYG{p}{,}
    \PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@64}
\PYG{k}{class} \PYG{n+nc}{Actor}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{0.0001}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{learning\PYGZus{}rate}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}action} \PYG{o}{=} \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}

        \PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{clone\PYGZus{}model}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Constraints for log\PYGZus{}std to ensure numerical stability}
        \PYG{c+c1}{\PYGZsh{} and reasonable exploration}
        \PYG{c+c1}{\PYGZsh{} log\PYGZus{}std values are usually clipped to a range like [\PYGZhy{}20, 2]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}log\PYGZus{}std} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{constant}\PYG{p}{(}\PYG{l+m+mf}{2.0}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{min\PYGZus{}log\PYGZus{}std} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{constant}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{20.0}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}model}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{x} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{f} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}
        \PYG{n}{f} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}
        \PYG{n}{mu} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}
        \PYG{n}{log\PYGZus{}std} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{f}\PYG{p}{)}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,}
               \PYG{n}{outputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{log\PYGZus{}std}\PYG{p}{]}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{actor}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{mean}\PYG{p}{,} \PYG{n}{log\PYGZus{}std} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
        \PYG{n}{log\PYGZus{}std} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}value}\PYG{p}{(}\PYG{n}{log\PYGZus{}std}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{min\PYGZus{}log\PYGZus{}std}\PYG{p}{,}
                                       \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}log\PYGZus{}std}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{mean}\PYG{p}{,} \PYG{n}{log\PYGZus{}std}

    \PYG{k}{def} \PYG{n+nf}{sample\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{reparameterize}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{mean}\PYG{p}{,} \PYG{n}{log\PYGZus{}std} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}
        \PYG{n}{std} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n}{log\PYGZus{}std}\PYG{p}{)}
        \PYG{n}{dist} \PYG{o}{=} \PYG{n}{tfp}\PYG{o}{.}\PYG{n}{distributions}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{n}{mean}\PYG{p}{,} \PYG{n}{std}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{reparameterize}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Reparameterization trick}
            \PYG{n}{z} \PYG{o}{=} \PYG{n}{mean} \PYG{o}{+} \PYG{n}{std} \PYG{o}{*} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{(}\PYG{n}{mean}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{z} \PYG{o}{=} \PYG{n}{dist}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}

        \PYG{n}{action} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{tanh}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)} \PYG{o}{*} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}action}
        \PYG{n}{squash} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{pow}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{tanh}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} squash values to (0,1)}
        \PYG{n}{squash} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}value}\PYG{p}{(}\PYG{n}{squash}\PYG{p}{,} \PYG{l+m+mf}{1e\PYGZhy{}6}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Avoid division by zero}
        \PYG{n}{log\PYGZus{}prob} \PYG{o}{=} \PYG{n}{dist}\PYG{o}{.}\PYG{n}{log\PYGZus{}prob}\PYG{p}{(}\PYG{n}{z}\PYG{p}{)}
        \PYG{n}{log\PYGZus{}prob} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{squash}\PYG{p}{)}
        \PYG{n}{log\PYGZus{}prob} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}sum}\PYG{p}{(}\PYG{n}{log\PYGZus{}prob}\PYG{p}{,}  \PYG{n}{axis}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{keepdims}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{action}\PYG{p}{,} \PYG{n}{log\PYGZus{}prob}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@65}
\PYG{k}{class} \PYG{n+nc}{Critic}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Approximates Q(s,a) function \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,}
                 \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}    \PYG{c+c1}{\PYGZsh{} shape: (m,)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{action\PYGZus{}shape}  \PYG{c+c1}{\PYGZsh{} shape: (n, )}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{learning\PYGZus{}rate}
        \PYG{c+c1}{\PYGZsh{} create NN model}
        \PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}net}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{clone\PYGZus{}model}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}net}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{state\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{action\PYGZus{}input} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{concat} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Concatenate}\PYG{p}{(}\PYG{p}{)}\PYG{p}{(}\PYG{p}{[}\PYG{n}{state\PYGZus{}input}\PYG{p}{,} \PYG{n}{action\PYGZus{}input}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
           \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{concat}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
           \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
           \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{n}{net\PYGZus{}out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{p}{[}\PYG{n}{state\PYGZus{}input}\PYG{p}{,} \PYG{n}{action\PYGZus{}input}\PYG{p}{]}\PYG{p}{,}
            \PYG{n}{outputs}\PYG{o}{=}\PYG{n}{net\PYGZus{}out}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{critic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Returns Q(s,a) value \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{return} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{p}{[}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{]}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@66}
\PYG{k}{class} \PYG{n+nc}{SACAgent}\PYG{p}{:}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                 \PYG{n}{lr\PYGZus{}a}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}4}\PYG{p}{,} \PYG{n}{lr\PYGZus{}c}\PYG{o}{=}\PYG{l+m+mf}{3e\PYGZhy{}4}\PYG{p}{,}
                 \PYG{n}{lr\PYGZus{}alpha}\PYG{o}{=}\PYG{l+m+mf}{3e\PYGZhy{}4}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,}
                 \PYG{n}{gamma}\PYG{o}{=}\PYG{l+m+mf}{0.99}\PYG{p}{,} \PYG{n}{polyak}\PYG{o}{=}\PYG{l+m+mf}{0.999}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                 \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{1000000}\PYG{p}{,}
                 \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{256}\PYG{p}{,}
                 \PYG{n}{reward\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                 \PYG{n}{max\PYGZus{}grad\PYGZus{}norm}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{c+c1}{\PYGZsh{} required for gradient clipping}
                 \PYG{n}{actor\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                 \PYG{n}{critic\PYGZus{}model}\PYG{o}{=}\PYG{k+kc}{None} \PYG{p}{)}\PYG{p}{:}

        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{action\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{=} \PYG{n}{gamma}  \PYG{c+c1}{\PYGZsh{} Discount factor}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{polyak} \PYG{o}{=} \PYG{n}{polyak} \PYG{c+c1}{\PYGZsh{} Polyak averaging coefficient}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n}{batch\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer\PYGZus{}size} \PYG{o}{=} \PYG{n}{buffer\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor\PYGZus{}lr} \PYG{o}{=} \PYG{n}{lr\PYGZus{}a}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr} \PYG{o}{=} \PYG{n}{lr\PYGZus{}c}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha\PYGZus{}lr} \PYG{o}{=} \PYG{n}{lr\PYGZus{}alpha}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{reward\PYGZus{}scale} \PYG{o}{=} \PYG{n}{reward\PYGZus{}scale}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm} \PYG{o}{=} \PYG{n}{max\PYGZus{}grad\PYGZus{}norm}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SAC}\PYG{l+s+s1}{\PYGZsq{}}

        \PYG{c+c1}{\PYGZsh{} Initialize actor and critic networks}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor} \PYG{o}{=} \PYG{n}{Actor}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor\PYGZus{}lr}\PYG{p}{,}
                           \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{actor\PYGZus{}model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1} \PYG{o}{=} \PYG{n}{Critic}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr}\PYG{p}{,}
                                                   \PYG{n}{model}\PYG{o}{=}\PYG{n}{critic\PYGZus{}model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2} \PYG{o}{=} \PYG{n}{Critic}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr}\PYG{p}{,}
                                                \PYG{n}{model}\PYG{o}{=}\PYG{n}{critic\PYGZus{}model}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Target networks for soft updates}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}critic\PYGZus{}1} \PYG{o}{=} \PYG{n}{Critic}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                           \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{critic\PYGZus{}model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}critic\PYGZus{}2} \PYG{o}{=} \PYG{n}{Critic}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                           \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{critic\PYGZus{}model}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} make alpha a trainable variable}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}alpha} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{alpha}\PYG{p}{)}\PYG{p}{,}
            \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{,} \PYG{n}{trainable}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log\PYGZus{}alpha}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha\PYGZus{}optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}
                                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha\PYGZus{}lr}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Target entropy for action space}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}entropy} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{constant}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{prod}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}shape}\PYG{p}{)}\PYG{p}{,}
                                                   \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Replay buffer}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer} \PYG{o}{=} \PYG{n}{ReplayBuffer}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer\PYGZus{}size}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Initialize target networks with the same weights as the main networks}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}
                     \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}target\PYGZus{}networks}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Soft update target networks using Polyak averaging}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{for} \PYG{n}{target\PYGZus{}var}\PYG{p}{,} \PYG{n}{var} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{,}
                                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{target\PYGZus{}var}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{polyak} \PYG{o}{*} \PYG{n}{target\PYGZus{}var} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{polyak}\PYG{p}{)} \PYG{o}{*} \PYG{n}{var}\PYG{p}{)}

        \PYG{k}{for} \PYG{n}{target\PYGZus{}var}\PYG{p}{,} \PYG{n}{var} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{,}
                                \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{target\PYGZus{}var}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{polyak} \PYG{o}{*} \PYG{n}{target\PYGZus{}var} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{polyak}\PYG{p}{)} \PYG{o}{*} \PYG{n}{var}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{choose\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{evaluate}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Choose an action based on the current state}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{expand\PYGZus{}dims}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
        \PYG{n}{action}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{sample\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{reparameterize}\PYG{o}{=}\PYG{o+ow}{not} \PYG{n}{evaluate}\PYG{p}{)}
        \PYG{n}{action} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{squeeze}\PYG{p}{(}\PYG{n}{action}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Remove batch dimension}
        \PYG{k}{return} \PYG{n}{action}\PYG{o}{.}\PYG{n}{numpy}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{store\PYGZus{}transition}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Store a transition in the replay buffer}
\PYG{l+s+sd}{        : inputs are numpy arrays}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{o}{.}\PYG{n}{add}\PYG{p}{(}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}critic}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Update the critic networks using the sampled transitions}
\PYG{l+s+sd}{        : inputs are tensors}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}

        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{n}{persistent}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} predict next\PYGZus{}actions and log\PYGZus{}probs for next states}
            \PYG{n}{next\PYGZus{}actions}\PYG{p}{,} \PYG{n}{next\PYGZus{}log\PYGZus{}probs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{sample\PYGZus{}action}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} compute target Q\PYGZhy{}values using the target critic networks}
            \PYG{n}{target\PYGZus{}q1} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}critic\PYGZus{}1}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{next\PYGZus{}actions}\PYG{p}{)}
            \PYG{n}{target\PYGZus{}q2} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}critic\PYGZus{}2}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{next\PYGZus{}actions}\PYG{p}{)}
            \PYG{n}{min\PYGZus{}target\PYGZus{}q\PYGZus{}next} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{minimum}\PYG{p}{(}\PYG{n}{target\PYGZus{}q1}\PYG{p}{,} \PYG{n}{target\PYGZus{}q2}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Soft Bellman backup equation for target Q\PYGZhy{}value}
            \PYG{c+c1}{\PYGZsh{} y = r + gamma * (1 \PYGZhy{} done) * (min\PYGZus{}Q\PYGZus{}target(s\PYGZsq{}, a\PYGZsq{}) \PYGZhy{} alpha * log\PYGZus{}pi(a\PYGZsq{}|s\PYGZsq{}))}
            \PYG{n}{target\PYGZus{}q\PYGZus{}values} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{reward\PYGZus{}scale} \PYG{o}{*} \PYG{n}{rewards} \PYG{o}{+} \PYGZbs{}
                  \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{ones\PYGZus{}like}\PYG{p}{(}\PYG{n}{dones}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{dones}\PYG{p}{)} \PYG{o}{*} \PYGZbs{}
                  \PYG{p}{(}\PYG{n}{min\PYGZus{}target\PYGZus{}q\PYGZus{}next} \PYG{o}{\PYGZhy{}} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}alpha}\PYG{p}{)} \PYG{o}{*} \PYG{n}{next\PYGZus{}log\PYGZus{}probs}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} compute current Q\PYGZhy{}values}
            \PYG{n}{current\PYGZus{}q1} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{)}
            \PYG{n}{current\PYGZus{}q2} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} compute critic losses}
            \PYG{n}{critic\PYGZus{}1\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{current\PYGZus{}q1} \PYG{o}{\PYGZhy{}} \PYG{n}{target\PYGZus{}q\PYGZus{}values}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}2\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{current\PYGZus{}q2} \PYG{o}{\PYGZhy{}} \PYG{n}{target\PYGZus{}q\PYGZus{}values}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} compute gradients for critic networks}
        \PYG{n}{critic\PYGZus{}1\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{critic\PYGZus{}1\PYGZus{}loss}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}
        \PYG{n}{critic\PYGZus{}2\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{critic\PYGZus{}2\PYGZus{}loss}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}

        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Clip gradients to avoid exploding gradients}
            \PYG{n}{critic\PYGZus{}1\PYGZus{}grads}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}global\PYGZus{}norm}\PYG{p}{(}\PYG{n}{critic\PYGZus{}1\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}2\PYGZus{}grads}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}global\PYGZus{}norm}\PYG{p}{(}\PYG{n}{critic\PYGZus{}2\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} apply gradients to the critic networks if gradients are not None}
        \PYG{k}{if} \PYG{n}{critic\PYGZus{}1\PYGZus{}grads} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}1\PYGZus{}grads}\PYG{p}{,}
                                            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{critic\PYGZus{}2\PYGZus{}grads} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}2\PYGZus{}grads}\PYG{p}{,}
                                            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}

        \PYG{n}{mean\PYGZus{}c\PYGZus{}loss} \PYG{o}{=} \PYG{p}{(}\PYG{n}{critic\PYGZus{}1\PYGZus{}loss} \PYG{o}{+} \PYG{n}{critic\PYGZus{}2\PYGZus{}loss}\PYG{p}{)} \PYG{o}{/} \PYG{l+m+mf}{2.0}
        \PYG{k}{return} \PYG{n}{mean\PYGZus{}c\PYGZus{}loss}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}actor}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Update the actor network}
\PYG{l+s+sd}{        inputs are tensors}
\PYG{l+s+sd}{        outputs: actor\PYGZus{}loss and alpha\PYGZus{}loss}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}

        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{n}{persistent}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Sample actions and log probabilities for current states}
            \PYG{n}{new\PYGZus{}actions}\PYG{p}{,} \PYG{n}{log\PYGZus{}probs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{sample\PYGZus{}action}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Compute Q\PYGZhy{}values for the sampled actions}
            \PYG{n}{q1\PYGZus{}new} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{new\PYGZus{}actions}\PYG{p}{)}
            \PYG{n}{q2\PYGZus{}new} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{new\PYGZus{}actions}\PYG{p}{)}
            \PYG{n}{min\PYGZus{}q} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{minimum}\PYG{p}{(}\PYG{n}{q1\PYGZus{}new}\PYG{p}{,} \PYG{n}{q2\PYGZus{}new}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Actor loss is the mean of the Q\PYGZhy{}values minus the entropy term}
            \PYG{c+c1}{\PYGZsh{} Actor loss (maximize soft Q\PYGZhy{}value, incorporating entropy)}
            \PYG{c+c1}{\PYGZsh{} J\PYGZus{}pi = E\PYGZus{}s,a\PYGZti{}pi [alpha * log\PYGZus{}pi(a|s) \PYGZhy{} Q(s,a)] \PYGZhy{}\PYGZgt{} minimize \PYGZhy{}J\PYGZus{}pi}
            \PYG{n}{actor\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}alpha}\PYG{p}{)} \PYG{o}{*} \PYG{n}{log\PYGZus{}probs} \PYG{o}{\PYGZhy{}} \PYG{n}{min\PYGZus{}q}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} alpha loss is computed as the mean of the target entropy minus the log probability}
            \PYG{n}{alpha\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{negative}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}alpha}\PYG{p}{)} \PYG{o}{*} \PYGZbs{}
                                          \PYG{p}{(}\PYG{n}{log\PYGZus{}probs} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}entropy}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Compute gradients for the actor network}
        \PYG{n}{actor\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{actor\PYGZus{}loss}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Compute gradients for alpha}
        \PYG{n}{alpha\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{alpha\PYGZus{}loss}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}alpha}\PYG{p}{]}\PYG{p}{)}

        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Clip gradients to avoid exploding gradients}
            \PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}global\PYGZus{}norm}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm}\PYG{p}{)}
            \PYG{n}{alpha\PYGZus{}grads}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}global\PYGZus{}norm}\PYG{p}{(}\PYG{n}{alpha\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Apply gradients to the actor network if gradients are not None}
        \PYG{k}{if} \PYG{n}{actor\PYGZus{}grads} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{actor\PYGZus{}grads}\PYG{p}{,}
                                            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Apply gradients to alpha if gradients are not None}
        \PYG{k}{if} \PYG{n}{alpha\PYGZus{}grads} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha\PYGZus{}optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{alpha\PYGZus{}grads}\PYG{p}{,} \PYG{p}{[}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}alpha}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{actor\PYGZus{}loss}\PYG{p}{,} \PYG{n}{alpha\PYGZus{}loss}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{update\PYGZus{}per\PYGZus{}step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{:}
            \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}

        \PYG{n}{c\PYGZus{}losses}\PYG{p}{,} \PYG{n}{a\PYGZus{}losses}\PYG{p}{,} \PYG{n}{alpha\PYGZus{}losses} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{update\PYGZus{}per\PYGZus{}step}\PYG{p}{)}\PYG{p}{:}

            \PYG{c+c1}{\PYGZsh{} Sample a batch of transitions from the replay buffer}
            \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones} \PYG{o}{=} \PYGZbs{}
                  \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{o}{.}\PYG{n}{sample\PYGZus{}unpacked}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}

            \PYG{n}{states} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
            \PYG{n}{actions} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
            \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}states} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
            \PYG{n}{dones} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{dones}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Update Critic networks}
            \PYG{n}{critic\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}critic}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{)}
            \PYG{n}{actor\PYGZus{}loss}\PYG{p}{,} \PYG{n}{alpha\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}actor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Update target networks}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}target\PYGZus{}networks}\PYG{p}{(}\PYG{p}{)}

            \PYG{n}{c\PYGZus{}losses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{critic\PYGZus{}loss}\PYG{p}{)}
            \PYG{n}{a\PYGZus{}losses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{actor\PYGZus{}loss}\PYG{p}{)}
            \PYG{n}{alpha\PYGZus{}losses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{alpha\PYGZus{}loss}\PYG{p}{)}
        \PYG{n}{mean\PYGZus{}critic\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{c\PYGZus{}losses}\PYG{p}{)}
        \PYG{n}{mean\PYGZus{}actor\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{a\PYGZus{}losses}\PYG{p}{)}
        \PYG{n}{mean\PYGZus{}alpha\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{alpha\PYGZus{}losses}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{mean\PYGZus{}critic\PYGZus{}loss}\PYG{p}{,} \PYG{n}{mean\PYGZus{}actor\PYGZus{}loss}\PYG{p}{,} \PYG{n}{mean\PYGZus{}alpha\PYGZus{}loss}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@67}
    \PYG{k}{def} \PYG{n+nf}{sample\PYGZus{}unpacked}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,} \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{24}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Returns a batch of experiences as a tuple of numpy arrays.}
\PYG{l+s+sd}{        Input:}
\PYG{l+s+sd}{            batch\PYGZus{}size: int}
\PYG{l+s+sd}{            obs\PYGZus{}shape: tuple, shape of the observation space}
\PYG{l+s+sd}{        returns: (states, actions, rewards, next\PYGZus{}states, dones) \PYGZdq{}\PYGZdq{}\PYGZdq{}}

        \PYG{n}{mini\PYGZus{}batch} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}
        \PYG{k}{assert} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Each experience tuple must have 5 elements: (s,a,r,s}\PYG{l+s+s2}{\PYGZsq{}}\PYG{l+s+s2}{,d)}\PYG{l+s+s2}{\PYGZdq{}}
        \PYG{n}{states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{*}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}states} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{*}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{o}{*}\PYG{n}{action\PYGZus{}shape}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{dones} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{batch\PYGZus{}size}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}

        \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{mini\PYGZus{}batch}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{states}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
            \PYG{n}{actions}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
            \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}
            \PYG{n}{next\PYGZus{}states}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}
            \PYG{n}{dones}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}  \PYG{o}{=} \PYG{n}{mini\PYGZus{}batch}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}
        \PYG{k}{return} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@68}
\PYG{k}{class} \PYG{n+nc}{ValueNetwork}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}  Approximates V(s) function   \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,}
                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}3}\PYG{p}{,}
                 \PYG{n}{model}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr} \PYG{o}{=} \PYG{n}{learning\PYGZus{}rate}
        \PYG{c+c1}{\PYGZsh{} create NN model}
        \PYG{k}{if} \PYG{n}{model} \PYG{o+ow}{is} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{\PYGZus{}build\PYGZus{}net}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{models}\PYG{o}{.}\PYG{n}{clone\PYGZus{}model}\PYG{p}{(}\PYG{n}{model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{lr}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{\PYGZus{}build\PYGZus{}net}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{inp} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Input}\PYG{p}{(}\PYG{n}{shape}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{inp}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{n}{out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{n}{activation}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{relu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                \PYG{n}{kernel\PYGZus{}initializer}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{initializers}\PYG{o}{.}\PYG{n}{HeUniform}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{n}{net\PYGZus{}out} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{layers}\PYG{o}{.}\PYG{n}{Dense}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{(}\PYG{n}{out}\PYG{p}{)}
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{n}{inputs}\PYG{o}{=}\PYG{n}{inp}\PYG{p}{,} \PYG{n}{outputs}\PYG{o}{=}\PYG{n}{net\PYGZus{}out}\PYG{p}{,} \PYG{n}{name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{value\PYGZus{}network}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{model}

    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}call\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Returns V(s) value  \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{n}{v} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{model}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{v}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@70}
\PYG{k}{def} \PYG{n+nf}{train\PYGZus{}sac\PYGZus{}agent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,}
                    \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{200}\PYG{p}{,}
                    \PYG{n}{warmup\PYGZus{}steps}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,}
                    \PYG{n}{update\PYGZus{}per\PYGZus{}step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                    \PYG{n}{log\PYGZus{}freq}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{:}

    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Environment name: }\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{env}\PYG{o}{.}\PYG{n}{spec}\PYG{o}{.}\PYG{n}{id}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RL Agent name:}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{name}\PYG{p}{)}

    \PYG{n}{ep\PYGZus{}scores} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{inf}
    \PYG{n}{total\PYGZus{}steps} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{n}{a\PYGZus{}loss}\PYG{p}{,} \PYG{n}{c\PYGZus{}loss}\PYG{p}{,} \PYG{n}{alpha\PYGZus{}loss} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}
    \PYG{k}{if} \PYG{n}{SAC2}\PYG{p}{:}
        \PYG{n}{v\PYGZus{}loss} \PYG{o}{=} \PYG{l+m+mi}{0}
    \PYG{k}{for} \PYG{n}{e} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}episodes}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{done} \PYG{o}{=} \PYG{k+kc}{False}
        \PYG{n}{truncated} \PYG{o}{=} \PYG{k+kc}{False}
        \PYG{n}{state} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{reset}\PYG{p}{(}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n}{ep\PYGZus{}score} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{ep\PYGZus{}steps} \PYG{o}{=} \PYG{l+m+mi}{0}
        \PYG{n}{c\PYGZus{}losses}\PYG{p}{,} \PYG{n}{a\PYGZus{}losses}\PYG{p}{,} \PYG{n}{alpha\PYGZus{}losses} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{if} \PYG{n}{SAC2}\PYG{p}{:}
            \PYG{n}{v\PYGZus{}losses} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{while} \PYG{o+ow}{not} \PYG{n}{done} \PYG{o+ow}{and} \PYG{o+ow}{not} \PYG{n}{truncated}\PYG{p}{:}

            \PYG{k}{if} \PYG{n}{total\PYGZus{}steps} \PYG{o}{\PYGZlt{}} \PYG{n}{warmup\PYGZus{}steps}\PYG{p}{:}
                \PYG{n}{action} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{p}{)}
            \PYG{k}{else}\PYG{p}{:}
                \PYG{c+c1}{\PYGZsh{} Use the agent\PYGZsq{}s policy to select an action}
                \PYG{n}{action} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{choose\PYGZus{}action}\PYG{p}{(}\PYG{n}{state}\PYG{p}{)}

            \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{done}\PYG{p}{,} \PYG{n}{truncated}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{step}\PYG{p}{(}\PYG{n}{action}\PYG{p}{)}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{store\PYGZus{}transition}\PYG{p}{(}\PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}

            \PYG{n}{state} \PYG{o}{=} \PYG{n}{next\PYGZus{}state}
            \PYG{n}{ep\PYGZus{}score} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
            \PYG{n}{ep\PYGZus{}steps} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
            \PYG{n}{total\PYGZus{}steps} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}

            \PYG{c+c1}{\PYGZsh{} train the agent}
            \PYG{k}{if} \PYG{n}{total\PYGZus{}steps} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{warmup\PYGZus{}steps}\PYG{p}{:}
                \PYG{n}{c\PYGZus{}l}\PYG{p}{,} \PYG{n}{a\PYGZus{}l}\PYG{p}{,} \PYG{n}{ap\PYGZus{}l} \PYG{o}{=} \PYG{n}{agent}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{update\PYGZus{}per\PYGZus{}step}\PYG{o}{=}\PYG{n}{update\PYGZus{}per\PYGZus{}step}\PYG{p}{)}
                \PYG{n}{c\PYGZus{}losses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{c\PYGZus{}l}\PYG{p}{)}
                \PYG{n}{a\PYGZus{}losses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{a\PYGZus{}l}\PYG{p}{)}
                \PYG{n}{alpha\PYGZus{}losses}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ap\PYGZus{}l}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} while loop ends here \PYGZhy{} end of episode}
        \PYG{n}{ep\PYGZus{}scores}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ep\PYGZus{}score}\PYG{p}{)}
        \PYG{n}{c\PYGZus{}loss} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{c\PYGZus{}losses}\PYG{p}{)}
        \PYG{n}{a\PYGZus{}loss} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{a\PYGZus{}losses}\PYG{p}{)}
        \PYG{n}{alpha\PYGZus{}loss} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{alpha\PYGZus{}losses}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{e} \PYG{o}{\PYGZpc{}} \PYG{n}{log\PYGZus{}freq} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{e:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, ep\PYGZus{}score:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}score}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, avg\PYGZus{}ep\PYGZus{}score:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}scores}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{,}\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{            avg100score:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+se}{\PYGZbs{}}
\PYG{l+s+s1}{                best\PYGZus{}score:}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{best\PYGZus{}score}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{ep\PYGZus{}score} \PYG{o}{\PYGZgt{}} \PYG{n}{best\PYGZus{}score}\PYG{p}{:}
            \PYG{n}{best\PYGZus{}score} \PYG{o}{=} \PYG{n}{ep\PYGZus{}score}
            \PYG{n}{agent}\PYG{o}{.}\PYG{n}{save\PYGZus{}weights}\PYG{p}{(}\PYG{n}{filename}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ll\PYGZus{}sac.weights.h5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Best Score: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ep\PYGZus{}score}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, episode: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{e}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{. Model saved.}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ep\PYGZus{}scores}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{100}\PYG{p}{:}\PYG{p}{]}\PYG{p}{)} \PYG{o}{\PYGZgt{}} \PYG{n}{stop\PYGZus{}score}\PYG{p}{:}
            \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{The problem is solved in }\PYG{l+s+si}{\PYGZob{}\PYGZcb{}}\PYG{l+s+s1}{ episodes}\PYG{l+s+s1}{\PYGZsq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{e}\PYG{p}{)}\PYG{p}{)}
            \PYG{k}{break}
    \PYG{c+c1}{\PYGZsh{} for loop ends here}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@71}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{from} \PYG{n+nn}{sac} \PYG{k+kn}{import} \PYG{n}{SACAgent}
\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Create the gym environment}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pendulum\PYGZhy{}v1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{g}\PYG{o}{=}\PYG{l+m+mf}{9.81}\PYG{p}{)}

    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Observation shape: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{obs\PYGZus{}shape}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Action shape: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{action\PYGZus{}shape}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{action\PYGZus{}upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{high}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} Assuming continuous action space}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action upper bound: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action lower bound: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{low}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Initialize the SAC agent}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{SACAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                     \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{,}
                     \PYG{n}{reward\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                     \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{1000000}\PYG{p}{,}
                     \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{256}\PYG{p}{,}
                     \PYG{n}{max\PYGZus{}grad\PYGZus{}norm}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}


    \PYG{c+c1}{\PYGZsh{} train the agent}
    \PYG{n}{train\PYGZus{}sac\PYGZus{}agent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,}
                    \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{200}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@72}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{from} \PYG{n+nn}{sac2} \PYG{k+kn}{import} \PYG{n}{SACAgent}
\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Create the LunarLanderContinuous\PYGZhy{}v3 environment}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{LunarLander\PYGZhy{}v3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{continuous}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Observation shape: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{obs\PYGZus{}shape}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Action shape: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{action\PYGZus{}shape}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{action\PYGZus{}upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{high}  \PYG{c+c1}{\PYGZsh{} Assuming continuous action space}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action upper bound: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action lower bound: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{low}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Initialize the SAC agent}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{SACAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                 \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{1000000}\PYG{p}{,}
                 \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{256}\PYG{p}{,}
                 \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{,}
                 \PYG{n}{reward\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,}
                 \PYG{n}{lr\PYGZus{}a}\PYG{o}{=}\PYG{l+m+mf}{0.001}\PYG{p}{,} \PYG{n}{lr\PYGZus{}c}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}4}\PYG{p}{,} \PYG{n}{lr\PYGZus{}alpha}\PYG{o}{=}\PYG{l+m+mf}{1e\PYGZhy{}4}\PYG{p}{,}
                 \PYG{n}{polyak}\PYG{o}{=}\PYG{l+m+mf}{0.995}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} train the agent}
    \PYG{n}{train\PYGZus{}sac\PYGZus{}agent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,}
                    \PYG{n}{max\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{n}{min\PYGZus{}score}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{300}\PYG{p}{,}
                    \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,}
                    \PYG{n}{update\PYGZus{}per\PYGZus{}step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,}
                    \PYG{n}{wandb\PYGZus{}log}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@73}
\PYG{k+kn}{import} \PYG{n+nn}{gymnasium} \PYG{k}{as} \PYG{n+nn}{gym}
\PYG{k+kn}{from} \PYG{n+nn}{sac2} \PYG{k+kn}{import} \PYG{n}{SACAgent}
\PYG{k}{if} \PYG{n+nv+vm}{\PYGZus{}\PYGZus{}name\PYGZus{}\PYGZus{}} \PYG{o}{==} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZus{}\PYGZus{}main\PYGZus{}\PYGZus{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} Create the LunarLanderContinuous\PYGZhy{}v3 environment}
    \PYG{n}{env} \PYG{o}{=} \PYG{n}{gym}\PYG{o}{.}\PYG{n}{make}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FetchReachDense\PYGZhy{}v3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
            \PYG{n}{max\PYGZus{}episode\PYGZus{}steps}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{render\PYGZus{}mode}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rgb\PYGZus{}array}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{observation\PYGZus{}space}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{observation}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{shape}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Observation shape: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{obs\PYGZus{}shape}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Action shape: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{action\PYGZus{}shape}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{action\PYGZus{}upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{high}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action upper bound: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Action lower bound: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{env}\PYG{o}{.}\PYG{n}{action\PYGZus{}space}\PYG{o}{.}\PYG{n}{low}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Initialize the SAC agent}
    \PYG{n}{agent} \PYG{o}{=} \PYG{n}{SACAgent}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                    \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{,}
                    \PYG{n}{reward\PYGZus{}scale}\PYG{o}{=}\PYG{l+m+mf}{2.0}\PYG{p}{,}
                    \PYG{n}{buffer\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{1000000}\PYG{p}{,}
                    \PYG{n}{batch\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{256}\PYG{p}{,} \PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} train the agent}
    \PYG{n}{train\PYGZus{}sac\PYGZus{}agent}\PYG{p}{(}\PYG{n}{env}\PYG{p}{,} \PYG{n}{agent}\PYG{p}{,} \PYG{n}{num\PYGZus{}episodes}\PYG{o}{=}\PYG{l+m+mi}{1500}\PYG{p}{,}
                    \PYG{n}{max\PYGZus{}score}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{min\PYGZus{}score}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                    \PYG{n}{stop\PYGZus{}score}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,}
                    \PYG{n}{ep\PYGZus{}max\PYGZus{}steps}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,}
                    \PYG{n}{update\PYGZus{}per\PYGZus{}step}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\end{pytx@SaveVerbatim}

\begin{pytx@SaveVerbatim}[commandchars=\\\{\}, frame=single, indent=L]{pytx@PYGpython@default@defaultverb@69}
\PYG{k}{class} \PYG{n+nc}{SACAgent}\PYG{p}{:}
\PYG{+w}{    }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}  Soft Actor\PYGZhy{}Critic Agent \PYGZdq{}\PYGZdq{}\PYGZdq{}}
    \PYG{k}{def} \PYG{n+nf+fm}{\PYGZus{}\PYGZus{}init\PYGZus{}\PYGZus{}}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
               \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{o}{\PYGZlt{}}\PYG{n}{other} \PYG{n}{args} \PYG{o}{.}\PYG{o}{.}\PYG{o}{.}\PYG{o}{\PYGZgt{}}\PYG{p}{)}\PYG{p}{:}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape} \PYG{o}{=} \PYG{n}{obs\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}shape} \PYG{o}{=} \PYG{n}{action\PYGZus{}shape}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}size} \PYG{o}{=} \PYG{n}{action\PYGZus{}shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}upper\PYGZus{}bound} \PYG{o}{=} \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size} \PYG{o}{=} \PYG{n}{batch\PYGZus{}size}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{name} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SAC2}\PYG{l+s+s1}{\PYGZsq{}}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}entropy} \PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{n}{np}\PYG{o}{.}\PYG{n}{prod}\PYG{p}{(}\PYG{n}{action\PYGZus{}shape}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Target entropy = \PYGZhy{}|A|}
        \PYG{o}{\PYGZlt{}}\PYG{n}{snip}\PYG{o}{\PYGZgt{}}

        \PYG{c+c1}{\PYGZsh{} Initialize networks}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor} \PYG{o}{=} \PYG{n}{Actor}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,} \PYG{n}{lr\PYGZus{}a}\PYG{p}{,}
                                 \PYG{n}{action\PYGZus{}upper\PYGZus{}bound}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{actor\PYGZus{}model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1} \PYG{o}{=} \PYG{n}{Critic}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor\PYGZus{}lr}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{critic\PYGZus{}model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2} \PYG{o}{=} \PYG{n}{Critic}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{action\PYGZus{}shape}\PYG{p}{,}
                                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{critic\PYGZus{}model}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value\PYGZus{}network} \PYG{o}{=} \PYG{n}{ValueNetwork}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,}
                                 \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr}\PYG{p}{,} \PYG{n}{model} \PYG{o}{=} \PYG{n}{value\PYGZus{}model}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} create a replay buffer}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer} \PYG{o}{=} \PYG{n}{ReplayBuffer}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer\PYGZus{}size}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Target networks for stability}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}value\PYGZus{}network} \PYG{o}{=} \PYG{n}{ValueNetwork}\PYG{p}{(}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,} \PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}lr}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}value\PYGZus{}network}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{set\PYGZus{}weights}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value\PYGZus{}network}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{get\PYGZus{}weights}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Initialize alpha (temperature parameter)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}alpha} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{Variable}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{math}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{alpha}\PYG{p}{)}\PYG{p}{,} \PYG{n}{trainable}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha\PYGZus{}optimizer} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{keras}\PYG{o}{.}\PYG{n}{optimizers}\PYG{o}{.}\PYG{n}{Adam}\PYG{p}{(}\PYG{n}{learning\PYGZus{}rate}\PYG{o}{=}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{alpha\PYGZus{}lr}\PYG{p}{)}


    \PYG{k}{def} \PYG{n+nf}{choose\PYGZus{}action}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{evaluate}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} same as before \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{store\PYGZus{}transition}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{state}\PYG{p}{,} \PYG{n}{action}\PYG{p}{,} \PYG{n}{reward}\PYG{p}{,} \PYG{n}{next\PYGZus{}state}\PYG{p}{,} \PYG{n}{done}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} same as before \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}target\PYGZus{}networks}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} Update only target value network using Polyak averaging   \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{for} \PYG{n}{target\PYGZus{}var}\PYG{p}{,} \PYG{n}{var} \PYG{o+ow}{in} \PYG{n+nb}{zip}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}value\PYGZus{}network}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{,}
                                   \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value\PYGZus{}network}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{target\PYGZus{}var}\PYG{o}{.}\PYG{n}{assign}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{polyak} \PYG{o}{*} \PYG{n}{target\PYGZus{}var} \PYG{o}{+} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{polyak}\PYG{p}{)} \PYG{o}{*} \PYG{n}{var}\PYG{p}{)}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}value\PYGZus{}network}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
            \PYG{n}{next\PYGZus{}actions}\PYG{p}{,} \PYG{n}{next\PYGZus{}log\PYGZus{}probs} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{actor}\PYG{o}{.}\PYG{n}{sample\PYGZus{}action}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}q1} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{next\PYGZus{}actions}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}q2} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{next\PYGZus{}actions}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}q} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{minimum}\PYG{p}{(}\PYG{n}{next\PYGZus{}q1}\PYG{p}{,} \PYG{n}{next\PYGZus{}q2}\PYG{p}{)}
            \PYG{n}{value\PYGZus{}target} \PYG{o}{=} \PYG{n}{next\PYGZus{}q} \PYG{o}{\PYGZhy{}} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{log\PYGZus{}alpha}\PYG{p}{)} \PYG{o}{*} \PYG{n}{next\PYGZus{}log\PYGZus{}probs}
            \PYG{n}{value} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value\PYGZus{}network}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}
            \PYG{n}{value\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{value} \PYG{o}{\PYGZhy{}} \PYG{n}{value\PYGZus{}target}\PYG{p}{)}\PYG{p}{)}

        \PYG{n}{value\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{value\PYGZus{}loss}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value\PYGZus{}network}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}

        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{value\PYGZus{}grads}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}global\PYGZus{}norm}\PYG{p}{(}\PYG{n}{value\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{value\PYGZus{}grads} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Apply gradients to the value network}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value\PYGZus{}network}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{value\PYGZus{}grads}\PYG{p}{,} \PYGZbs{}
                                            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{value\PYGZus{}network}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{value\PYGZus{}loss}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}critic}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{with} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{GradientTape}\PYG{p}{(}\PYG{n}{persistent}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)} \PYG{k}{as} \PYG{n}{tape}\PYG{p}{:}
            \PYG{n}{q1} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{)}
            \PYG{n}{q2} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{)}
            \PYG{n}{next\PYGZus{}v} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{target\PYGZus{}value\PYGZus{}network}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{)}
            \PYG{n}{target\PYGZus{}q} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{reward\PYGZus{}scale} \PYG{o}{*} \PYG{n}{rewards} \PYG{o}{+} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{gamma} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{dones}\PYG{p}{)} \PYG{o}{*} \PYG{n}{next\PYGZus{}v}
            \PYG{n}{critic\PYGZus{}1\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{target\PYGZus{}q} \PYG{o}{\PYGZhy{}} \PYG{n}{q1}\PYG{p}{)}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}2\PYGZus{}loss} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{reduce\PYGZus{}mean}\PYG{p}{(}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{square}\PYG{p}{(}\PYG{n}{target\PYGZus{}q} \PYG{o}{\PYGZhy{}} \PYG{n}{q2}\PYG{p}{)}\PYG{p}{)}

        \PYG{n}{critic\PYGZus{}1\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{critic\PYGZus{}1\PYGZus{}loss}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}
        \PYG{n}{critic\PYGZus{}2\PYGZus{}grads} \PYG{o}{=} \PYG{n}{tape}\PYG{o}{.}\PYG{n}{gradient}\PYG{p}{(}\PYG{n}{critic\PYGZus{}2\PYGZus{}loss}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}

        \PYG{k}{if} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n}{critic\PYGZus{}1\PYGZus{}grads}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}global\PYGZus{}norm}\PYG{p}{(}\PYG{n}{critic\PYGZus{}1\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm}\PYG{p}{)}
            \PYG{n}{critic\PYGZus{}2\PYGZus{}grads}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{clip\PYGZus{}by\PYGZus{}global\PYGZus{}norm}\PYG{p}{(}\PYG{n}{critic\PYGZus{}2\PYGZus{}grads}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{max\PYGZus{}grad\PYGZus{}norm}\PYG{p}{)}

        \PYG{k}{if} \PYG{n}{critic\PYGZus{}1\PYGZus{}grads} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}1\PYGZus{}grads}\PYG{p}{,} \PYGZbs{}
                                            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}1}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{critic\PYGZus{}2\PYGZus{}grads} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{optimizer}\PYG{o}{.}\PYG{n}{apply\PYGZus{}gradients}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{critic\PYGZus{}2\PYGZus{}grads}\PYG{p}{,} \PYGZbs{}
                                            \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{critic\PYGZus{}2}\PYG{o}{.}\PYG{n}{model}\PYG{o}{.}\PYG{n}{trainable\PYGZus{}variables}\PYG{p}{)}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{critic\PYGZus{}1\PYGZus{}loss}\PYG{p}{,} \PYG{n}{critic\PYGZus{}2\PYGZus{}loss}

    \PYG{k}{def} \PYG{n+nf}{update\PYGZus{}actor}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{,} \PYG{n}{states}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{} same as before \PYGZdq{}\PYGZdq{}\PYGZdq{}}
        \PYG{k}{pass}

    \PYG{k}{def} \PYG{n+nf}{train}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{p}{)}\PYG{p}{:}
\PYG{+w}{        }\PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}}
\PYG{l+s+sd}{        Train the agent using a batch of transitions from the replay buffer}
\PYG{l+s+sd}{        \PYGZdq{}\PYGZdq{}\PYGZdq{}}

        \PYG{k}{if} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{:}
            \PYG{k}{return} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}

        \PYG{c+c1}{\PYGZsh{} Sample a batch of transitions from the replay buffer}
        \PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,} \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones} \PYG{o}{=} \PYGZbs{}
               \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{buffer}\PYG{o}{.}\PYG{n}{sample\PYGZus{}unpacked}\PYG{p}{(}\PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{obs\PYGZus{}shape}\PYG{p}{,}
                           \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{action\PYGZus{}shape}\PYG{p}{,} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{batch\PYGZus{}size}\PYG{p}{)}
        \PYG{c+c1}{\PYGZsh{} Convert to tensors}
        \PYG{n}{states} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{actions} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{actions}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{next\PYGZus{}states} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}
        \PYG{n}{dones} \PYG{o}{=} \PYG{n}{tf}\PYG{o}{.}\PYG{n}{convert\PYGZus{}to\PYGZus{}tensor}\PYG{p}{(}\PYG{n}{dones}\PYG{p}{,} \PYG{n}{dtype}\PYG{o}{=}\PYG{n}{tf}\PYG{o}{.}\PYG{n}{float32}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Update value network}
        \PYG{n}{value\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}value\PYGZus{}network}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Update critic networks}
        \PYG{n}{critic\PYGZus{}1\PYGZus{}loss}\PYG{p}{,} \PYG{n}{critic\PYGZus{}2\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}critic}\PYG{p}{(}\PYG{n}{states}\PYG{p}{,} \PYG{n}{actions}\PYG{p}{,}
                                             \PYG{n}{rewards}\PYG{p}{,} \PYG{n}{next\PYGZus{}states}\PYG{p}{,} \PYG{n}{dones}\PYG{p}{)}

        \PYG{n}{critic\PYGZus{}loss} \PYG{o}{=} \PYG{p}{(}\PYG{n}{critic\PYGZus{}1\PYGZus{}loss} \PYG{o}{+} \PYG{n}{critic\PYGZus{}2\PYGZus{}loss}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mf}{2.0}

        \PYG{c+c1}{\PYGZsh{} Update actor network}
        \PYG{n}{actor\PYGZus{}loss}\PYG{p}{,} \PYG{n}{alpha\PYGZus{}loss} \PYG{o}{=} \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}actor}\PYG{p}{(}\PYG{n}{states}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Update target networks}
        \PYG{n+nb+bp}{self}\PYG{o}{.}\PYG{n}{update\PYGZus{}target\PYGZus{}networks}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{return} \PYG{n}{value\PYGZus{}loss}\PYG{p}{,} \PYG{n}{critic\PYGZus{}loss}\PYG{p}{,} \PYG{n}{actor\PYGZus{}loss}\PYG{p}{,} \PYG{n}{alpha\PYGZus{}loss}
\end{pytx@SaveVerbatim}

